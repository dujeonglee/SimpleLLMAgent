"""
Orchestrator Module (Updated)
=============================
LLMTool ì—°ë™ ë° ì´ì „ ê²°ê³¼ ì°¸ì¡° ê¸°ëŠ¥ ì¶”ê°€.

ì£¼ìš” ë³€ê²½ì‚¬í•­:
1. LLMTool ì´ˆê¸°í™” ì‹œ llm_callerì™€ previous_result_getter ì£¼ì…
2. [PREVIOUS_RESULT] í”Œë ˆì´ìŠ¤í™€ë” ìë™ ëŒ€ì²´
3. ì´ì „ ê²°ê³¼ë¥¼ í™œìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ê°œì„ 
"""

import json
import re
from dataclasses import dataclass, field, asdict
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Dict, Generator, List, Optional

from core.shared_storage import SharedStorage
from core.debug_logger import DebugLogger
from core.base_tool import ToolRegistry, ToolResult, ActionSchema
from core.json_parser import (parse_json_robust, remove_think_tags)
from core.prompt_builder import PromptBuilder
from core.execution_events import (
    ExecutionEvent,
    PlanPromptEvent,
    PlanReadyEvent,
    ThinkingEvent,
    StepPromptEvent,
    ToolCallEvent,
    ToolResultEvent,
    FinalAnswerEvent,
    ErrorEvent
)


# =============================================================================
# Data Classes (ë³€ê²½ ì—†ìŒ)
# =============================================================================

@dataclass
class LLMConfig:
    """LLM ë™ì  ì„¤ì •"""
    base_url: str = "http://localhost:11434"
    model: str = "llama3.2"
    timeout: int = 120
    temperature: float = 0.7
    max_tokens: int = 2048
    num_ctx: int = 4096
    top_p: float = 0.9
    top_k: int = 40
    repeat_penalty: float = 1.1
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    max_steps: int = 10

    def to_dict(self) -> Dict:
        return asdict(self)

    def update(self, **kwargs):
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)


@dataclass
class ToolCall:
    """LLMì´ ìš”ì²­í•œ Tool í˜¸ì¶œ ì •ë³´"""
    name: str
    arguments: Dict
    known_actions: List[str] = field(default_factory=list)

    def __post_init__(self):
        if "." in self.name:
            parts = self.name.split(".", 1)
            self.name = parts[0]
            if "action" not in self.arguments:
                self.arguments["action"] = parts[1]

        if not self.arguments.get("action"):
            inferred_action = self._infer_action_from_prefix()
            if inferred_action:
                self.arguments["action"] = inferred_action

    def _infer_action_from_prefix(self) -> Optional[str]:
        for key in self.arguments.keys():
            if "_" in key:
                prefix = key.split("_")[0]
                if prefix in self.known_actions:
                    return prefix
        return None
    
    @property
    def action(self) -> str:
        return self.arguments.get("action", "")
    
    @property
    def params(self) -> Dict:
        return {k: v for k, v in self.arguments.items() if k != "action"}


@dataclass
class LLMResponse:
    """LLM ì‘ë‹µ íŒŒì‹± ê²°ê³¼"""
    thought: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None
    content: Optional[str] = None
    raw_response: str = ""
    
    @property
    def is_final_answer(self) -> bool:
        return self.tool_calls is None and self.content is not None


class StepType(Enum):
    PLANNING = "planning"
    PLAN_PROMPT = "plan_prompt"
    PLAN_READY = "plan_ready"
    THINKING = "thinking"
    STEP_PROMPT = "step_prompt"
    TOOL_CALL = "tool_call"
    TOOL_RESULT = "tool_result"
    FINAL_ANSWER = "final_answer"
    ERROR = "error"


@dataclass
class PlannedStep:
    """ê³„íšëœ ë‹¨ê³„"""
    step: int
    tool_name: str
    action: str
    description: str
    params_hint: Dict = field(default_factory=dict)
    status: str = "pending"

@dataclass
class StepInfo:
    """Streaming ì¶œë ¥ìš© Step ì •ë³´"""
    type: StepType
    step: int
    content: Any
    tool_name: Optional[str] = None
    action: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def to_dict(self) -> Dict:
        return {
            "type": self.type.value,
            "step": self.step,
            "content": self.content,
            "tool_name": self.tool_name,
            "action": self.action,
            "timestamp": self.timestamp
        }


# =============================================================================
# Orchestrator (Updated)
# =============================================================================

class Orchestrator:
    """
    Multi-Agent Chatbot Orchestrator
    
    ë³€ê²½ì‚¬í•­:
    - LLMTool ìë™ ì—°ë™
    - [PREVIOUS_RESULT] í”Œë ˆì´ìŠ¤í™€ë” ì²˜ë¦¬
    - ì´ì „ ê²°ê³¼ ì°¸ì¡° í”„ë¡¬í”„íŠ¸ ê°œì„ 
    """
    
    def __init__(
        self,
        tools: ToolRegistry,
        storage: SharedStorage,
        llm_config: LLMConfig = None,
        max_steps: int = 10,
        on_step_complete: Callable[[ExecutionEvent], None] = None,
        debug_enabled: bool = True
    ):
        self.tools = tools
        self.storage = storage
        self.llm_config = llm_config or LLMConfig()
        self.max_steps = max_steps
        self.on_step_complete = on_step_complete
        self.logger = DebugLogger("Orchestrator", levels=None if debug_enabled else [])

        self._stopped = False
        self._current_step = 0
        self._plan: List[PlannedStep] = []
        self._needs_tools: bool = True  # ê¸°ë³¸ê°’: íˆ´ í•„ìš”
        
        # LLMTool ì—°ë™
        self._setup_llm_tool()
        
        self.logger.debug("Orchestrator ì´ˆê¸°í™”", {
            "max_steps": max_steps,
            "llm_model": self.llm_config.model,
            "tools": self.tools.list_tools()
        })
    
    def _setup_llm_tool(self):
        """LLMToolì— í•„ìš”í•œ í•¨ìˆ˜ ì£¼ì…"""
        llm_tool = self.tools.get("llm_tool")
        if llm_tool is None:
            self.logger.debug("llm_toolì´ ë“±ë¡ë˜ì§€ ì•ŠìŒ")
            return

        # LLMTool íƒ€ì… ì²´í¬ (duck typing)
        if hasattr(llm_tool, 'set_llm_caller'):
            llm_tool.set_llm_caller(self._call_llm_api)
            self.logger.debug("LLMToolì— llm_caller ì£¼ì… ì™„ë£Œ")

    def _get_known_actions(self) -> Dict[str, List[str]]:
        """ë“±ë¡ëœ toolë“¤ì˜ action ì •ë³´ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        known_actions = {}
        for tool in self.tools.get_all():
            known_actions[tool.name] = tool.get_action_names()
        self.logger.debug(f"Known actions ìƒì„± ì™„ë£Œ: {known_actions}")
        return known_actions
    
    # =========================================================================
    # Public Methods
    # =========================================================================
    
    def update_llm_config(self, **kwargs):
        """LLM ì„¤ì • ë™ì  ë³€ê²½"""
        old_values = {}
        for key in kwargs:
            if hasattr(self.llm_config, key):
                old_values[key] = getattr(self.llm_config, key)
        
        self.llm_config.update(**kwargs)
        
        self.logger.debug("LLM ì„¤ì • ë³€ê²½", {
            "changes": {k: {"old": old_values.get(k), "new": v} 
                       for k, v in kwargs.items() if k in old_values}
        })
    
    def stop(self):
        """ì‹¤í–‰ ì¤‘ì§€"""
        self._stopped = True
        self.logger.debug("ì‹¤í–‰ ì¤‘ì§€ ìš”ì²­ë¨")

    def _initialize_session(self, user_query: str, files_context: str = ""):
        """ì„¸ì…˜ ì´ˆê¸°í™”"""
        self._stopped = False
        self._current_step = 0
        self._plan = []
        self._needs_tools = True
        self._files_context = files_context
        self.storage.start_session(user_query)
        self.logger.debug(f"ì‹¤í–‰ ì‹œì‘: {user_query[:50]}...")

    def run_stream(self, user_query: str, files_context: str = "") -> Generator[ExecutionEvent, None, None]:
        """Streaming ì‹¤í–‰ - Plan & Execute íŒ¨í„´"""
        self._initialize_session(user_query, files_context)

        try:
            # Phase 1: Planning
            for event in self._create_plan(user_query):
                yield event

                # Planì´ direct answerë‚˜ errorë¡œ ëë‚˜ë©´ ì¢…ë£Œ
                if isinstance(event, (FinalAnswerEvent, ErrorEvent)):
                    return

            # Phase 2: Execution
            for event in self._execute_plan(user_query):
                yield event

                # Executionì´ errorë¡œ ëë‚˜ë©´ ì¢…ë£Œ
                if isinstance(event, ErrorEvent):
                    return

            # Phase 3: Final Answer
            for event in self._generate_final_answer_stream(user_query):
                yield event

        except Exception as e:
            error_msg = f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            self.logger.error(error_msg, {"exception": type(e).__name__})
            self.storage.complete_session(final_response=error_msg, status="error")
            yield ErrorEvent(error_message=error_msg)
    
    # =========================================================================
    # Tool Execution (Updated)
    # =========================================================================

    def _execute_tool_with_retry(
        self,
        tool_call: ToolCall,
        planned_step: PlannedStep,
        max_retries: int = 2
    ) -> ToolResult:
        """
        Tool ì‹¤í–‰ ë° ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„

        Args:
            tool_call: ì‹¤í–‰í•  tool call
            planned_step: ê³„íšëœ step ì •ë³´
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ 2íšŒ)

        Returns:
            ToolResult: ìµœì¢… ì‹¤í–‰ ê²°ê³¼ (ì„±ê³µ ë˜ëŠ” ìµœì¢… ì‹¤íŒ¨)
                       metadataì— ì¬ì‹œë„ ì •ë³´ í¬í•¨:
                       - retry_count: ì¬ì‹œë„ íšŸìˆ˜
                       - retry_history: ê° ì‹œë„ì˜ ì—ëŸ¬ ê¸°ë¡
                       - corrected_params: ìˆ˜ì •ëœ íŒŒë¼ë¯¸í„° (ì¬ì‹œë„ ì„±ê³µ ì‹œ)
        """
        retry_history = []

        # ì²« ì‹œë„
        result = self._execute_tool(tool_call)

        if result.success:
            # ì²« ì‹œë„ ì„±ê³µ - ì¬ì‹œë„ ì •ë³´ ì—†ìŒ
            result.metadata["retry_count"] = 0
            return result

        # ì²« ì‹œë„ ì‹¤íŒ¨ - ì¬ì‹œë„ ì‹œì‘
        retry_history.append({
            "attempt": 0,
            "error": result.error,
            "params": tool_call.params.copy()
        })

        # ì¬ì‹œë„ ë£¨í”„
        for retry_count in range(1, max_retries + 1):
            self.logger.debug(f"ì¬ì‹œë„ {retry_count}/{max_retries}: {tool_call.name}.{tool_call.action}", {
                "previous_error": result.error
            })

            # LLMì— ì—ëŸ¬ ì „ë‹¬í•˜ì—¬ ìˆ˜ì •ëœ íŒŒë¼ë¯¸í„° ìš”ì²­
            corrected_call = self._request_parameter_correction(
                original_call=tool_call,
                error_message=result.error,
                planned_step=planned_step,
                retry_count=retry_count
            )

            if not corrected_call:
                # LLMì´ ìˆ˜ì • ë¶ˆê°€ëŠ¥í•˜ë‹¤ê³  íŒë‹¨
                self.logger.warn(f"ì¬ì‹œë„ ì¤‘ë‹¨: LLMì´ íŒŒë¼ë¯¸í„° ìˆ˜ì • ë¶ˆê°€ íŒë‹¨")
                break

            # ìˆ˜ì •ëœ íŒŒë¼ë¯¸í„°ë¡œ ì¬ì‹œë„
            result = self._execute_tool(corrected_call)

            retry_history.append({
                "attempt": retry_count,
                "error": result.error if not result.success else None,
                "params": corrected_call.params.copy(),
                "success": result.success
            })

            if result.success:
                # ì¬ì‹œë„ ì„±ê³µ!
                self.logger.debug(f"ì¬ì‹œë„ ì„±ê³µ: {retry_count}ë²ˆì§¸ ì‹œë„ì—ì„œ ì„±ê³µ")
                result.metadata["retry_count"] = retry_count
                result.metadata["retry_history"] = retry_history
                result.metadata["corrected_params"] = corrected_call.params
                return result

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        result.metadata["retry_count"] = len(retry_history) - 1
        result.metadata["retry_history"] = retry_history
        result.metadata["retry_exhausted"] = True

        self.logger.error(f"ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨: {max_retries}íšŒ ì‹œë„ í›„ ì‹¤íŒ¨")

        return result

    def _request_parameter_correction(
        self,
        original_call: ToolCall,
        error_message: str,
        planned_step: PlannedStep,
        retry_count: int
    ) -> Optional[ToolCall]:
        """
        LLMì— ì—ëŸ¬ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì—¬ ìˆ˜ì •ëœ íŒŒë¼ë¯¸í„° ìš”ì²­

        Args:
            original_call: ì›ë³¸ tool call
            error_message: ë°œìƒí•œ ì—ëŸ¬ ë©”ì‹œì§€
            planned_step: ê³„íšëœ step
            retry_count: í˜„ì¬ ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            Optional[ToolCall]: ìˆ˜ì •ëœ tool call (ìˆ˜ì • ë¶ˆê°€ëŠ¥í•˜ë©´ None)
        """
        # í˜„ì¬ tool ê°€ì ¸ì˜¤ê¸°
        current_tool = self.tools.get(original_call.name)
        if not current_tool:
            return None

        # í˜„ì¬ actionì˜ schemaë§Œ ê°€ì ¸ì˜¤ê¸°
        action_schema = current_tool.get_action_schema(original_call.action)
        if not action_schema:
            return None

        # ì‚¬ìš© ê°€ëŠ¥í•œ ê²°ê³¼ ëª©ë¡
        available_results = self.storage.get_available_results_summary()

        # PromptBuilder ì‚¬ìš©
        builder = PromptBuilder()
        system_prompt, user_prompt = builder.for_retry(
            tool_name=original_call.name,
            action=original_call.action,
            action_schema=action_schema.to_dict(),
            error_message=error_message,
            failed_params=original_call.params,
            previous_results=available_results
        ).build()

        try:
            raw_response = self._call_llm_api(system_prompt, user_prompt)
            parsed = self._parse_tool_call_response(raw_response)

            if not parsed.tool_calls:
                self.logger.debug(f"LLM íŒë‹¨: íŒŒë¼ë¯¸í„° ìˆ˜ì •ìœ¼ë¡œ í•´ê²° ë¶ˆê°€ëŠ¥", {
                    "step": planned_step.step,
                    "description": planned_step.description,
                    "thought": parsed.thought
                })
                return None

            self.logger.debug(f"LLMì´ íŒŒë¼ë¯¸í„° ìˆ˜ì • ì œì•ˆ", {
                "step": planned_step.step,
                "description": planned_step.description,
                "thought": parsed.thought,
                "corrected_params": parsed.tool_calls[0].params
            })

            return parsed.tool_calls[0]

        except Exception as e:
            self.logger.error(f"íŒŒë¼ë¯¸í„° ìˆ˜ì • ìš”ì²­ ì‹¤íŒ¨: {str(e)}", {
                "step": planned_step.step,
                "description": planned_step.description
            })
            return None

    def _execute_tool(self, tool_call: ToolCall) -> ToolResult:
        """Tool ì‹¤í–‰ - REF ìë™ ì¹˜í™˜

        ì§€ì›í•˜ëŠ” ì°¸ì¡° í˜•ì‹:
        - [RESULT:session_id_step]: íŠ¹ì • session_id_stepì˜ outputìœ¼ë¡œ ì¹˜í™˜ (ì˜ˆ: [RESULT:Session0_1])

        Note: ì´ ë©”ì„œë“œëŠ” ì¬ì‹œë„ ë¡œì§ì´ ì—†ëŠ” ë‹¨ìˆœ ì‹¤í–‰ë§Œ ìˆ˜í–‰.
              ì¬ì‹œë„ê°€ í•„ìš”í•œ ê²½ìš° _execute_tool_with_retry() ì‚¬ìš©.
        """
        # íŒŒë¼ë¯¸í„° ë³µì‚¬ ë° REF ì¹˜í™˜
        params = tool_call.params.copy()
        params = self._substitute_all_refs(params)

        self.logger.debug(f"Tool ì‹¤í–‰: {tool_call.name}.{tool_call.action}", {
            "params_keys": list(params.keys())
        })

        result = self.tools.execute(
            tool_name=tool_call.name,
            action=tool_call.action,
            params=params,
            session_id=self.storage.get_current_context().session_id,
            step=self._current_step
        )

        return result
    
    # =========================================================================
    # Planning Methods
    # =========================================================================
    
    def _create_plan(self, user_query: str) -> Generator[ExecutionEvent, None, None]:
        """Phase 1: ì „ì²´ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ë° ì´ˆê¸° ì‘ë‹µ ìƒì„±"""
        # 1. Planning ì‹œì‘
        yield ThinkingEvent(message="ì‘ì—… ê³„íš ìˆ˜ë¦½ ì¤‘...")

        # 2. ì¤‘ì§€ ì²´í¬
        if self._stopped:
            self.storage.complete_session(final_response="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨", status="error")
            yield ErrorEvent(error_message="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨")
            return

        # 3. Query classification - íˆ´ í•„ìš” ì—¬ë¶€ íŒë‹¨
        self._needs_tools = self._classify_query(user_query)
        self.logger.debug("Query classification ì™„ë£Œ", {"needs_tools": self._needs_tools})

        # 4. LLM í˜¸ì¶œí•˜ì—¬ plan ìƒì„± (ë˜ëŠ” direct answer)
        plan_response, plan_system_prompt, plan_user_prompt, plan_raw_response = self._call_plan_llm(user_query)

        # 5. Direct answer ì²˜ë¦¬
        if plan_response.get("direct_answer"):
            self.storage.complete_session(
                final_response=plan_response["direct_answer"],
                status="completed"
            )
            yield FinalAnswerEvent(answer=plan_response["direct_answer"])
            return

        yield PlanPromptEvent(
            system_prompt=plan_system_prompt,
            user_prompt=plan_user_prompt,
            raw_response=plan_raw_response
        )

        # 5. Plan íŒŒì‹±
        self._plan = self._parse_plan(plan_response.get("plan", []))

        # 6. Plan ì—†ìŒ ì²˜ë¦¬
        if not self._plan:
            direct_response = plan_response.get("thought", "ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.storage.complete_session(final_response=direct_response, status="completed")
            yield FinalAnswerEvent(answer=direct_response)
            return

        # 7. Plan ì¤€ë¹„ ì™„ë£Œ
        plan_summary = self._format_plan_summary()
        yield PlanReadyEvent(plan_content=plan_summary)

    def _classify_query(self, user_query: str) -> bool:
        """Queryê°€ íˆ´ ì‚¬ìš©ì´ í•„ìš”í•œì§€ íŒë‹¨"""
        tools_list = "\n".join([f"- {name}: {tool.get_schema().get('description', 'No description')}"
                                for name, tool in self.tools._tools.items()])

        # PromptBuilder ì‚¬ìš©
        builder = PromptBuilder()
        system_prompt, user_prompt = builder.for_query_classification(tools_list).build(
            user_query=user_query
        )

        self.logger.debug("Query classification ì‹œì‘")

        raw_response = self._call_llm_api(system_prompt, user_prompt)

        try:
            parsed = parse_json_robust(raw_response)
            needs_tools = parsed.get("needs_tools", True)  # ê¸°ë³¸ê°’: True (ì•ˆì „)
            self.logger.debug("Classification ê²°ê³¼", {
                "needs_tools": needs_tools,
                "reasoning": parsed.get("reasoning", "")
            })
            return needs_tools
        except Exception as e:
            self.logger.warn(f"Classification íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’(True) ì‚¬ìš©: {e}")
            return True  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ True ë°˜í™˜

    def _call_plan_llm(self, user_query: str) -> tuple[Dict, str, str, str]:
        """LLMì— ê³„íš ìˆ˜ë¦½ ë˜ëŠ” direct answer ìš”ì²­

        Returns:
            tuple: (parsed_response, system_prompt, user_prompt, raw_response)
        """
        # PromptBuilder ì‚¬ìš©
        builder = PromptBuilder()

        if self._needs_tools:
            # Tool schemas ê°€ì ¸ì˜¤ê¸°
            tool_schemas = {
                tool_name: tool.get_schema()
                for tool_name, tool in self.tools._tools.items()
            }

            system_prompt, user_prompt = builder.for_planning(
                tool_schemas=tool_schemas,
                max_steps=self.max_steps,
                needs_tools=True,
                files_context=self._files_context
            ).build(user_query=user_query)

        else:
            # íˆ´ì´ í•„ìš” ì—†ëŠ” ê²½ìš°: direct answer
            system_prompt, user_prompt = builder.for_planning(
                tool_schemas={},
                max_steps=self.max_steps,
                needs_tools=False
            ).build(user_query=user_query)

        # ê³µí†µ: LLM í˜¸ì¶œ ë° íŒŒì‹±
        self.logger.debug("Planning í˜¸ì¶œ", {
            "query": user_query[:50],
            "needs_tools": self._needs_tools
        })

        raw_response = self._call_llm_api(system_prompt, user_prompt)

        try:
            parsed = parse_json_robust(raw_response)
            self.logger.debug("Plan ìƒì„± ì™„ë£Œ", {
                "has_plan": "plan" in parsed,
                "has_direct_answer": "direct_answer" in parsed,
                "needs_tools": self._needs_tools
            })
            return parsed, system_prompt, user_prompt, raw_response
        except Exception as e:
            self.logger.error(f"Plan íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {"direct_answer": f"ê³„íš ìˆ˜ë¦½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, system_prompt, user_prompt, raw_response

    # =========================================================================
    # Execution Methods
    # =========================================================================

    def _execute_plan(self, user_query: str) -> Generator[ExecutionEvent, None, None]:
        """Phase 2: ê³„íšëœ ë‹¨ê³„ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰"""
        # Precondition: Planì´ ì¡´ì¬í•´ì•¼ í•¨
        if not self._plan:
            error_msg = "CRITICAL: _execute_plan called with empty plan"
            self.logger.error(error_msg)
            self.storage.complete_session(final_response=error_msg, status="error")
            yield ErrorEvent(error_message=error_msg)
            return

        for planned_step in self._plan:
            self._current_step = planned_step.step
            planned_step.status = "running"

            yield ThinkingEvent(message=f"Step {self._current_step}: {planned_step.description}")

            # LLM í˜¸ì¶œ ì „ ì¤‘ì§€ ì²´í¬
            if self._stopped:
                self.storage.complete_session(final_response="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨", status="error")
                yield ErrorEvent(error_message="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨")
                return

            execution_response, prompt_info = self._get_execution_params(user_query, planned_step)

            # Step í”„ë¡¬í”„íŠ¸ ì •ë³´ yield (ToolResultEventì—ì„œ ì‚¬ìš©í•  ì •ë³´)
            step_prompt_event = StepPromptEvent(
                step=self._current_step,
                tool_name=planned_step.tool_name,
                action=planned_step.action,
                system_prompt=prompt_info["system_prompt"],
                user_prompt=prompt_info["user_prompt"],
                raw_response=prompt_info["raw_response"]
            )
            yield step_prompt_event

            # LLM í˜¸ì¶œ í›„ ì¤‘ì§€ ì²´í¬
            if self._stopped:
                self.storage.complete_session(final_response="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨", status="error")
                yield ErrorEvent(error_message="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨")
                return

            if execution_response.thought:
                yield ThinkingEvent(message=execution_response.thought)

            # Check if tool_calls count is not exactly 1
            tool_calls_count = len(execution_response.tool_calls) if execution_response.tool_calls else 0
            if tool_calls_count != 1:
                error_msg = f"LLM failed to generate exactly one tool_call for Step {self._current_step} ({planned_step.tool_name}.{planned_step.action}). Expected 1, got {tool_calls_count}."
                self.logger.error(error_msg, {
                    "step": self._current_step,
                    "planned_tool": planned_step.tool_name,
                    "planned_action": planned_step.action,
                    "tool_calls_count": tool_calls_count,
                    "llm_thought": execution_response.thought
                })

                planned_step.status = "failed"

                yield ErrorEvent(
                    error_message=f"âš ï¸ Step {self._current_step} failed: {error_msg}\n\nLLM response: {execution_response.thought or 'No explanation provided'}"
                )
                # Continue to next step instead of stopping entirely
                continue

            # Execute the single tool call
            tool_call = execution_response.tool_calls[0]

            # Tool ì‹¤í–‰ ì „ ì¤‘ì§€ ì²´í¬
            if self._stopped:
                self.storage.complete_session(final_response="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨", status="error")
                yield ErrorEvent(error_message="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨")
                return

            yield ToolCallEvent(
                step=self._current_step,
                tool_name=tool_call.name,
                action=tool_call.action,
                arguments=tool_call.arguments
            )

            # Tool ì‹¤í–‰ (ìë™ ì¬ì‹œë„ í¬í•¨)
            result = self._execute_tool_with_retry(tool_call, planned_step, max_retries=2)

            # Tool ì‹¤í–‰ í›„ ì¤‘ì§€ ì²´í¬
            if self._stopped:
                self.storage.complete_session(final_response="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨", status="error")
                yield ErrorEvent(error_message="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨")
                return

            # ToolResultë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
            self.storage.add_result(result)

            planned_step.status = "completed" if result.success else "failed"

            yield ToolResultEvent(
                step=self._current_step,
                tool_name=tool_call.name,
                action=tool_call.action,
                result=result.output if result.success else result.error,
                success=result.success
            )

            if self.on_step_complete:
                self.on_step_complete(ToolResultEvent(
                    step=self._current_step,
                    tool_name=tool_call.name,
                    action=tool_call.action,
                    result="Step completed",
                    success=result.success
                ))

        # ì¤‘ì§€ëœ ê²½ìš° Final Answer ìƒì„± ìŠ¤í‚µ
        if self._stopped:
            self.storage.complete_session(final_response="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨", status="error")
            yield ErrorEvent(error_message="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨")
            return

    def _generate_final_answer_stream(self, user_query: str) -> Generator[ExecutionEvent, None, None]:
        """Phase 3: ìµœì¢… ì‘ë‹µ ìƒì„±"""
        # 1. Thinking ì‹œì‘
        yield ThinkingEvent(message="ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...")

        # 2. ì¤‘ì§€ ì²´í¬
        if self._stopped:
            self.storage.complete_session(final_response="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨", status="error")
            yield ErrorEvent(error_message="ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì·¨ì†Œë¨")
            return

        # 3. ìµœì¢… ì‘ë‹µ ìƒì„±
        final_response = self._generate_final_answer(user_query)

        # 4. ì„¸ì…˜ ì™„ë£Œ
        self.storage.complete_session(final_response=final_response, status="completed")

        # 5. ìµœì¢… ì‘ë‹µ ë°˜í™˜
        yield FinalAnswerEvent(answer=final_response)

    def _get_execution_params(self, user_query: str, planned_step: PlannedStep) -> tuple[LLMResponse, Dict]:
        """
        ê³„íšëœ step ì‹¤í–‰ì„ ìœ„í•œ ì •í™•í•œ íŒŒë¼ë¯¸í„° ê²°ì •

        Returns:
            tuple: (LLMResponse, prompt_info dict with system_prompt, user_prompt, raw_response)
        """
        # í˜„ì¬ stepì˜ toolê³¼ action ê°€ì ¸ì˜¤ê¸°
        current_tool = self.tools.get(planned_step.tool_name)
        if not current_tool:
            raise ValueError(f"Tool not found: {planned_step.tool_name}")

        # í˜„ì¬ actionì˜ schema ê°€ì ¸ì˜¤ê¸°
        action_schema = current_tool.get_action_schema(planned_step.action)
        if not action_schema:
            raise ValueError(f"Action not found: {planned_step.tool_name}.{planned_step.action}")

        # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        execution_context = self.storage.get_available_results_summary(max_output_preview=300)

        # PromptBuilder ì‚¬ìš©
        builder = PromptBuilder()
        system_prompt, user_prompt = builder.for_execution(
            tool_name=planned_step.tool_name,
            action=planned_step.action,
            action_schema=action_schema.to_dict(),
            step_num=planned_step.step,
            params_hint=planned_step.params_hint,
            execution_context=execution_context
        ).build()

        raw_response = self._call_llm_api(system_prompt, user_prompt)

        # í”„ë¡¬í”„íŠ¸ ì •ë³´ ìˆ˜ì§‘
        prompt_info = {
            "system_prompt": system_prompt,
            "user_prompt": user_prompt,
            "raw_response": raw_response
        }

        return self._parse_tool_call_response(raw_response), prompt_info
    
    # =========================================================================
    # Helper Methods
    # =========================================================================


    def _substitute_result_refs(self, params: Dict) -> Dict:
        """íŒŒë¼ë¯¸í„°ì˜ [RESULT:*] ì°¸ì¡°ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜"""
        import re

        def replace_ref(value):
            if not isinstance(value, str):
                return value

            try:
                # [RESULT:session_id_step] íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: [RESULT:Session0_1])
                pattern = r'\[RESULT:([^\]]+)\]'
                matches = re.findall(pattern, value)

                for result_key in matches:
                    output = self.storage.get_output_by_key(result_key)
                    if output is not None:
                        # ì „ì²´ ë¬¸ìì—´ì´ REFë§Œ ìˆìœ¼ë©´ outputì„ ì§ì ‘ ì‚¬ìš©, ì•„ë‹ˆë©´ ë¬¸ìì—´ ì¹˜í™˜
                        if value.strip() == f"[RESULT:{result_key}]":
                            return output
                        else:
                            value = value.replace(f"[RESULT:{result_key}]", str(output))
                    else:
                        raise ValueError(f"[RESULT:{result_key}] - Cannot find {result_key}")
                return value
            except Exception as e:
                self.logger.error(f"RESULT reference substitution failed: {e}")
                raise

        # ëª¨ë“  íŒŒë¼ë¯¸í„° ê°’ì— ëŒ€í•´ ì¹˜í™˜ ìˆ˜í–‰
        substituted = {}
        try:
            for key, value in params.items():
                try:
                    if isinstance(value, str):
                        substituted[key] = replace_ref(value)
                    elif isinstance(value, dict):
                        substituted[key] = self._substitute_result_refs(value)
                    elif isinstance(value, list):
                        substituted[key] = [
                            replace_ref(v) if isinstance(v, str) else v
                            for v in value
                        ]
                    else:
                        substituted[key] = value
                except Exception as e:
                    self.logger.error(f"Failed to substitute RESULT refs for key '{key}': {e}")
                    raise
        except Exception as e:
            self.logger.error(f"RESULT reference substitution failed: {e}")
            raise

        return substituted

    def _substitute_file_refs(self, params: Dict) -> Dict:
        """íŒŒë¼ë¯¸í„°ì˜ [FILE:*] ì°¸ì¡°ë¥¼ ì‹¤ì œ íŒŒì¼ ë‚´ìš©ìœ¼ë¡œ ì¹˜í™˜"""
        import re
        from pathlib import Path

        BASE_PATH = Path(".") / "workspace" / "files"

        def read_file_content(file_path: str) -> str:
            """íŒŒì¼ ë‚´ìš©ì„ ì½ì–´ì„œ ë°˜í™˜"""
            try:
                # ìƒëŒ€ ê²½ë¡œë¥¼ base_path ê¸°ì¤€ìœ¼ë¡œ í•´ì„
                path = BASE_PATH / file_path

                if not path.exists():
                    raise FileNotFoundError(f"[FILE:{file_path}] - File not found: {path}")
                if not path.is_file():
                    raise ValueError(f"[FILE:{file_path}] - Not a file: {path}")

                # ê²½ë¡œ íƒˆì¶œ ë°©ì§€ (ë³´ì•ˆ)
                try:
                    path.resolve().relative_to(BASE_PATH.resolve())
                except ValueError:
                    raise ValueError(f"[FILE:{file_path}] - Path escape not allowed: {file_path}")

                try:
                    return path.read_text(encoding='utf-8')
                except UnicodeDecodeError as e:
                    raise ValueError(f"[FILE:{file_path}] - Cannot read binary file as text: {path}") from e
            except Exception as e:
                self.logger.error(f"Failed to read file '{file_path}': {e}")
                raise

        def replace_ref(value):
            if not isinstance(value, str):
                return value

            try:
                # [FILE:path] íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: [FILE:src/main.c])
                pattern = r'\[FILE:([^\]]+)\]'
                matches = re.findall(pattern, value)

                for file_path in matches:
                    file_path_stripped = file_path.strip()
                    content = read_file_content(file_path_stripped)

                    # ì „ì²´ ë¬¸ìì—´ì´ FILE REFë§Œ ìˆìœ¼ë©´ contentë¥¼ ì§ì ‘ ì‚¬ìš©
                    if value.strip() == f"[FILE:{file_path}]":
                        return content
                    else:
                        value = value.replace(f"[FILE:{file_path}]", content)

                return value
            except Exception as e:
                self.logger.error(f"FILE reference substitution failed: {e}")
                raise

        # ëª¨ë“  íŒŒë¼ë¯¸í„° ê°’ì— ëŒ€í•´ ì¹˜í™˜ ìˆ˜í–‰
        substituted = {}
        try:
            for key, value in params.items():
                try:
                    if isinstance(value, str):
                        substituted[key] = replace_ref(value)
                    elif isinstance(value, dict):
                        substituted[key] = self._substitute_file_refs(value)
                    elif isinstance(value, list):
                        substituted[key] = [
                            replace_ref(v) if isinstance(v, str)
                            else self._substitute_file_refs(v) if isinstance(v, dict)
                            else v
                            for v in value
                        ]
                    else:
                        substituted[key] = value
                except Exception as e:
                    self.logger.error(f"Failed to substitute FILE refs for key '{key}': {e}")
                    raise
        except Exception as e:
            self.logger.error(f"FILE reference substitution failed: {e}")
            raise

        return substituted


    def _substitute_all_refs(self, params: Dict) -> Dict:
        """ëª¨ë“  REF ([FILE:*], [RESULT:*])ë¥¼ ì¹˜í™˜"""
        try:
            params = self._substitute_file_refs(params)
            params = self._substitute_result_refs(params)
            return params
        except Exception as e:
            self.logger.error(f"Reference substitution failed: {e}")
            raise

    def _parse_plan(self, plan_data: List[Dict]) -> List[PlannedStep]:
        """ê³„íš ë°ì´í„°ë¥¼ PlannedStep ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        planned_steps = []
        if not isinstance(plan_data, list):
            return planned_steps
        
        for item in plan_data:
            if not isinstance(item, dict):
                continue
            try:
                step = PlannedStep(
                    step=item.get("step", len(planned_steps) + 1),
                    tool_name=item.get("tool_name", item.get("tool", "")),
                    action=item.get("action", ""),
                    description=item.get("description", ""),
                    params_hint=item.get("params_hint", item.get("params", {})),
                    status="pending"
                )
                planned_steps.append(step)
            except Exception as e:
                self.logger.warn(f"Step íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        return planned_steps
    
    def _format_plan_summary(self) -> str:
        """ê³„íš ìš”ì•½ ë¬¸ìì—´"""
        if not self._plan:
            return "ê³„íš ì—†ìŒ"
        
        lines = ["ğŸ“‹ **ì‹¤í–‰ ê³„íš**\n"]
        for step in self._plan:
            status_icon = {"pending": "â³", "running": "ğŸ”„", "completed": "âœ…", "failed": "âŒ"}.get(step.status, "â³")
            lines.append(f"{status_icon} Step {step.step}: {step.tool_name}.{step.action}")
            lines.append(f"   â””â”€ {step.description}")
        
        return "\n".join(lines)
    
    def _format_plan_with_status(self) -> str:
        """ìƒíƒœ í¬í•¨ ê³„íš"""
        if not self._plan:
            return "No plan"
        
        lines = []
        for step in self._plan:
            status = {"pending": "[ ]", "running": "[â†’]", "completed": "[âœ“]", "failed": "[âœ—]"}.get(step.status, "[ ]")
            lines.append(f"{status} Step {step.step}: {step.tool_name}.{step.action} - {step.description}")
        
        return "\n".join(lines)
    
    def _format_previous_results(self, results: List[Dict]) -> str:
        """ì´ì „ ê²°ê³¼ í¬ë§·"""
        if not results:
            return "None yet."
        
        formatted = ""
        for r in results:
            step = r.get('step', '?')
            tool = r.get('tool', r.get('executor', 'unknown'))
            action = r.get('action', 'unknown')
            status = "success" if r.get('success', False) == True else "failure"
            output = str(r.get("output", ""))
            error = str(r.get("error", ""))
            
            status_icon = "âœ…" if status == "success" else "âŒ"
            
            formatted += f"""
[Step {step}] {status_icon} {tool}.{action} - {status.upper()}
"""
            if status == "success":
                formatted += f"""Output: {output}"""
            else:
                formatted += f"""Error: {error}"""

        return formatted
    
    def _generate_final_answer(self, user_query: str) -> str:
        """Phase 3: ìµœì¢… ì‘ë‹µ ìƒì„±"""
        results = self.storage.get_results()
        previous_results = self._format_previous_results(results)
        plan_summary = self._format_plan_with_status()

        # PromptBuilder ì‚¬ìš©
        builder = PromptBuilder()
        system_prompt, user_prompt = builder.for_final_answer(
            user_query=user_query,
            plan_summary=plan_summary,
            results_summary=previous_results
        ).build()

        response = self._call_llm_api(system_prompt, user_prompt)
        return response.strip()
    
    # =========================================================================
    # LLM API & Parsing (ë³€ê²½ ì—†ìŒ)
    # =========================================================================
    
    def _call_llm_api(self, system_prompt: str, user_prompt: str) -> str:
        """Ollama Chat API í˜¸ì¶œ (with performance logging)"""
        try:
            import requests
            import time

            url = f"{self.llm_config.base_url}/api/chat"

            # Chat API í˜•ì‹: messages ë°°ì—´ ì‚¬ìš©
            payload = {
                "model": self.llm_config.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": False,
                "options": {
                    "temperature": self.llm_config.temperature,
                    "top_p": self.llm_config.top_p,
                    "top_k": self.llm_config.top_k,
                    "num_predict": self.llm_config.max_tokens,
                    "repeat_penalty": self.llm_config.repeat_penalty,
                    "frequency_penalty": self.llm_config.frequency_penalty,
                    "presence_penalty": self.llm_config.presence_penalty,
                    "num_ctx": self.llm_config.num_ctx
                }
            }

            # Measure performance
            start_time = time.time()
            response = requests.post(url, json=payload, timeout=self.llm_config.timeout)
            response.raise_for_status()
            elapsed_time = time.time() - start_time

            data = response.json()

            # Extract performance metrics from Ollama response
            # Ollama provides: eval_count (output tokens), prompt_eval_count (input tokens),
            # eval_duration (ns), prompt_eval_duration (ns)
            prompt_tokens = data.get("prompt_eval_count", 0)
            output_tokens = data.get("eval_count", 0)
            total_duration_ns = data.get("total_duration", 0)
            eval_duration_ns = data.get("eval_duration", 0)
            done_reson = data.get("done_reason", "none")

            # Calculate metrics
            prompt_length = len(system_prompt) + len(user_prompt)  # Character count
            response_length = len(data.get("message", {}).get("content", ""))  # Character count

            # Tokens per second (based on eval_duration for output tokens)
            tokens_per_sec = 0
            if eval_duration_ns > 0:
                tokens_per_sec = (output_tokens / eval_duration_ns) * 1e9  # Convert ns to seconds

            # TTFT approximation (prompt eval duration)
            ttft_ms = data.get("prompt_eval_duration", 0) / 1e6  # Convert ns to ms

            # Log performance metrics
            self.logger.debug("LLM Performance Metrics", {
                "elapsed_time_sec": round(elapsed_time, 2),  # ì „ì²´ ì™•ë³µ ì‹œê°„
                "server_duration_sec": round(total_duration_ns / 1e9, 2),  # ì„œë²„ ì²˜ë¦¬ ì‹œê°„
                "network_overhead_sec": round(elapsed_time - (total_duration_ns / 1e9), 2),  # ë„¤íŠ¸ì›Œí¬ ì§€ì—°
                "prompt_tokens": prompt_tokens,
                "output_tokens": output_tokens,
                "total_tokens": prompt_tokens + output_tokens,
                "tokens_per_sec": round(tokens_per_sec, 2),
                "ttft_ms": round(ttft_ms, 2),
                "prompt_chars": prompt_length,
                "response_chars": response_length,
                "done_reson" : done_reson,
                "model": self.llm_config.model
            })

            # Chat API ì‘ë‹µ í˜•ì‹: message.content
            content = data.get("message", {}).get("content", "")

            # <think>, <thinking> íƒœê·¸ ì œê±°
            return remove_think_tags(content)

        except ImportError:
            self.logger.warn("requests íŒ¨í‚¤ì§€ ì—†ìŒ - Mock ì‘ë‹µ ë°˜í™˜")
            return self._mock_llm_response()
        except Exception as e:
            self.logger.error(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise

    def _parse_tool_call_response(self, raw_response: str) -> LLMResponse:
        """LLM ì‘ë‹µì—ì„œ tool_callsë¥¼ ì¶”ì¶œí•˜ì—¬ LLMResponse ê°ì²´ë¡œ ë³€í™˜"""
        try:
            data = parse_json_robust(raw_response)

            tool_calls = None
            if data.get("tool_calls"):
                all_known_actions = self._get_known_actions()
                tool_calls = []
                for tc in data["tool_calls"]:
                    tool_name = tc.get("name", "")
                    # tool_nameì—ì„œ "." ì´í›„ ì œê±° (e.g., "file_tool.read" -> "file_tool")
                    base_tool_name = tool_name.split(".")[0] if "." in tool_name else tool_name
                    # í•´ë‹¹ toolì˜ known_actionsë§Œ ê°€ì ¸ì˜¤ê¸°
                    tool_known_actions = all_known_actions.get(base_tool_name, [])
                    tool_calls.append(
                        ToolCall(
                            name=tool_name,
                            arguments=tc.get("arguments", {}),
                            known_actions=tool_known_actions
                        )
                    )

            return LLMResponse(
                thought=data.get("thought"),
                tool_calls=tool_calls,
                content=data.get("content"),
                raw_response=raw_response
            )

        except (ValueError, json.JSONDecodeError) as e:
            self.logger.warn(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return LLMResponse(content=raw_response, raw_response=raw_response)
    
    def _mock_llm_response(self) -> str:
        """í…ŒìŠ¤íŠ¸ìš© Mock ì‘ë‹µ"""
        results = self.storage.get_results()
        
        if not results:
            return json.dumps({
                "thought": "íŒŒì¼ì„ ë¨¼ì € ì½ì–´ì•¼ í•©ë‹ˆë‹¤ (Mock)",
                "tool_calls": [{
                    "name": "file_tool",
                    "arguments": {"action": "read", "read_path": "test.txt"}
                }]
            })
        else:
            return json.dumps({
                "thought": "ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤ (Mock)",
                "tool_calls": None,
                "content": f"Mock ë¶„ì„ ì™„ë£Œ. {len(results)}ê°œì˜ stepì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤."
            })

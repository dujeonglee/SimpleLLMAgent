"""
Orchestrator Module (Updated)
=============================
LLMTool Ïó∞Îèô Î∞è Ïù¥Ï†Ñ Í≤∞Í≥º Ï∞∏Ï°∞ Í∏∞Îä• Ï∂îÍ∞Ä.

Ï£ºÏöî Î≥ÄÍ≤ΩÏÇ¨Ìï≠:
1. LLMTool Ï¥àÍ∏∞Ìôî Ïãú llm_callerÏôÄ previous_result_getter Ï£ºÏûÖ
2. [PREVIOUS_RESULT] ÌîåÎ†àÏù¥Ïä§ÌôÄÎçî ÏûêÎèô ÎåÄÏ≤¥
3. Ïù¥Ï†Ñ Í≤∞Í≥ºÎ•º ÌôúÏö©ÌïòÎäî ÌîÑÎ°¨ÌîÑÌä∏ Í∞úÏÑ†
"""

import json
import re
from dataclasses import dataclass, field, asdict
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Dict, Generator, List, Optional

from core.shared_storage import SharedStorage, DebugLogger
from core.base_tool import ToolRegistry, ToolResult, ActionSchema


# =============================================================================
# Data Classes (Î≥ÄÍ≤Ω ÏóÜÏùå)
# =============================================================================

@dataclass
class LLMConfig:
    """LLM ÎèôÏ†Å ÏÑ§Ï†ï"""
    base_url: str = "http://localhost:11434"
    model: str = "llama3.2"
    timeout: int = 120
    temperature: float = 0.7
    max_tokens: int = 2048
    num_ctx: int = 4096
    top_p: float = 0.9
    repeat_penalty: float = 1.1
    max_steps: int = 10
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    def update(self, **kwargs):
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)


@dataclass
class ToolCall:
    """LLMÏù¥ ÏöîÏ≤≠Ìïú Tool Ìò∏Ï∂ú Ï†ïÎ≥¥"""
    name: str
    arguments: Dict
    known_actions: List[str] = field(default_factory=list)

    def __post_init__(self):
        if "." in self.name:
            parts = self.name.split(".", 1)
            self.name = parts[0]
            if "action" not in self.arguments:
                self.arguments["action"] = parts[1]

        if not self.arguments.get("action"):
            inferred_action = self._infer_action_from_prefix()
            if inferred_action:
                self.arguments["action"] = inferred_action

    def _infer_action_from_prefix(self) -> Optional[str]:
        for key in self.arguments.keys():
            if "_" in key:
                prefix = key.split("_")[0]
                if prefix in self.known_actions:
                    return prefix
        return None
    
    @property
    def action(self) -> str:
        return self.arguments.get("action", "")
    
    @property
    def params(self) -> Dict:
        return {k: v for k, v in self.arguments.items() if k != "action"}


@dataclass
class LLMResponse:
    """LLM ÏùëÎãµ ÌååÏã± Í≤∞Í≥º"""
    thought: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None
    content: Optional[str] = None
    raw_response: str = ""
    
    @property
    def is_final_answer(self) -> bool:
        return self.tool_calls is None and self.content is not None


class StepType(Enum):
    PLANNING = "planning"
    PLAN_READY = "plan_ready"
    PLAN_REVIEW = "plan_review"
    PLAN_REVISE = "plan_revise"
    PLAN_APPROVED = "plan_approved"
    THINKING = "thinking"
    TOOL_CALL = "tool_call"
    TOOL_RESULT = "tool_result"
    FINAL_ANSWER = "final_answer"
    ERROR = "error"


class ChatState(Enum):
    """Chat Ïã§Ìñâ ÏÉÅÌÉú"""
    PLANNING = "planning"
    PLAN_READY = "plan_ready"
    PLAN_REVIEW = "plan_review"
    PLAN_REVISE = "plan_revise"
    PLAN_APPROVED = "plan_approved"
    THINKING = "thinking"
    TOOL_CALL = "tool_call"
    TOOL_RESULT = "tool_result"
    ERROR = "error"
    FINAL_ANSWER = "final_answer"


@dataclass
class PlannedStep:
    """Í≥ÑÌöçÎêú Îã®Í≥Ñ"""
    step: int
    tool_name: str
    action: str
    description: str
    params_hint: Dict = field(default_factory=dict)
    status: str = "pending"
    
    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class StepInfo:
    """Streaming Ï∂úÎ†•Ïö© Step Ï†ïÎ≥¥"""
    type: StepType
    step: int
    content: Any
    tool_name: Optional[str] = None
    action: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def to_dict(self) -> Dict:
        return {
            "type": self.type.value,
            "step": self.step,
            "content": self.content,
            "tool_name": self.tool_name,
            "action": self.action,
            "timestamp": self.timestamp
        }


# =============================================================================
# Orchestrator (Updated)
# =============================================================================

class Orchestrator:
    """
    Multi-Agent Chatbot Orchestrator
    
    Î≥ÄÍ≤ΩÏÇ¨Ìï≠:
    - LLMTool ÏûêÎèô Ïó∞Îèô
    - [PREVIOUS_RESULT] ÌîåÎ†àÏù¥Ïä§ÌôÄÎçî Ï≤òÎ¶¨
    - Ïù¥Ï†Ñ Í≤∞Í≥º Ï∞∏Ï°∞ ÌîÑÎ°¨ÌîÑÌä∏ Í∞úÏÑ†
    """
    
    def __init__(
        self,
        tools: ToolRegistry,
        storage: SharedStorage,
        llm_config: LLMConfig = None,
        max_steps: int = 10,
        on_step_complete: Callable[[StepInfo], None] = None,
        debug_enabled: bool = True
    ):
        self.tools = tools
        self.storage = storage
        self.llm_config = llm_config or LLMConfig()
        self.max_steps = max_steps
        self.on_step_complete = on_step_complete
        self.logger = DebugLogger("Orchestrator", enabled=debug_enabled)

        self._stopped = False
        self._current_step = 0
        self._plan: List[PlannedStep] = []

        # ÏÉÅÌÉú Í¥ÄÎ¶¨
        self._current_state: ChatState = ChatState.PLANNING
        self._user_query: str = ""
        self._context: Dict[str, Any] = {}  # ÏÉÅÌÉú Í∞Ñ Îç∞Ïù¥ÌÑ∞ Ï†ÑÎã¨Ïö©

        # LLMTool Ïó∞Îèô
        self._setup_llm_tool()

        self.logger.info("Orchestrator Ï¥àÍ∏∞Ìôî", {
            "max_steps": max_steps,
            "llm_model": self.llm_config.model,
            "tools": self.tools.list_tools()
        })
    
    def _setup_llm_tool(self):
        """LLMToolÏóê ÌïÑÏöîÌïú Ìï®Ïàò Ï£ºÏûÖ"""
        llm_tool = self.tools.get("llm_tool")
        if llm_tool is None:
            self.logger.debug("llm_toolÏù¥ Îì±Î°ùÎêòÏßÄ ÏïäÏùå")
            return

        # LLMTool ÌÉÄÏûÖ Ï≤¥ÌÅ¨ (duck typing)
        if hasattr(llm_tool, 'set_llm_caller'):
            llm_tool.set_llm_caller(self._call_llm_api)
            self.logger.info("LLMToolÏóê llm_caller Ï£ºÏûÖ ÏôÑÎ£å")

    def _get_known_actions(self) -> Dict[str, List[str]]:
        """Îì±Î°ùÎêú toolÎì§Ïùò action Ï†ïÎ≥¥Î•º ÎèôÏ†ÅÏúºÎ°ú Í∞ÄÏ†∏Ïò§Í∏∞"""
        known_actions = {}
        for tool in self.tools.get_all():
            known_actions[tool.name] = tool.get_action_names()
        self.logger.debug(f"Known actions ÏÉùÏÑ± ÏôÑÎ£å: {known_actions}")
        return known_actions
    
    # =========================================================================
    # Public Methods
    # =========================================================================
    
    def update_llm_config(self, **kwargs):
        """LLM ÏÑ§Ï†ï ÎèôÏ†Å Î≥ÄÍ≤Ω"""
        old_values = {}
        for key in kwargs:
            if hasattr(self.llm_config, key):
                old_values[key] = getattr(self.llm_config, key)
        
        self.llm_config.update(**kwargs)
        
        self.logger.info("LLM ÏÑ§Ï†ï Î≥ÄÍ≤Ω", {
            "changes": {k: {"old": old_values.get(k), "new": v} 
                       for k, v in kwargs.items() if k in old_values}
        })
    
    def stop(self):
        """Ïã§Ìñâ Ï§ëÏßÄ"""
        self._stopped = True
        self.logger.info("Ïã§Ìñâ Ï§ëÏßÄ ÏöîÏ≤≠Îê®")
    
    def run(self, user_query: str) -> str:
        """
        ÎèôÍ∏∞ Ïã§Ìñâ - ÏÉÅÌÉú Î®∏Ïã† Ìå®ÌÑ¥

        PLANNING ‚Üí PLAN_READY ‚Üí PLAN_REVIEW ‚Üí PLAN_APPROVED ‚Üí THINKING ‚Üí TOOL_CALL ‚Üí
        TOOL_RESULT ‚Üí THINKING ‚Üí ... ‚Üí FINAL_ANSWER
        """
        self._stopped = False
        self._current_step = 0
        self._plan = []
        self._user_query = user_query
        self._current_state = ChatState.PLANNING

        session_id = self.storage.start_session(user_query)
        self.logger.info(f"Ïã§Ìñâ ÏãúÏûë: {user_query[:50]}...")

        final_response = ""

        # ÏÉÅÌÉú Î®∏Ïã† Î£®ÌîÑ: FINAL_ANSWER ÏÉÅÌÉúÍ∞Ä Îê† ÎïåÍπåÏßÄ Í≥ÑÏÜç Ïã§Ìñâ
        while self._current_state != ChatState.FINAL_ANSWER:
            if self._stopped:
                final_response = "ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®"
                self.storage.complete_session(final_response=final_response, status="error")
                break

            # ÌòÑÏû¨ ÏÉÅÌÉúÏóê Îî∞Îùº ÏûëÏóÖ ÏàòÌñâÌïòÍ≥† Îã§Ïùå ÏÉÅÌÉúÎ°ú Ï≤úÏù¥
            result = self._process_current_state(user_query)

            # resultÏóêÏÑú Îã§Ïùå ÏÉÅÌÉúÏôÄ Ï∂úÎ†• Ï†ïÎ≥¥ Ï∂îÏ∂ú
            next_state = result.get("next_state")
            output = result.get("output", "")

            # ÏóêÎü¨ Ï≤òÎ¶¨
            if next_state == ChatState.ERROR:
                final_response = output
                self.storage.complete_session(final_response=final_response, status="error")
                break

            # FINAL_ANSWER ÏÉÅÌÉúÏóêÏÑúÎäî ÏµúÏ¢Ö ÏùëÎãµ Ï†ÄÏû•
            if next_state == ChatState.FINAL_ANSWER:
                final_response = output
                self.storage.complete_session(final_response=final_response, status="completed")
                self._current_state = next_state
                break

            # ÏÉÅÌÉú Ï≤úÏù¥
            self._transition_to(next_state)

        return final_response

    def _transition_to(self, new_state: ChatState):
        """ÏÉÅÌÉú Ï≤úÏù¥"""
        old_state = self._current_state
        self._current_state = new_state
        self.logger.info(f"State transition: {old_state.value} ‚Üí {new_state.value}")

    def _process_current_state(self, user_query: str) -> Dict:
        """
        ÌòÑÏû¨ ÏÉÅÌÉúÏóê Îî∞Îùº ÏûëÏóÖÏùÑ ÏàòÌñâÌïòÍ≥† Îã§Ïùå ÏÉÅÌÉúÏôÄ Í≤∞Í≥ºÎ•º Î∞òÌôò

        Returns:
            Dict: {
                "next_state": ChatState,
                "output": Any,
                ... Í∏∞ÌÉÄ Îã§Ïùå ÏÉÅÌÉúÎ°ú Ï†ÑÎã¨Ìï† Îç∞Ïù¥ÌÑ∞Îì§ ...
            }
        """
        # Switch-case Ìå®ÌÑ¥
        result = {}

        if self._current_state == ChatState.PLANNING:
            result = self._handle_planning(user_query)

        elif self._current_state == ChatState.PLAN_READY:
            result = self._handle_plan_ready()

        elif self._current_state == ChatState.PLAN_REVIEW:
            result = self._handle_plan_review()

        elif self._current_state == ChatState.PLAN_REVISE:
            result = self._handle_plan_revise()

        elif self._current_state == ChatState.PLAN_APPROVED:
            result = self._handle_plan_approved()

        elif self._current_state == ChatState.THINKING:
            result = self._handle_thinking(user_query)

        elif self._current_state == ChatState.TOOL_CALL:
            result = self._handle_tool_call()

        elif self._current_state == ChatState.TOOL_RESULT:
            result = self._handle_tool_result()

        elif self._current_state == ChatState.ERROR:
            result = {"next_state": ChatState.ERROR, "output": "Error occurred"}

        else:
            result = {"next_state": ChatState.ERROR, "output": f"Unknown state: {self._current_state}"}

        # Î∞òÌôòÎêú Î™®Îì† Îç∞Ïù¥ÌÑ∞Î•º contextÏóê Ï†ÄÏû• (next_state, output Ï†úÏô∏)
        self._context = {k: v for k, v in result.items() if k not in ["next_state", "output"]}

        return result

    # =========================================================================
    # State Handlers
    # =========================================================================

    def _handle_planning(self, user_query: str) -> Dict:
        """PLANNING ÏÉÅÌÉú Ï≤òÎ¶¨: Í≥ÑÌöç ÏàòÎ¶Ω"""
        self.logger.info("Planning ÏãúÏûë")

        try:
            plan_response = self._create_plan(user_query)

            # Direct answerÏù∏ Í≤ΩÏö∞
            if plan_response.get("direct_answer"):
                return {
                    "next_state": ChatState.FINAL_ANSWER,
                    "output": plan_response["direct_answer"]
                }

            # Plan ÌååÏã±
            self._plan = self._parse_plan(plan_response.get("plan", []))

            if not self._plan:
                return {
                    "next_state": ChatState.FINAL_ANSWER,
                    "output": plan_response.get("thought", "ÏöîÏ≤≠ÏùÑ Ï≤òÎ¶¨Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")
                }

            # Í≥ÑÌöç ÏàòÎ¶Ω ÏôÑÎ£å
            return {
                "next_state": ChatState.PLAN_READY,
                "output": self._format_plan_summary()
            }

        except Exception as e:
            error_msg = f"Í≥ÑÌöç ÏàòÎ¶Ω Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"
            self.logger.error(error_msg)
            return {
                "next_state": ChatState.ERROR,
                "output": error_msg
            }

    def _handle_plan_ready(self) -> Dict:
        """PLAN_READY ÏÉÅÌÉú Ï≤òÎ¶¨: ÏûêÎèô ÏäπÏù∏ ÎòêÎäî Î¶¨Î∑∞ ÏöîÏ≤≠"""
        # Ïó¨Í∏∞ÏÑúÎäî ÏûêÎèô ÏäπÏù∏ÏúºÎ°ú ÏßÑÌñâ (Î¶¨Î∑∞ Í∏∞Îä•ÏùÄ ÎÇòÏ§ëÏóê Ï∂îÍ∞Ä Í∞ÄÎä•)
        return {
            "next_state": ChatState.PLAN_APPROVED,
            "output": "Í≥ÑÌöçÏù¥ ÏäπÏù∏ÎêòÏóàÏäµÎãàÎã§."
        }

    def _handle_plan_review(self) -> Dict:
        """PLAN_REVIEW ÏÉÅÌÉú Ï≤òÎ¶¨: ÏÇ¨Ïö©Ïûê Î¶¨Î∑∞ ÎåÄÍ∏∞ (ÌòÑÏû¨Îäî ÎØ∏Íµ¨ÌòÑ)"""
        # TODO: ÏÇ¨Ïö©Ïûê Î¶¨Î∑∞ ÏûÖÎ†• Î∞õÍ∏∞
        return {
            "next_state": ChatState.PLAN_APPROVED,
            "output": "Î¶¨Î∑∞ ÏôÑÎ£å"
        }

    def _handle_plan_revise(self) -> Dict:
        """PLAN_REVISE ÏÉÅÌÉú Ï≤òÎ¶¨: Í≥ÑÌöç ÏàòÏ†ï (ÌòÑÏû¨Îäî ÎØ∏Íµ¨ÌòÑ)"""
        # TODO: Í≥ÑÌöç ÏàòÏ†ï Î°úÏßÅ
        return {
            "next_state": ChatState.PLAN_READY,
            "output": "Í≥ÑÌöç ÏàòÏ†ï ÏôÑÎ£å"
        }

    def _handle_plan_approved(self) -> Dict:
        """PLAN_APPROVED ÏÉÅÌÉú Ï≤òÎ¶¨: Ïã§Ìñâ ÏãúÏûë"""
        if not self._plan:
            return {
                "next_state": ChatState.FINAL_ANSWER,
                "output": "Ïã§ÌñâÌï† Í≥ÑÌöçÏù¥ ÏóÜÏäµÎãàÎã§."
            }

        # Ï≤´ Î≤àÏß∏ step Ï§ÄÎπÑ
        self._current_step = 0
        return {
            "next_state": ChatState.THINKING,
            "output": "Ïã§Ìñâ ÏãúÏûë"
        }

    def _handle_thinking(self, user_query: str) -> Dict:
        """THINKING ÏÉÅÌÉú Ï≤òÎ¶¨: Îã§Ïùå ÎèÑÍµ¨ Ìò∏Ï∂ú Í≤∞Ï†ï"""
        # Î™®Îì† stepÏùÑ ÏôÑÎ£åÌñàÎäîÏßÄ ÌôïÏù∏
        if self._current_step >= len(self._plan):
            # Î™®Îì† step ÏôÑÎ£å ‚Üí Final Answer ÏÉùÏÑ±
            try:
                final_response = self._generate_final_answer(user_query)
                return {
                    "next_state": ChatState.FINAL_ANSWER,
                    "output": final_response
                }
            except Exception as e:
                return {
                    "next_state": ChatState.ERROR,
                    "output": f"ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}"
                }

        # Îã§Ïùå step Ïã§Ìñâ Ï§ÄÎπÑ
        planned_step = self._plan[self._current_step]
        planned_step.status = "running"

        self.logger.info(f"Step {planned_step.step}: {planned_step.description}")

        try:
            # LLMÏóê ÌååÎùºÎØ∏ÌÑ∞ ÏöîÏ≤≠
            execution_response = self._get_execution_params(user_query, planned_step)

            if not execution_response.tool_calls:
                # Tool callÏù¥ ÏóÜÏúºÎ©¥ ÏóêÎü¨
                error_msg = f"LLM failed to generate tool_call for Step {planned_step.step}"
                self.logger.error(error_msg)
                planned_step.status = "failed"
                # Îã§Ïùå stepÏúºÎ°ú ÏßÑÌñâ (Ïã§Ìå®ÌñàÏßÄÎßå Í≥ÑÏÜç ÏãúÎèÑ)
                self._current_step += 1
                return {
                    "next_state": ChatState.THINKING,
                    "output": error_msg
                }

            # Tool callÏù¥ ÏûàÏúºÎ©¥ TOOL_CALL ÏÉÅÌÉúÎ°ú Ï†ÑÌôòÌïòÎ©¥ÏÑú tool_call Ï†ÑÎã¨
            tool_call = execution_response.tool_calls[0]
            return {
                "next_state": ChatState.TOOL_CALL,
                "output": f"Tool Ìò∏Ï∂ú Ï§ÄÎπÑ: {tool_call.name}.{tool_call.action}",
                "tool_call": tool_call,  # Îã§Ïùå ÏÉÅÌÉúÎ°ú Ï†ÑÎã¨
                "planned_step": planned_step
            }

        except Exception as e:
            error_msg = f"Thinking Îã®Í≥Ñ Ïò§Î•ò: {str(e)}"
            self.logger.error(error_msg)
            return {
                "next_state": ChatState.ERROR,
                "output": error_msg
            }

    def _handle_tool_call(self) -> Dict:
        """TOOL_CALL ÏÉÅÌÉú Ï≤òÎ¶¨: ÎèÑÍµ¨ Ïã§Ìñâ"""
        # contextÏóêÏÑú tool_callÍ≥º planned_step Í∞ÄÏ†∏Ïò§Í∏∞
        tool_call = self._context.get("tool_call")
        planned_step = self._context.get("planned_step")

        if not tool_call or not planned_step:
            return {
                "next_state": ChatState.ERROR,
                "output": "Ïã§ÌñâÌï† tool call Ï†ïÎ≥¥Í∞Ä ÏóÜÏäµÎãàÎã§."
            }

        try:
            # Tool Ïã§Ìñâ (Ïû¨ÏãúÎèÑ Ìè¨Ìï®)
            result = self._execute_tool_with_retry(tool_call, planned_step, max_retries=2)

            # Í≤∞Í≥º Ï†ÄÏû•
            self.storage.add_result(result)
            planned_step.status = "completed" if result.success else "failed"

            return {
                "next_state": ChatState.TOOL_RESULT,
                "output": result.output if result.success else result.error,
                "tool_result": result
            }

        except Exception as e:
            error_msg = f"Tool Ïã§Ìñâ Ïò§Î•ò: {str(e)}"
            self.logger.error(error_msg)
            return {
                "next_state": ChatState.ERROR,
                "output": error_msg
            }

    def _handle_tool_result(self) -> Dict:
        """TOOL_RESULT ÏÉÅÌÉú Ï≤òÎ¶¨: Í≤∞Í≥º Ï≤òÎ¶¨ ÌõÑ Îã§Ïùå stepÏúºÎ°ú"""
        # ÌòÑÏû¨ step Ï¶ùÍ∞Ä
        self._current_step += 1

        # Thinking ÏÉÅÌÉúÎ°ú ÎèåÏïÑÍ∞ÄÏÑú Îã§Ïùå step Ï≤òÎ¶¨
        return {
            "next_state": ChatState.THINKING,
            "output": "Tool Ïã§Ìñâ ÏôÑÎ£å, Îã§Ïùå stepÏúºÎ°ú Ïù¥Îèô"
        }

    def run_stream(self, user_query: str) -> Generator[StepInfo, None, None]:
        """Streaming Ïã§Ìñâ - Plan & Execute Ìå®ÌÑ¥"""
        self._stopped = False
        self._current_step = 0
        self._plan = []
        
        session_id = self.storage.start_session(user_query)
        self.logger.info(f"Ïã§Ìñâ ÏãúÏûë: {user_query[:50]}...")
        
        try:
            yield StepInfo(type=StepType.PLANNING, step=0, content="ÏûëÏóÖ Í≥ÑÌöç ÏàòÎ¶Ω Ï§ë...")
            
            plan_response = self._create_plan(user_query)
            
            if self._stopped:
                self.storage.complete_session(final_response="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®", status="error")
                yield StepInfo(type=StepType.ERROR, step=self._current_step, content="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®")
                return
            
            if plan_response.get("direct_answer"):
                self.storage.complete_session(
                    final_response=plan_response["direct_answer"],
                    status="completed"
                )
                yield StepInfo(type=StepType.FINAL_ANSWER, step=0, content=plan_response["direct_answer"])
                return
            
            self._plan = self._parse_plan(plan_response.get("plan", []))
            
            if not self._plan:
                direct_response = plan_response.get("thought", "ÏöîÏ≤≠ÏùÑ Ï≤òÎ¶¨Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")
                self.storage.complete_session(final_response=direct_response, status="completed")
                yield StepInfo(type=StepType.FINAL_ANSWER, step=0, content=direct_response)
                return
            
            plan_summary = self._format_plan_summary()
            yield StepInfo(type=StepType.PLAN_READY, step=0, content=plan_summary)
            
            # Phase 2: Execution
            for planned_step in self._plan:
                self._current_step = planned_step.step
                planned_step.status = "running"
                
                yield StepInfo(
                    type=StepType.THINKING,
                    step=self._current_step,
                    content=f"Step {self._current_step}: {planned_step.description}"
                )
                
                # LLM Ìò∏Ï∂ú Ï†Ñ Ï§ëÏßÄ Ï≤¥ÌÅ¨
                if self._stopped:
                    self.storage.complete_session(final_response="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®", status="error")
                    yield StepInfo(type=StepType.ERROR, step=self._current_step, content="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®")
                    return
                
                execution_response = self._get_execution_params(user_query, planned_step)
                
                # LLM Ìò∏Ï∂ú ÌõÑ Ï§ëÏßÄ Ï≤¥ÌÅ¨
                if self._stopped:
                    self.storage.complete_session(final_response="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®", status="error")
                    yield StepInfo(type=StepType.ERROR, step=self._current_step, content="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®")
                    return
                
                if execution_response.thought:
                    yield StepInfo(
                        type=StepType.THINKING,
                        step=self._current_step,
                        content=execution_response.thought
                    )

                # Check if tool_calls is empty - this is an error since we expect exactly one tool call per step
                if not execution_response.tool_calls:
                    error_msg = f"LLM failed to generate tool_call for Step {self._current_step} ({planned_step.tool_name}.{planned_step.action}). Expected exactly one tool call."
                    self.logger.error(error_msg, {
                        "step": self._current_step,
                        "planned_tool": planned_step.tool_name,
                        "planned_action": planned_step.action,
                        "llm_thought": execution_response.thought
                    })

                    planned_step.status = "failed"

                    yield StepInfo(
                        type=StepType.ERROR,
                        step=self._current_step,
                        content=f"‚ö†Ô∏è Step {self._current_step} failed: {error_msg}\n\nLLM response: {execution_response.thought or 'No explanation provided'}"
                    )
                    # Continue to next step instead of stopping entirely
                    continue

                if execution_response.tool_calls:
                    for tool_call in execution_response.tool_calls:
                        # Tool Ïã§Ìñâ Ï†Ñ Ï§ëÏßÄ Ï≤¥ÌÅ¨
                        if self._stopped:
                            self.storage.complete_session(final_response="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®", status="error")
                            yield StepInfo(type=StepType.ERROR, step=self._current_step, content="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®")
                            return
                        
                        yield StepInfo(
                            type=StepType.TOOL_CALL,
                            step=self._current_step,
                            content=tool_call.arguments,
                            tool_name=tool_call.name,
                            action=tool_call.action
                        )

                        # Tool Ïã§Ìñâ (ÏûêÎèô Ïû¨ÏãúÎèÑ Ìè¨Ìï®)
                        result = self._execute_tool_with_retry(tool_call, planned_step, max_retries=2)

                        # Tool Ïã§Ìñâ ÌõÑ Ï§ëÏßÄ Ï≤¥ÌÅ¨
                        if self._stopped:
                            self.storage.complete_session(final_response="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®", status="error")
                            yield StepInfo(type=StepType.ERROR, step=self._current_step, content="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®")
                            return

                        # ToolResultÎ•º Í∑∏ÎåÄÎ°ú Ï†ÑÎã¨
                        self.storage.add_result(result)
                        
                        planned_step.status = "completed" if result.success else "failed"
                        
                        yield StepInfo(
                            type=StepType.TOOL_RESULT,
                            step=self._current_step,
                            content=result.output if result.success else result.error,
                            tool_name=tool_call.name,
                            action=tool_call.action
                        )
                    # inner loopÏóêÏÑú Ï§ëÏßÄÎêú Í≤ΩÏö∞
                    if self._stopped:
                        self.storage.complete_session(final_response="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®", status="error")
                        yield StepInfo(type=StepType.ERROR, step=self._current_step, content="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®")
                        return
                if self.on_step_complete:
                    self.on_step_complete(StepInfo(
                        type=StepType.TOOL_RESULT,
                        step=self._current_step,
                        content="Step completed"
                    ))
            
            # Ï§ëÏßÄÎêú Í≤ΩÏö∞ Final Answer ÏÉùÏÑ± Ïä§ÌÇµ
            if self._stopped:
                self.storage.complete_session(final_response="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®", status="error")
                yield StepInfo(type=StepType.ERROR, step=self._current_step, content="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®")
                return
            
            # Phase 3: Final Answer
            yield StepInfo(
                type=StepType.THINKING,
                step=self._current_step + 1,
                content="ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ± Ï§ë..."
            )
            
            if self._stopped:
                self.storage.complete_session(final_response="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®", status="error")
                yield StepInfo(type=StepType.ERROR, step=self._current_step, content="ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏúºÎ°ú Ï∑®ÏÜåÎê®")
                return
            
            final_response = self._generate_final_answer(user_query)
            
            self.storage.complete_session(final_response=final_response, status="completed")
            
            yield StepInfo(type=StepType.FINAL_ANSWER, step=self._current_step + 1, content=final_response)
        
        except Exception as e:
            error_msg = f"Ïã§Ìñâ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"
            self.logger.error(error_msg, {"exception": type(e).__name__})
            self.storage.complete_session(final_response=error_msg, status="error")
            yield StepInfo(type=StepType.ERROR, step=self._current_step, content=error_msg)
    
    # =========================================================================
    # Tool Execution (Updated)
    # =========================================================================

    def _execute_tool_with_retry(
        self,
        tool_call: ToolCall,
        planned_step: PlannedStep,
        max_retries: int = 2
    ) -> ToolResult:
        """
        Tool Ïã§Ìñâ Î∞è Ïã§Ìå® Ïãú ÏûêÎèô Ïû¨ÏãúÎèÑ

        Args:
            tool_call: Ïã§ÌñâÌï† tool call
            planned_step: Í≥ÑÌöçÎêú step Ï†ïÎ≥¥
            max_retries: ÏµúÎåÄ Ïû¨ÏãúÎèÑ ÌöüÏàò (Í∏∞Î≥∏ 2Ìöå)

        Returns:
            ToolResult: ÏµúÏ¢Ö Ïã§Ìñâ Í≤∞Í≥º (ÏÑ±Í≥µ ÎòêÎäî ÏµúÏ¢Ö Ïã§Ìå®)
                       metadataÏóê Ïû¨ÏãúÎèÑ Ï†ïÎ≥¥ Ìè¨Ìï®:
                       - retry_count: Ïû¨ÏãúÎèÑ ÌöüÏàò
                       - retry_history: Í∞Å ÏãúÎèÑÏùò ÏóêÎü¨ Í∏∞Î°ù
                       - corrected_params: ÏàòÏ†ïÎêú ÌååÎùºÎØ∏ÌÑ∞ (Ïû¨ÏãúÎèÑ ÏÑ±Í≥µ Ïãú)
        """
        retry_history = []

        # Ï≤´ ÏãúÎèÑ
        result = self._execute_tool(tool_call)

        if result.success:
            # Ï≤´ ÏãúÎèÑ ÏÑ±Í≥µ - Ïû¨ÏãúÎèÑ Ï†ïÎ≥¥ ÏóÜÏùå
            result.metadata["retry_count"] = 0
            return result

        # Ï≤´ ÏãúÎèÑ Ïã§Ìå® - Ïû¨ÏãúÎèÑ ÏãúÏûë
        retry_history.append({
            "attempt": 0,
            "error": result.error,
            "params": tool_call.params.copy()
        })

        # Ïû¨ÏãúÎèÑ Î£®ÌîÑ
        for retry_count in range(1, max_retries + 1):
            self.logger.info(f"Ïû¨ÏãúÎèÑ {retry_count}/{max_retries}: {tool_call.name}.{tool_call.action}", {
                "previous_error": result.error
            })

            # LLMÏóê ÏóêÎü¨ Ï†ÑÎã¨ÌïòÏó¨ ÏàòÏ†ïÎêú ÌååÎùºÎØ∏ÌÑ∞ ÏöîÏ≤≠
            corrected_call = self._request_parameter_correction(
                original_call=tool_call,
                error_message=result.error,
                planned_step=planned_step,
                retry_count=retry_count
            )

            if not corrected_call:
                # LLMÏù¥ ÏàòÏ†ï Î∂àÍ∞ÄÎä•ÌïòÎã§Í≥† ÌåêÎã®
                self.logger.warn(f"Ïû¨ÏãúÎèÑ Ï§ëÎã®: LLMÏù¥ ÌååÎùºÎØ∏ÌÑ∞ ÏàòÏ†ï Î∂àÍ∞Ä ÌåêÎã®")
                break

            # ÏàòÏ†ïÎêú ÌååÎùºÎØ∏ÌÑ∞Î°ú Ïû¨ÏãúÎèÑ
            result = self._execute_tool(corrected_call)

            retry_history.append({
                "attempt": retry_count,
                "error": result.error if not result.success else None,
                "params": corrected_call.params.copy(),
                "success": result.success
            })

            if result.success:
                # Ïû¨ÏãúÎèÑ ÏÑ±Í≥µ!
                self.logger.info(f"Ïû¨ÏãúÎèÑ ÏÑ±Í≥µ: {retry_count}Î≤àÏß∏ ÏãúÎèÑÏóêÏÑú ÏÑ±Í≥µ")
                result.metadata["retry_count"] = retry_count
                result.metadata["retry_history"] = retry_history
                result.metadata["corrected_params"] = corrected_call.params
                return result

        # Î™®Îì† Ïû¨ÏãúÎèÑ Ïã§Ìå®
        result.metadata["retry_count"] = len(retry_history) - 1
        result.metadata["retry_history"] = retry_history
        result.metadata["retry_exhausted"] = True

        self.logger.error(f"Ïû¨ÏãúÎèÑ Î™®Îëê Ïã§Ìå®: {max_retries}Ìöå ÏãúÎèÑ ÌõÑ Ïã§Ìå®")

        return result

    def _request_parameter_correction(
        self,
        original_call: ToolCall,
        error_message: str,
        planned_step: PlannedStep,
        retry_count: int
    ) -> Optional[ToolCall]:
        """
        LLMÏóê ÏóêÎü¨ Ï†ïÎ≥¥Î•º Ï†ÑÎã¨ÌïòÏó¨ ÏàòÏ†ïÎêú ÌååÎùºÎØ∏ÌÑ∞ ÏöîÏ≤≠

        Args:
            original_call: ÏõêÎ≥∏ tool call
            error_message: Î∞úÏÉùÌïú ÏóêÎü¨ Î©îÏãúÏßÄ
            planned_step: Í≥ÑÌöçÎêú step
            retry_count: ÌòÑÏû¨ Ïû¨ÏãúÎèÑ ÌöüÏàò

        Returns:
            Optional[ToolCall]: ÏàòÏ†ïÎêú tool call (ÏàòÏ†ï Î∂àÍ∞ÄÎä•ÌïòÎ©¥ None)
        """
        # ÌòÑÏû¨ tool Í∞ÄÏ†∏Ïò§Í∏∞
        current_tool = self.tools.get(original_call.name)
        if not current_tool:
            return None

        # ÌòÑÏû¨ actionÏùò schemaÎßå Í∞ÄÏ†∏Ïò§Í∏∞
        action_schema = current_tool.get_action_schema(original_call.action)
        if not action_schema:
            return None

        # action schemaÎ•º JSON ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò
        action_schema_dict = action_schema.to_dict()
        action_schema_json = json.dumps(
            {
                "action": original_call.action,
                **action_schema_dict
            },
            indent=2,
            ensure_ascii=False
        )

        # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Í≤∞Í≥º Î™©Î°ù
        available_results = self._format_available_results()

        system_prompt = f"""You are analyzing a tool execution error and providing corrected parameters.

## Current Step Context
Step {planned_step.step}: {planned_step.description}

## Current Action Schema
Tool: {original_call.name}
Action: {original_call.action}

{action_schema_json}

## Previous Execution Attempt
Parameters Used:
{json.dumps(original_call.params, indent=2, ensure_ascii=False)}

Error Occurred:
{error_message}

## Available Previous Results (for reference)
{available_results}

## Your Task
1. Analyze why the execution failed based on the error message
2. Determine if the error can be fixed by modifying parameters
3. If fixable: provide corrected parameters
4. If NOT fixable (e.g., file doesn't exist, permission denied): return empty tool_calls

## Response Format (JSON only)

If error is fixable with parameter changes:
{{
    "thought": "Analysis: why it failed and how to fix it",
    "tool_calls": [{{
        "name": "{original_call.name}",
        "arguments": {{
            "action": "{original_call.action}",
            ... corrected parameters ...
        }}
    }}]
}}

If error CANNOT be fixed by changing parameters:
{{
    "thought": "Analysis: why this error cannot be fixed with parameter changes",
    "tool_calls": []
}}

## Rules
- You MUST respond with valid JSON only
- Analyze the error carefully before deciding
- Common fixable errors: wrong path format, missing required params, invalid parameter values
- Common unfixable errors: file not found, permission denied, network unreachable
- Use [RESULT:result_id] to reference previous results if helpful"""

        user_prompt = f"""## Retry Attempt {retry_count}

The tool execution failed. Analyze the error and provide corrected parameters if possible.

Respond with JSON only."""

        try:
            raw_response = self._call_llm_api(system_prompt, user_prompt)
            parsed = self._parse_llm_response(raw_response)

            if not parsed.tool_calls:
                self.logger.info(f"LLM ÌåêÎã®: ÌååÎùºÎØ∏ÌÑ∞ ÏàòÏ†ïÏúºÎ°ú Ìï¥Í≤∞ Î∂àÍ∞ÄÎä•", {
                    "step": planned_step.step,
                    "description": planned_step.description,
                    "thought": parsed.thought
                })
                return None

            self.logger.info(f"LLMÏù¥ ÌååÎùºÎØ∏ÌÑ∞ ÏàòÏ†ï Ï†úÏïà", {
                "step": planned_step.step,
                "description": planned_step.description,
                "thought": parsed.thought,
                "corrected_params": parsed.tool_calls[0].params
            })

            return parsed.tool_calls[0]

        except Exception as e:
            self.logger.error(f"ÌååÎùºÎØ∏ÌÑ∞ ÏàòÏ†ï ÏöîÏ≤≠ Ïã§Ìå®: {str(e)}", {
                "step": planned_step.step,
                "description": planned_step.description
            })
            return None

    def _execute_tool(self, tool_call: ToolCall) -> ToolResult:
        """Tool Ïã§Ìñâ - REF ÏûêÎèô ÏπòÌôò

        ÏßÄÏõêÌïòÎäî Ï∞∏Ï°∞ ÌòïÏãù:
        - [RESULT:result_id]: ÌäπÏ†ï result_idÏùò outputÏúºÎ°ú ÏπòÌôò

        Note: Ïù¥ Î©îÏÑúÎìúÎäî Ïû¨ÏãúÎèÑ Î°úÏßÅÏù¥ ÏóÜÎäî Îã®Ïàú Ïã§ÌñâÎßå ÏàòÌñâ.
              Ïû¨ÏãúÎèÑÍ∞Ä ÌïÑÏöîÌïú Í≤ΩÏö∞ _execute_tool_with_retry() ÏÇ¨Ïö©.
        """
        # ÌååÎùºÎØ∏ÌÑ∞ Î≥µÏÇ¨ Î∞è REF ÏπòÌôò
        params = tool_call.params.copy()
        params = self._substitute_result_refs(params)

        self.logger.info(f"Tool Ïã§Ìñâ: {tool_call.name}.{tool_call.action}", {
            "params_keys": list(params.keys())
        })

        result = self.tools.execute(
            tool_name=tool_call.name,
            action=tool_call.action,
            params=params
        )

        # ToolResultÏóê step Ï†ïÎ≥¥ ÏÑ§Ï†ï (ÎÇòÎ®∏ÏßÄÎäî BaseTool.execute()ÏóêÏÑú ÏûêÎèô ÏÑ§Ï†ïÎê®)
        result.step = self._current_step

        return result
    
    # =========================================================================
    # Planning Methods
    # =========================================================================
    
    def _create_plan(self, user_query: str) -> Dict:
        """Phase 1: Ï†ÑÏ≤¥ Ïã§Ìñâ Í≥ÑÌöç ÏàòÎ¶Ω"""
        tools_schema = json.dumps(
            self.tools.get_all_schemas(),
            indent=2,
            ensure_ascii=False
        )
        
        system_prompt = f"""You are a task planning expert. Analyze the user's request and create an execution plan.

## Available Tools
{tools_schema}

## Your Task
1. Analyze what the user wants
2. Decide if tools are needed
3. If yes, create a step-by-step plan
4. If no tools needed, provide direct answer

## IMPORTANT: Data Flow Between Steps
- Each step's output becomes available for the next step with a unique result ID
- Reference previous results using [RESULT:result_id] format in parameters
- Example: Step 1 output ‚Üí [RESULT:result_001], Step 2 output ‚Üí [RESULT:result_002]

## Response Format (JSON only)

If tools are needed:
{{
    "thought": "Analysis of what needs to be done",
    "plan": [
        {{
            "step": 1,
            "tool_name": "file_tool",
            "action": "read",
            "description": "Read the source file"
        }},
        {{
            "step": 2,
            "tool_name": "llm_tool",
            "action": "analyze",
            "description": "Analyze the file content"
        }},
        {{
            "step": 3,
            "tool_name": "file_tool",
            "action": "write",
            "description": "Save analysis result to file"
        }}
    ]
}}

If no tools needed:
{{
    "thought": "This is a simple question I can answer directly",
    "direct_answer": "Your answer here (IMPORTANT: Respond in Korean/ÌïúÍ∏Ä)"
}}

## Rules
- Maximum {self.max_steps} steps
- Use only available tools and actions
- Each step should have clear purpose
- Order steps logically (data dependencies)
- Respond with valid JSON only"""

        user_prompt = f"""## User Request
{user_query}

Create an execution plan or provide direct answer. Respond with JSON only."""

        self.logger.debug("Planning Ìò∏Ï∂ú", {"query": user_query[:50]})
        
        raw_response = self._call_llm_api(system_prompt, user_prompt)

        try:
            parsed = self._extract_json(raw_response)
            self.logger.debug("Plan ÏÉùÏÑ± ÏôÑÎ£å", {
                "has_plan": "plan" in parsed,
                "has_direct_answer": "direct_answer" in parsed
            })
            return parsed
        except Exception as e:
            self.logger.error(f"Plan ÌååÏã± Ïã§Ìå®: {e}")
            return {"direct_answer": f"Í≥ÑÌöç ÏàòÎ¶Ω Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"}
    
    def _get_execution_params(self, user_query: str, planned_step: PlannedStep) -> LLMResponse:
        """
        Í≥ÑÌöçÎêú step Ïã§ÌñâÏùÑ ÏúÑÌïú Ï†ïÌôïÌïú ÌååÎùºÎØ∏ÌÑ∞ Í≤∞Ï†ï

        Note: user_queryÎäî API ÏùºÍ¥ÄÏÑ±ÏùÑ ÏúÑÌï¥ ÌååÎùºÎØ∏ÌÑ∞Î°ú Ïú†ÏßÄÌïòÏßÄÎßå,
              Ïã§Ï†úÎ°úÎäî execution_summaryÏóê Ïù¥ÎØ∏ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏùå
        """
        # ÌòÑÏû¨ stepÏùò tool schemaÎßå Í∞ÄÏ†∏Ïò§Í∏∞
        current_tool = self.tools.get(planned_step.tool_name)
        if not current_tool:
            raise ValueError(f"Tool not found: {planned_step.tool_name}")

        tool_schema = json.dumps(
            current_tool.get_schema(),
            indent=2,
            ensure_ascii=False
        )

        # get_summary()Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ïã§Ìñâ Ïª®ÌÖçÏä§Ìä∏ Í∞ÄÏ†∏Ïò§Í∏∞ (user_query Ìè¨Ìï®)
        execution_summary = self.storage.get_summary(
            include_all_outputs=True,
            include_previous_sessions=False,  # Ïù¥Ï†Ñ ÏÑ∏ÏÖò Ï†úÏô∏ (ÌÜ†ÌÅ∞ Ï†àÏïΩ)
            include_plan=True,
            include_session_info=False,
            max_output_length=300
        )

        # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Í≤∞Í≥º Î™©Î°ù (REF ÌòïÏãù)
        available_results = self._format_available_results()

        # ÌòÑÏû¨ actionÏùò schemaÎ•º Í∏∞Î∞òÏúºÎ°ú response format ÏÉùÏÑ±
        action_schema = current_tool.get_action_schema(planned_step.action)
        response_format = self._generate_response_format(planned_step, action_schema)

        system_prompt = f"""You are executing a planned task. Provide exact parameters for the current step.

## Current Tool Schema
{tool_schema}

## Using Previous Results

You can reference previous step results using: [RESULT:result_id]

Example for llm_tool.staticanalysis:
{{
  "action": "staticanalysis",
  "staticanalysis_content": "[RESULT:result_001]",
  "staticanalysis_instruction": "Find memory leaks"
}}

Example for file_tool.write:
{{
  "action": "write",
  "write_path": "output.md",
  "write_content": "[RESULT:result_002]"
}}

## Available Previous Step Results
{available_results}

## Response Format for Current Step
{response_format}

## Rules
- Execute ONLY the current step (Step {planned_step.step})
- You MUST generate exactly ONE tool_call for this step
- Use parameter names with action prefix (e.g., read_path, write_content)
- Reference previous results using [RESULT:result_id] format when appropriate
- All required parameters must be provided
- Respond with valid JSON only"""

        user_prompt = f"""## Execution Context
{execution_summary}

## Current Step to Execute
Step {planned_step.step}: {planned_step.tool_name}.{planned_step.action}
Description: {planned_step.description}

Provide exact parameters for this step. Respond with JSON only."""

        raw_response = self._call_llm_api(system_prompt, user_prompt)
        return self._parse_llm_response(raw_response)
    
    # =========================================================================
    # Helper Methods
    # =========================================================================

    def _format_available_results(self) -> str:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïù¥Ï†Ñ Í≤∞Í≥ºÎ•º REF ÌòïÏãùÏúºÎ°ú Ìè¨Îß∑"""
        results = self.storage.get_results()
        if not results:
            return "No previous results available."

        lines = []
        for r in results:
            result_dict = r if isinstance(r, dict) else r
            result_id = result_dict.get('result_id', 'unknown')
            step = result_dict.get('step', '?')
            executor = result_dict.get('executor', 'unknown')
            action = result_dict.get('action', 'unknown')
            output = result_dict.get('output', '')

            # Output ÎØ∏Î¶¨Î≥¥Í∏∞
            if isinstance(output, str) and len(output) > 100:
                preview = output[:100] + "..."
            else:
                preview = str(output)[:100]

            lines.append(
                f"- [RESULT:{result_id}]: Step {step} ({executor}.{action})\n"
                f"  Preview: {preview}"
            )

        return "\n".join(lines)

    def _generate_response_format(self, planned_step: PlannedStep, action_schema: Optional[ActionSchema]) -> str:
        """
        Generate response format template based on the action schema

        Args:
            planned_step: The planned step to execute
            action_schema: The ActionSchema for the current action

        Returns:
            str: JSON template showing the expected response format
        """
        if not action_schema:
            # Fallback to generic format if schema not found
            return f"""{{
    "thought": "Brief explanation of parameter choices",
    "tool_calls": [
        {{
            "name": "{planned_step.tool_name}",
            "arguments": {{
                "action": "{planned_step.action}",
                ... parameters ...
            }}
        }}
    ]
}}"""

        # Build parameter examples from schema
        param_examples = {}
        param_examples["action"] = planned_step.action

        for param in action_schema.params:
            param_name = param.name
            param_desc = param.description

            # Generate example value based on description
            if "path" in param_name.lower() or "file" in param_name.lower():
                example_value = '"example.txt"'
            elif "content" in param_name.lower():
                example_value = '"[RESULT:result_xxx]" or "actual content"'
            elif param.param_type == "int":
                example_value = "100"
            elif param.param_type == "bool":
                example_value = "true"
            else:
                example_value = f'"<{param_desc[:50]}>"' if param_desc else '"example value"'

            # Mark required parameters
            if param.required:
                param_examples[param_name] = f"{example_value}  // REQUIRED"
            else:
                param_examples[param_name] = f"{example_value}  // optional"

        # Format as JSON with comments
        param_lines = []
        for key, value in param_examples.items():
            param_lines.append(f'                "{key}": {value}')

        params_str = ",\n".join(param_lines)

        return f"""{{
    "thought": "Brief explanation of why these specific parameters are needed for this step",
    "tool_calls": [
        {{
            "name": "{planned_step.tool_name}",
            "arguments": {{
{params_str}
            }}
        }}
    ]
}}"""

    def _substitute_result_refs(self, params: Dict) -> Dict:
        """ÌååÎùºÎØ∏ÌÑ∞Ïùò [RESULT:*] Ï∞∏Ï°∞Î•º Ïã§Ï†ú Í∞íÏúºÎ°ú ÏπòÌôò"""
        import re

        def replace_ref(value):
            if not isinstance(value, str):
                return value

            # [RESULT:result_id] Ìå®ÌÑ¥ Ï∞æÍ∏∞
            pattern = r'\[RESULT:([^\]]+)\]'
            matches = re.findall(pattern, value)

            for result_id in matches:
                output = self.storage.get_output_by_id(result_id)
                if output is not None:
                    # Ï†ÑÏ≤¥ Î¨∏ÏûêÏó¥Ïù¥ REFÎßå ÏûàÏúºÎ©¥ outputÏùÑ ÏßÅÏ†ë ÏÇ¨Ïö©, ÏïÑÎãàÎ©¥ Î¨∏ÏûêÏó¥ ÏπòÌôò
                    if value.strip() == f"[RESULT:{result_id}]":
                        return output
                    else:
                        value = value.replace(f"[RESULT:{result_id}]", str(output))

            return value

        # Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞ Í∞íÏóê ÎåÄÌï¥ ÏπòÌôò ÏàòÌñâ
        substituted = {}
        for key, value in params.items():
            if isinstance(value, str):
                substituted[key] = replace_ref(value)
            elif isinstance(value, dict):
                substituted[key] = self._substitute_result_refs(value)
            elif isinstance(value, list):
                substituted[key] = [replace_ref(v) if isinstance(v, str) else v for v in value]
            else:
                substituted[key] = value

        return substituted

    def _parse_plan(self, plan_data: List[Dict]) -> List[PlannedStep]:
        """Í≥ÑÌöç Îç∞Ïù¥ÌÑ∞Î•º PlannedStep Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôò"""
        planned_steps = []
        if not isinstance(plan_data, list):
            return planned_steps
        
        for item in plan_data:
            if not isinstance(item, dict):
                continue
            try:
                step = PlannedStep(
                    step=item.get("step", len(planned_steps) + 1),
                    tool_name=item.get("tool_name", item.get("tool", "")),
                    action=item.get("action", ""),
                    description=item.get("description", ""),
                    params_hint=item.get("params_hint", item.get("params", {})),
                    status="pending"
                )
                planned_steps.append(step)
            except Exception as e:
                self.logger.warn(f"Step ÌååÏã± Ïã§Ìå®: {e}")
                continue
        
        return planned_steps
    
    def _format_plan_summary(self) -> str:
        """Í≥ÑÌöç ÏöîÏïΩ Î¨∏ÏûêÏó¥"""
        if not self._plan:
            return "Í≥ÑÌöç ÏóÜÏùå"
        
        lines = ["üìã **Ïã§Ìñâ Í≥ÑÌöç**\n"]
        for step in self._plan:
            status_icon = {"pending": "‚è≥", "running": "üîÑ", "completed": "‚úÖ", "failed": "‚ùå"}.get(step.status, "‚è≥")
            lines.append(f"{status_icon} Step {step.step}: {step.tool_name}.{step.action}")
            lines.append(f"   ‚îî‚îÄ {step.description}")
        
        return "\n".join(lines)
    
    def _format_plan_with_status(self) -> str:
        """ÏÉÅÌÉú Ìè¨Ìï® Í≥ÑÌöç"""
        if not self._plan:
            return "No plan"
        
        lines = []
        for step in self._plan:
            status = {"pending": "[ ]", "running": "[‚Üí]", "completed": "[‚úì]", "failed": "[‚úó]"}.get(step.status, "[ ]")
            lines.append(f"{status} Step {step.step}: {step.tool_name}.{step.action} - {step.description}")
        
        return "\n".join(lines)
    
    def _format_previous_results(self, results: List[Dict]) -> str:
        """Ïù¥Ï†Ñ Í≤∞Í≥º Ìè¨Îß∑"""
        if not results:
            return "None yet."
        
        formatted = ""
        for r in results:
            step = r.get('step', '?')
            tool = r.get('tool', r.get('executor', 'unknown'))
            action = r.get('action', 'unknown')
            status = "success" if r.get('success', False) == True else "failure"
            output = str(r.get("output", ""))
            error = str(r.get("error", ""))
            
            # Ï∂úÎ†• Í∏∏Ïù¥ Ï†úÌïú
            if len(output) > 100:
                output = output[:100] + "... (truncated)"
            
            status_icon = "‚úÖ" if status == "success" else "‚ùå"
            
            formatted += f"""
[Step {step}] {status_icon} {tool}.{action} - {status.upper()}
"""
            if status == "success":
                formatted += f"""Output: {output}"""
            else:
                formatted += f"""Error: {error}"""

        return formatted
    
    def _generate_final_answer(self, user_query: str) -> str:
        """Phase 3: ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ±"""
        results = self.storage.get_results()
        previous_results = self._format_previous_results(results)
        plan_status = self._format_plan_with_status()

        system_prompt = """You are generating the final response based on executed tasks.

## Your Task
1. Review what was done (plan execution results)
2. Summarize the findings
3. Provide a helpful, complete answer to the user

## Rules
- Be concise but thorough
- Reference specific results when relevant
- If any step failed, mention it and provide alternatives if possible
- Respond naturally, not in JSON format
- If the task was to save a file, confirm it was saved successfully
- IMPORTANT: Respond in Korean (ÌïúÍ∏ÄÎ°ú ÎãµÎ≥ÄÌïòÏÑ∏Ïöî)"""

        user_prompt = f"""## Original User Query
{user_query}

## Execution Plan
{plan_status}

## Execution Results
{previous_results}

Based on the above results, provide a final answer to the user's query."""

        response = self._call_llm_api(system_prompt, user_prompt)
        return response.strip()
    
    def _is_complete(self) -> bool:
        if self._stopped:
            return True
        if self._current_step >= self.max_steps:
            return True
        return False
    
    # =========================================================================
    # LLM API & Parsing (Î≥ÄÍ≤Ω ÏóÜÏùå)
    # =========================================================================
    
    def _call_llm_api(self, system_prompt: str, user_prompt: str) -> str:
        """Ollama Chat API Ìò∏Ï∂ú"""
        try:
            import requests

            url = f"{self.llm_config.base_url}/api/chat"

            # Chat API ÌòïÏãù: messages Î∞∞Ïó¥ ÏÇ¨Ïö©
            payload = {
                "model": self.llm_config.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": False,
                "options": {
                    "temperature": self.llm_config.temperature,
                    "top_p": self.llm_config.top_p,
                    "num_predict": self.llm_config.max_tokens,
                    "repeat_penalty": self.llm_config.repeat_penalty,
                    "num_ctx": self.llm_config.num_ctx
                }
            }

            response = requests.post(url, json=payload, timeout=self.llm_config.timeout)
            response.raise_for_status()

            data = response.json()
            # Chat API ÏùëÎãµ ÌòïÏãù: message.content
            return data.get("message", {}).get("content", "")

        except ImportError:
            self.logger.warn("requests Ìå®ÌÇ§ÏßÄ ÏóÜÏùå - Mock ÏùëÎãµ Î∞òÌôò")
            return self._mock_llm_response()
        except Exception as e:
            self.logger.error(f"LLM API Ìò∏Ï∂ú Ïã§Ìå®: {str(e)}")
            raise
    
    def _extract_json(self, text: str) -> Dict:
        """ÌÖçÏä§Ìä∏ÏóêÏÑú JSON Ï∂îÏ∂ú"""
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', text)
        if json_match:
            return json.loads(json_match.group(1))
        
        json_match = re.search(r'\{[\s\S]*\}', text)
        if json_match:
            return json.loads(json_match.group(0))
        
        raise ValueError("JSON not found in response")
    
    def _extract_json_str(self, text: str) -> Optional[str]:
        """Í¥ÑÌò∏ Îß§Ïπ≠ÏúºÎ°ú JSON Ï∂îÏ∂ú"""
        start = text.find('{')
        if start == -1:
            return None
        
        depth = 0
        in_string = False
        escape = False
        
        for i, char in enumerate(text[start:], start):
            if escape:
                escape = False
                continue
            if char == '\\':
                escape = True
                continue
            if char == '"' and not escape:
                in_string = not in_string
                continue
            if in_string:
                continue
            if char == '{':
                depth += 1
            elif char == '}':
                depth -= 1
                if depth == 0:
                    return text[start:i+1]
        
        return None
    
    def _fix_triple_quotes(self, text: str) -> str:
        """ÏÇºÏ§ë Îî∞Ïò¥Ìëú Ï≤òÎ¶¨"""
        def replace_triple_quotes(match):
            content = match.group(1)
            content = content.replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
            content = content.replace('"', '\\"')
            return f'"{content}"'
        
        text = re.sub(r'"""(.*?)"""', replace_triple_quotes, text, flags=re.DOTALL)
        text = re.sub(r"'''(.*?)'''", replace_triple_quotes, text, flags=re.DOTALL)
        return text
    
    def _sanitize_json_string(self, text: str) -> str:
        """JSON Î¨∏ÏûêÏó¥ Ï†ïÎ¶¨"""
        text = self._fix_triple_quotes(text)
        
        result = []
        in_string = False
        escape = False
        
        for char in text:
            if escape:
                result.append(char)
                escape = False
                continue
            if char == '\\':
                result.append(char)
                escape = True
                continue
            if char == '"':
                in_string = not in_string
                result.append(char)
                continue
            
            if in_string:
                if char == '\n':
                    result.append('\\n')
                elif char == '\r':
                    result.append('\\r')
                elif char == '\t':
                    result.append('\\t')
                elif ord(char) < 32:
                    result.append(f'\\u{ord(char):04x}')
                else:
                    result.append(char)
            else:
                result.append(char)
        
        return ''.join(result)
    
    def _parse_llm_response(self, raw_response: str) -> LLMResponse:
        """LLM ÏùëÎãµ ÌååÏã±"""
        try:
            sanitized = self._sanitize_json_string(raw_response)
            json_str = self._extract_json_str(sanitized)

            if not json_str:
                return LLMResponse(content=raw_response, raw_response=raw_response)

            data = json.loads(json_str)

            tool_calls = None
            if data.get("tool_calls"):
                all_known_actions = self._get_known_actions()
                tool_calls = []
                for tc in data["tool_calls"]:
                    tool_name = tc.get("name", "")
                    # tool_nameÏóêÏÑú "." Ïù¥ÌõÑ Ï†úÍ±∞ (e.g., "file_tool.read" -> "file_tool")
                    base_tool_name = tool_name.split(".")[0] if "." in tool_name else tool_name
                    # Ìï¥Îãπ toolÏùò known_actionsÎßå Í∞ÄÏ†∏Ïò§Í∏∞
                    tool_known_actions = all_known_actions.get(base_tool_name, [])
                    tool_calls.append(
                        ToolCall(
                            name=tool_name,
                            arguments=tc.get("arguments", {}),
                            known_actions=tool_known_actions
                        )
                    )

            return LLMResponse(
                thought=data.get("thought"),
                tool_calls=tool_calls,
                content=data.get("content"),
                raw_response=raw_response
            )

        except json.JSONDecodeError as e:
            self.logger.warn(f"JSON ÌååÏã± Ïã§Ìå®: {str(e)}")
            return LLMResponse(content=raw_response, raw_response=raw_response)
    
    def _generate_partial_response(self) -> str:
        """ÏµúÎåÄ step ÎèÑÎã¨ Ïãú Î∂ÄÎ∂Ñ ÏùëÎãµ"""
        results = self.storage.get_results()
        
        if not results:
            return "ÏûëÏóÖÏùÑ ÏôÑÎ£åÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§."
        
        last_result = results[-1]
        return f"""ÏµúÎåÄ Ïã§Ìñâ Îã®Í≥Ñ({self.max_steps})Ïóê ÎèÑÎã¨ÌñàÏäµÎãàÎã§.

ÏßÄÍ∏àÍπåÏßÄÏùò ÏßÑÌñâ ÏÉÅÌô©:
- Ï¥ù {len(results)}Í∞úÏùò ÏûëÏóÖ ÏàòÌñâ
- ÎßàÏßÄÎßâ ÏûëÏóÖ: {last_result['executor']}.{last_result['action']}

Î∂ÄÎ∂Ñ Í≤∞Í≥º:
{str(last_result.get('output', ''))[:500]}"""
    
    def _mock_llm_response(self) -> str:
        """ÌÖåÏä§Ìä∏Ïö© Mock ÏùëÎãµ"""
        results = self.storage.get_results()
        
        if not results:
            return json.dumps({
                "thought": "ÌååÏùºÏùÑ Î®ºÏ†Ä ÏùΩÏñ¥Ïïº Ìï©ÎãàÎã§ (Mock)",
                "tool_calls": [{
                    "name": "file_tool",
                    "arguments": {"action": "read", "read_path": "test.txt"}
                }]
            })
        else:
            return json.dumps({
                "thought": "Ï∂©Î∂ÑÌïú Ï†ïÎ≥¥Î•º ÏñªÏóàÏäµÎãàÎã§ (Mock)",
                "tool_calls": None,
                "content": f"Mock Î∂ÑÏÑù ÏôÑÎ£å. {len(results)}Í∞úÏùò stepÏùÑ ÏàòÌñâÌñàÏäµÎãàÎã§."
            })

"""
Orchestrator Module
===================
Multi-Agent Chatbotì˜ ë‘ë‡Œ ì—­í• .
ReAct íŒ¨í„´ìœ¼ë¡œ Toolë“¤ì„ ì¡°ìœ¨í•˜ê³  ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

ì„¤ê³„ ê²°ì •:
- ReAct (ë‹¨ê³„ë³„ ê²°ì •): ë§¤ stepë§ˆë‹¤ LLMì´ ë‹¤ìŒ í–‰ë™ ê²°ì •
- Function Calling ìŠ¤íƒ€ì¼: tool_calls í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
- Streaming ì§€ì›: ì‹¤ì‹œê°„ ê²°ê³¼ ì¶œë ¥
- ë™ì  LLM ì„¤ì •: ëª¨ë¸/íŒŒë¼ë¯¸í„° ì‹¤ì‹œê°„ ë³€ê²½ ê°€ëŠ¥
"""

import json
import re
from dataclasses import dataclass, field, asdict
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Dict, Generator, List, Optional

from .shared_storage import SharedStorage, DebugLogger
from .base_tool import ToolRegistry, ToolResult


# =============================================================================
# Data Classes
# =============================================================================

@dataclass
class LLMConfig:
    """LLM ë™ì  ì„¤ì • - UIì—ì„œ ì‹¤ì‹œê°„ ë³€ê²½ ê°€ëŠ¥"""
    
    # ëª¨ë¸ ì„ íƒ
    model: str = "llama3.2"
    
    # ìƒì„± íŒŒë¼ë¯¸í„°
    temperature: float = 0.7
    top_p: float = 0.9
    max_tokens: int = 2048
    
    # ì—°ê²° ì„¤ì •
    base_url: str = "http://localhost:11434"
    timeout: int = 120
    
    # ê³ ê¸‰ ì˜µì…˜
    repeat_penalty: float = 1.1
    num_ctx: int = 4096
    max_steps: int = 10
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    def update(self, **kwargs):
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)


@dataclass
class ToolCall:
    """LLMì´ ìš”ì²­í•œ Tool í˜¸ì¶œ ì •ë³´"""
    name: str           # tool ì´ë¦„ (ì˜ˆ: file_tool)
    arguments: Dict     # action + params
    
    # Toolë³„ ì•Œë ¤ì§„ action ëª©ë¡ (prefix ì¶”ë¡ ìš©)
    KNOWN_ACTIONS = {
        "file_tool": ["read", "write", "append", "delete", "exists", "list_dir"],
        "web_tool": ["search", "fetch"],
    }
    
    def __post_init__(self):
        """nameì— actionì´ í¬í•¨ëœ ê²½ìš° ë¶„ë¦¬, action ì—†ìœ¼ë©´ ì¶”ë¡ """
        # 1. nameì— actionì´ í¬í•¨ëœ ê²½ìš° (ì˜ˆ: llm_tool.ask -> llm_tool, ask)
        if "." in self.name:
            parts = self.name.split(".", 1)
            self.name = parts[0]
            if "action" not in self.arguments:
                self.arguments["action"] = parts[1]
        
        # 2. actionì´ ì—¬ì „íˆ ì—†ìœ¼ë©´ íŒŒë¼ë¯¸í„° prefixì—ì„œ ì¶”ë¡ 
        if not self.arguments.get("action"):
            inferred_action = self._infer_action_from_prefix()
            if inferred_action:
                self.arguments["action"] = inferred_action
    
    def _infer_action_from_prefix(self) -> Optional[str]:
        """íŒŒë¼ë¯¸í„° prefixì—ì„œ action ì¶”ë¡ 
        
        ì˜ˆ: read_path -> read, write_content -> write
        """
        known_actions = self.KNOWN_ACTIONS.get(self.name, [])
        
        for key in self.arguments.keys():
            if "_" in key:
                prefix = key.split("_")[0]
                if prefix in known_actions:
                    return prefix
        
        return None
    
    @property
    def action(self) -> str:
        return self.arguments.get("action", "")
    
    @property
    def params(self) -> Dict:
        """actionì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°"""
        return {k: v for k, v in self.arguments.items() if k != "action"}


@dataclass
class LLMResponse:
    """LLM ì‘ë‹µ íŒŒì‹± ê²°ê³¼"""
    thought: Optional[str] = None      # LLMì˜ ì‚¬ê³  ê³¼ì •
    tool_calls: Optional[List[ToolCall]] = None  # Tool í˜¸ì¶œ ìš”ì²­
    content: Optional[str] = None      # ìµœì¢… ë‹µë³€ (tool_callsê°€ Noneì¼ ë•Œ)
    raw_response: str = ""             # ì›ë³¸ ì‘ë‹µ
    
    @property
    def is_final_answer(self) -> bool:
        """ìµœì¢… ë‹µë³€ì¸ì§€ ì—¬ë¶€"""
        return self.tool_calls is None and self.content is not None


class StepType(Enum):
    """Step ìœ í˜•"""
    PLANNING = "planning"       # ê³„íš ìˆ˜ë¦½ ì¤‘
    PLAN_READY = "plan_ready"   # ê³„íš ì™„ë£Œ
    THINKING = "thinking"
    TOOL_CALL = "tool_call"
    TOOL_RESULT = "tool_result"
    FINAL_ANSWER = "final_answer"
    ERROR = "error"


@dataclass
class PlannedStep:
    """ê³„íšëœ ë‹¨ê³„"""
    step: int
    tool_name: str
    action: str
    description: str
    params_hint: Dict = field(default_factory=dict)
    status: str = "pending"  # pending, running, completed, failed
    
    def to_dict(self) -> Dict:
        return {
            "step": self.step,
            "tool_name": self.tool_name,
            "action": self.action,
            "description": self.description,
            "params_hint": self.params_hint,
            "status": self.status
        }


@dataclass
class StepInfo:
    """Streaming ì¶œë ¥ìš© Step ì •ë³´"""
    type: StepType
    step: int
    content: Any
    tool_name: Optional[str] = None
    action: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def to_dict(self) -> Dict:
        return {
            "type": self.type.value,
            "step": self.step,
            "content": self.content,
            "tool_name": self.tool_name,
            "action": self.action,
            "timestamp": self.timestamp
        }


# =============================================================================
# Orchestrator
# =============================================================================

class Orchestrator:
    """
    Multi-Agent Chatbot Orchestrator
    
    Plan & Execute íŒ¨í„´ìœ¼ë¡œ ë™ì‘:
    1. Planning: LLMì´ ì „ì²´ ê³„íš ìˆ˜ë¦½ (PlannedStep ë°°ì—´)
    2. Execution: ê³„íšì— ë”°ë¼ Tool ì‹¤í–‰
    3. Final Answer: ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
    """
    
    def __init__(
        self,
        tools: ToolRegistry,
        storage: SharedStorage,
        llm_config: LLMConfig = None,
        max_steps: int = 10,
        on_step_complete: Callable[[StepInfo], None] = None,
        debug_enabled: bool = True
    ):
        self.tools = tools
        self.storage = storage
        self.llm_config = llm_config or LLMConfig()
        self.max_steps = max_steps
        self.on_step_complete = on_step_complete
        self.logger = DebugLogger("Orchestrator", enabled=debug_enabled)
        
        self._stopped = False
        self._current_step = 0
        self._plan: List[PlannedStep] = []  # ì‹¤í–‰ ê³„íš
        
        self.logger.info("Orchestrator ì´ˆê¸°í™”", {
            "max_steps": max_steps,
            "llm_model": self.llm_config.model,
            "tools": self.tools.list_tools()
        })
    
    # =========================================================================
    # Public Methods
    # =========================================================================
    
    def update_llm_config(self, **kwargs):
        """LLM ì„¤ì • ë™ì  ë³€ê²½ (UIì—ì„œ í˜¸ì¶œ)"""
        old_values = {}
        for key in kwargs:
            if hasattr(self.llm_config, key):
                old_values[key] = getattr(self.llm_config, key)
        
        self.llm_config.update(**kwargs)
        
        self.logger.info("LLM ì„¤ì • ë³€ê²½", {
            "changes": {k: {"old": old_values.get(k), "new": v} 
                       for k, v in kwargs.items() if k in old_values}
        })
    
    def stop(self):
        """ì‹¤í–‰ ì¤‘ì§€"""
        self._stopped = True
        self.logger.info("ì‹¤í–‰ ì¤‘ì§€ ìš”ì²­ë¨")
    
    def run(self, user_query: str) -> str:
        """
        ë™ê¸° ì‹¤í–‰ - ìµœì¢… ì‘ë‹µë§Œ ë°˜í™˜
        
        Args:
            user_query: ì‚¬ìš©ì ì¿¼ë¦¬
            
        Returns:
            str: ìµœì¢… ì‘ë‹µ
        """
        final_response = ""
        for step_info in self.run_stream(user_query):
            if step_info.type == StepType.FINAL_ANSWER:
                final_response = step_info.content
        
        return final_response
    
    def run_stream(self, user_query: str) -> Generator[StepInfo, None, None]:
        """
        Streaming ì‹¤í–‰ - Plan & Execute íŒ¨í„´
        
        Phase 1: Planning - ì „ì²´ ê³„íš ìˆ˜ë¦½
        Phase 2: Execution - ê³„íšì— ë”°ë¼ Tool ì‹¤í–‰
        Phase 3: Final Answer - ê²°ê³¼ ê¸°ë°˜ ìµœì¢… ì‘ë‹µ
        
        Args:
            user_query: ì‚¬ìš©ì ì¿¼ë¦¬
            
        Yields:
            StepInfo: ê° stepì˜ ì •ë³´
        """
        self._stopped = False
        self._current_step = 0
        self._plan = []
        
        # ì„¸ì…˜ ì‹œì‘
        session_id = self.storage.start_session(user_query)
        self.logger.info(f"ì‹¤í–‰ ì‹œì‘: {user_query[:50]}...")
        
        try:
            # =====================================================
            # Phase 1: Planning
            # =====================================================
            yield StepInfo(
                type=StepType.PLANNING,
                step=0,
                content="ì‘ì—… ê³„íš ìˆ˜ë¦½ ì¤‘..."
            )
            
            plan_response = self._create_plan(user_query)
            if plan_response.get("direct_answer"):
                # Tool ì—†ì´ ë°”ë¡œ ë‹µë³€ ê°€ëŠ¥í•œ ê²½ìš°
                self.storage.complete_session(
                    final_response=plan_response["direct_answer"],
                    status="completed"
                )
                yield StepInfo(
                    type=StepType.FINAL_ANSWER,
                    step=0,
                    content=plan_response["direct_answer"]
                )
                return
            
            # ê³„íš íŒŒì‹±
            self._plan = self._parse_plan(plan_response.get("plan", []))

            if not self._plan:
                # ê³„íšì´ ì—†ìœ¼ë©´ ì¼ë°˜ ëŒ€í™”ë¡œ ì²˜ë¦¬
                direct_response = plan_response.get("thought", "ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.storage.complete_session(
                    final_response=direct_response,
                    status="completed"
                )
                yield StepInfo(
                    type=StepType.FINAL_ANSWER,
                    step=0,
                    content=direct_response
                )
                return
            
            # ê³„íš ì •ë³´ ì¶œë ¥
            plan_summary = self._format_plan_summary()

            yield StepInfo(
                type=StepType.PLAN_READY,
                step=0,
                content=plan_summary
            )
            
            # =====================================================
            # Phase 2: Execution
            # =====================================================
            for planned_step in self._plan:
                if self._stopped:
                    break
                    
                self._current_step = planned_step.step
                planned_step.status = "running"
                
                # í˜„ì¬ ì‹¤í–‰í•  stepì— ëŒ€í•œ ìƒì„¸ íŒŒë¼ë¯¸í„° ê²°ì •
                yield StepInfo(
                    type=StepType.THINKING,
                    step=self._current_step,
                    content=f"Step {self._current_step}: {planned_step.description}"
                )
                
                # LLMì—ê²Œ ì •í™•í•œ íŒŒë¼ë¯¸í„° ìš”ì²­
                execution_response = self._get_execution_params(user_query, planned_step)
                
                if execution_response.thought:
                    yield StepInfo(
                        type=StepType.THINKING,
                        step=self._current_step,
                        content=execution_response.thought
                    )
                
                # Tool í˜¸ì¶œ ì‹¤í–‰
                if execution_response.tool_calls:
                    for tool_call in execution_response.tool_calls:
                        # Tool í˜¸ì¶œ ì•Œë¦¼
                        yield StepInfo(
                            type=StepType.TOOL_CALL,
                            step=self._current_step,
                            content=tool_call.arguments,
                            tool_name=tool_call.name,
                            action=tool_call.action
                        )
                        
                        # Tool ì‹¤í–‰
                        result = self._execute_tool(tool_call)
                        
                        # ê²°ê³¼ ì €ì¥
                        self.storage.add_result(
                            executor=tool_call.name,
                            executor_type="tool",
                            action=tool_call.action,
                            input_data=tool_call.params,
                            output=result.output,
                            status="success" if result.success else "error",
                            error_message=result.error
                        )
                        
                        # ìƒíƒœ ì—…ë°ì´íŠ¸
                        planned_step.status = "completed" if result.success else "failed"
                        
                        # ê²°ê³¼ ì¶œë ¥
                        yield StepInfo(
                            type=StepType.TOOL_RESULT,
                            step=self._current_step,
                            content=result.output if result.success else result.error,
                            tool_name=tool_call.name,
                            action=tool_call.action
                        )
                
                # Callback í˜¸ì¶œ
                if self.on_step_complete:
                    self.on_step_complete(StepInfo(
                        type=StepType.TOOL_RESULT,
                        step=self._current_step,
                        content="Step completed"
                    ))
            
            # =====================================================
            # Phase 3: Final Answer
            # =====================================================
            yield StepInfo(
                type=StepType.THINKING,
                step=self._current_step + 1,
                content="ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘..."
            )
            
            final_response = self._generate_final_answer(user_query)
            
            self.storage.complete_session(
                final_response=final_response,
                status="completed"
            )
            
            yield StepInfo(
                type=StepType.FINAL_ANSWER,
                step=self._current_step + 1,
                content=final_response
            )
        
        except Exception as e:
            error_msg = f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            self.logger.error(error_msg, {"exception": type(e).__name__})
            
            self.storage.complete_session(
                final_response=error_msg,
                status="error"
            )
            
            yield StepInfo(
                type=StepType.ERROR,
                step=self._current_step,
                content=error_msg
            )
    
    # =========================================================================
    # Private Methods
    # =========================================================================
    
    def _is_complete(self) -> bool:
        """ì™„ë£Œ ì—¬ë¶€ íŒë‹¨"""
        if self._stopped:
            self.logger.info("ìˆ˜ë™ ì¤‘ì§€ë¡œ ì¢…ë£Œ")
            return True
        
        if self._current_step >= self.max_steps:
            self.logger.info(f"ìµœëŒ€ step({self.max_steps}) ë„ë‹¬ë¡œ ì¢…ë£Œ")
            return True
        
        return False
    
    # =========================================================================
    # Planning Methods
    # =========================================================================
    
    def _create_plan(self, user_query: str) -> Dict:
        """
        Phase 1: ì „ì²´ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
        
        Returns:
            Dict: {"plan": [...], "thought": "..."} ë˜ëŠ” {"direct_answer": "..."}
        """
        tools_schema = json.dumps(
            self.tools.get_all_schemas(),
            indent=2,
            ensure_ascii=False
        )
        
        system_prompt = f"""You are a task planning expert. Analyze the user's request and create an execution plan.

## Available Tools
{tools_schema}

## Your Task
1. Analyze what the user wants
2. Decide if tools are needed
3. If yes, create a step-by-step plan
4. If no tools needed, provide direct answer

## Response Format (JSON only)

If tools are needed:
{{
    "thought": "Analysis of what needs to be done",
    "plan": [
        {{
            "step": 1,
            "tool_name": "file_tool",
            "action": "read",
            "description": "Read the source file to analyze"
        }},
        {{
            "step": 2,
            "tool_name": "web_tool",
            "action": "search",
            "description": "Search for related information"
        }}
    ]
}}

If no tools needed (simple question, greeting, etc.):
{{
    "thought": "This is a simple question I can answer directly",
    "direct_answer": "Your answer here"
}}

## Rules
- Maximum {self.max_steps} steps
- Use only available tools and actions
- Each step should have clear purpose
- Order steps logically
- Respond with valid JSON only"""

        user_prompt = f"""## User Request
{user_query}

Create an execution plan or provide direct answer. Respond with JSON only."""

        self.logger.debug("Planning í˜¸ì¶œ", {"query": user_query[:50]})
        
        raw_response = self._call_llm_api(system_prompt, user_prompt)
        
        # JSON íŒŒì‹±
        try:
            parsed = self._extract_json(raw_response)
            self.logger.debug("Plan ìƒì„± ì™„ë£Œ", {
                "has_plan": "plan" in parsed,
                "has_direct_answer": "direct_answer" in parsed
            })
            return parsed
        except Exception as e:
            self.logger.error(f"Plan íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {"direct_answer": f"ê³„íš ìˆ˜ë¦½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
    
    def _parse_plan(self, plan_data: List[Dict]) -> List[PlannedStep]:
        """ê³„íš ë°ì´í„°ë¥¼ PlannedStep ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        planned_steps = []

        # plan_dataê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        if not isinstance(plan_data, list):
            self.logger.warn(f"plan_dataê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(plan_data)}")
            return planned_steps

        for item in plan_data:
            if not isinstance(item, dict):
                continue
            try:
                step = PlannedStep(
                    step=item.get("step", len(planned_steps) + 1),
                    tool_name=item.get("tool_name", item.get("tool", "")),
                    action=item.get("action", ""),
                    description=item.get("description", ""),
                    params_hint=item.get("params_hint", item.get("params", {})),
                    status="pending"
                )
                planned_steps.append(step)
            except Exception as e:
                self.logger.warn(f"Step íŒŒì‹± ì‹¤íŒ¨: {e}, item: {item}")
                continue
        
        return planned_steps
    
    def _format_plan_summary(self) -> str:
        """ê³„íš ìš”ì•½ ë¬¸ìì—´ ìƒì„±"""
        if not self._plan:
            return "ê³„íš ì—†ìŒ"
        
        lines = ["ğŸ“‹ **ì‹¤í–‰ ê³„íš**\n"]
        for step in self._plan:
            status_icon = {
                "pending": "â³",
                "running": "ğŸ”„",
                "completed": "âœ…",
                "failed": "âŒ"
            }.get(step.status, "â³")
            
            lines.append(f"{status_icon} Step {step.step}: {step.tool_name}.{step.action}")
            lines.append(f"   â””â”€ {step.description}")
        
        return "\n".join(lines)
    
    def _get_execution_params(self, user_query: str, planned_step: PlannedStep) -> LLMResponse:
        """
        ê³„íšëœ step ì‹¤í–‰ì„ ìœ„í•œ ì •í™•í•œ íŒŒë¼ë¯¸í„° ê²°ì •
        """
        tools_schema = json.dumps(
            self.tools.get_all_schemas(),
            indent=2,
            ensure_ascii=False
        )
        
        # í˜„ì¬ê¹Œì§€ì˜ ê²°ê³¼ ìˆ˜ì§‘
        results = self.storage.get_results()
        previous_results = self._format_previous_results(results)
        
        # í˜„ì¬ ê³„íš ìƒíƒœ
        plan_status = self._format_plan_with_status()
        
        system_prompt = f"""You are executing a planned task. Provide exact parameters for the current step.

## Available Tools
{tools_schema}

## Current Plan Status
{plan_status}

## Response Format
{{
    "thought": "Why these parameters",
    "tool_calls": [
        {{
            "name": "{planned_step.tool_name}",
            "arguments": {{
                "action": "{planned_step.action}",
                "{planned_step.action}_param1": "value1",
                "{planned_step.action}_param2": "value2"
            }}
        }}
    ]
}}

## Rules
- Execute ONLY the current step (Step {planned_step.step})
- Use parameter names with action prefix (e.g., read_path, write_content)
- Provide exact values based on user query and previous results
- Respond with valid JSON only"""

        user_prompt = f"""## Original User Query
{user_query}

## Previous Results
{previous_results}

## Current Step to Execute
Step {planned_step.step}: {planned_step.tool_name}.{planned_step.action}
Description: {planned_step.description}

Provide exact parameters for this step. Respond with JSON only."""

        raw_response = self._call_llm_api(system_prompt, user_prompt)
        return self._parse_llm_response(raw_response)
    
    def _format_plan_with_status(self) -> str:
        """ìƒíƒœê°€ í¬í•¨ëœ ê³„íš í¬ë§·"""
        if not self._plan:
            return "No plan"
        
        lines = []
        for step in self._plan:
            status_icon = {
                "pending": "[ ]",
                "running": "[â†’]",
                "completed": "[âœ“]",
                "failed": "[âœ—]"
            }.get(step.status, "[ ]")
            
            lines.append(f"{status_icon} Step {step.step}: {step.tool_name}.{step.action} - {step.description}")
        
        return "\n".join(lines)
    
    def _format_previous_results(self, results: List[Dict]) -> str:
        """ì´ì „ ê²°ê³¼ í¬ë§·"""
        if not results:
            return "None yet."
        
        formatted = ""
        for r in results:
            step = r.get('step', '?')
            tool = r.get('tool', r.get('executor', 'unknown'))
            action = r.get('action', 'unknown')
            status = r.get('status', 'unknown')
            output = str(r.get("output", ""))
            
            status_icon = "âœ…" if status == "success" else "âŒ"
            
            formatted += f"""
[Step {step}] {status_icon} {tool}.{action} - {status.upper()}
  Result: {output}
"""
        return formatted
    
    def _generate_final_answer(self, user_query: str) -> str:
        """
        Phase 3: ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
        """
        results = self.storage.get_results()
        previous_results = self._format_previous_results(results)
        plan_status = self._format_plan_with_status()
        
        system_prompt = """You are generating the final response based on executed tasks.

## Your Task
1. Review what was done (plan execution results)
2. Summarize the findings
3. Provide a helpful, complete answer to the user

## Rules
- Be concise but thorough
- Reference specific results when relevant
- If any step failed, mention it and provide alternatives if possible
- Respond naturally, not in JSON format"""

        user_prompt = f"""## Original User Query
{user_query}

## Execution Plan
{plan_status}

## Execution Results
{previous_results}

Based on the above results, provide a final answer to the user's query."""

        response = self._call_llm_api(system_prompt, user_prompt)
        return response.strip()
    
    def _extract_json(self, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ"""
        import re
        
        # ```json ... ``` íŒ¨í„´
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', text)
        if json_match:
            return json.loads(json_match.group(1))
        
        # { ... } íŒ¨í„´
        json_match = re.search(r'\{[\s\S]*\}', text)
        if json_match:
            return json.loads(json_match.group(0))
        
        raise ValueError("JSON not found in response")
    
    def _call_llm_api(self, system_prompt: str, user_prompt: str) -> str:
        """Ollama API í˜¸ì¶œ"""
        try:
            import requests
            
            url = f"{self.llm_config.base_url}/api/generate"
            
            payload = {
                "model": self.llm_config.model,
                "prompt": user_prompt,
                "system": system_prompt,
                "stream": False,
                "options": {
                    "temperature": self.llm_config.temperature,
                    "top_p": self.llm_config.top_p,
                    "num_predict": self.llm_config.max_tokens,
                    "repeat_penalty": self.llm_config.repeat_penalty,
                    "num_ctx": self.llm_config.num_ctx
                }
            }
            
            response = requests.post(
                url,
                json=payload,
                timeout=self.llm_config.timeout
            )
            response.raise_for_status()
            
            data = response.json()
            return data.get("response", "")
            
        except ImportError:
            self.logger.warn("requests íŒ¨í‚¤ì§€ ì—†ìŒ - Mock ì‘ë‹µ ë°˜í™˜")
            return self._mock_llm_response()
        
        except requests.exceptions.ConnectionError:
            self.logger.error(f"Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {self.llm_config.base_url}")
            raise ConnectionError(f"Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.llm_config.base_url}")
        
        except Exception as e:
            self.logger.error(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _extract_json_str(self, text: str) -> Optional[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ìœ íš¨í•œ JSON ì¶”ì¶œ (ê´„í˜¸ ë§¤ì¹­ ë°©ì‹)"""
        # { ìœ„ì¹˜ ì°¾ê¸°
        start = text.find('{')
        if start == -1:
            return None
        
        # ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ ë ì°¾ê¸°
        depth = 0
        in_string = False
        escape = False
        
        for i, char in enumerate(text[start:], start):
            if escape:
                escape = False
                continue
            if char == '\\':
                escape = True
                continue
            if char == '"' and not escape:
                in_string = not in_string
                continue
            if in_string:
                continue
            if char == '{':
                depth += 1
            elif char == '}':
                depth -= 1
                if depth == 0:
                    return text[start:i+1]
        
        return None
    
    def _fix_triple_quotes(self, text: str) -> str:
        """ì‚¼ì¤‘ ë”°ì˜´í‘œë¥¼ ìœ íš¨í•œ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        
        LLMì´ ê°€ë” Python ìŠ¤íƒ€ì¼ì˜ ì‚¼ì¤‘ ë”°ì˜´í‘œë¥¼ ì‚¬ìš©í•  ë•Œ ë³µêµ¬
        ì˜ˆ: "content": \"\"\"code\"\"\" â†’ "content": "code"
        """
        import re
        
        # íŒ¨í„´: "key": """content""" ë˜ëŠ” "key": '''content'''
        # ì‚¼ì¤‘ ë”°ì˜´í‘œ ë‚´ìš©ì„ ìº¡ì²˜í•˜ê³  ì¼ë°˜ ë¬¸ìì—´ë¡œ ë³€í™˜
        
        def replace_triple_quotes(match):
            content = match.group(1)
            # ë‚´ë¶€ ì¤„ë°”ê¿ˆì„ \nìœ¼ë¡œ ë³€í™˜
            content = content.replace('\n', '\\n')
            content = content.replace('\r', '\\r')
            content = content.replace('\t', '\\t')
            # ë‚´ë¶€ ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„
            content = content.replace('"', '\\"')
            return f'"{content}"'
        
        # """ ... """ íŒ¨í„´ ì²˜ë¦¬
        text = re.sub(r'"""(.*?)"""', replace_triple_quotes, text, flags=re.DOTALL)
        
        # ''' ... ''' íŒ¨í„´ ì²˜ë¦¬  
        text = re.sub(r"'''(.*?)'''", replace_triple_quotes, text, flags=re.DOTALL)
        
        return text
    
    def _sanitize_json_string(self, text: str) -> str:
        """JSON ë¬¸ìì—´ ì •ë¦¬ (ì‚¼ì¤‘ ë”°ì˜´í‘œ + ì œì–´ ë¬¸ì)"""
        
        # 1. ì‚¼ì¤‘ ë”°ì˜´í‘œ ë¨¼ì € ì²˜ë¦¬
        text = self._fix_triple_quotes(text)
        
        # 2. ì œì–´ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
        result = []
        in_string = False
        escape = False
        i = 0
        
        while i < len(text):
            char = text[i]
            
            if escape:
                result.append(char)
                escape = False
                i += 1
                continue
            
            if char == '\\':
                result.append(char)
                escape = True
                i += 1
                continue
            
            if char == '"':
                in_string = not in_string
                result.append(char)
                i += 1
                continue
            
            # ë¬¸ìì—´ ë‚´ë¶€ì˜ ì œì–´ ë¬¸ì ì²˜ë¦¬
            if in_string:
                if char == '\n':
                    result.append('\\n')
                elif char == '\r':
                    result.append('\\r')
                elif char == '\t':
                    result.append('\\t')
                elif ord(char) < 32:  # ê¸°íƒ€ ì œì–´ ë¬¸ì
                    result.append(f'\\u{ord(char):04x}')
                else:
                    result.append(char)
            else:
                result.append(char)
            
            i += 1
        
        return ''.join(result)
    
    def _parse_llm_response(self, raw_response: str) -> LLMResponse:
        """LLM ì‘ë‹µ íŒŒì‹±"""
        
        try:
            # 1. ì œì–´ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ (ë¨¼ì €!)
            sanitized_response = self._sanitize_json_string(raw_response)
            
            # 2. JSON ì¶”ì¶œ (ê´„í˜¸ ë§¤ì¹­ ë°©ì‹)
            json_str = self._extract_json_str(sanitized_response)
            
            if not json_str:
                # JSONì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ final answerë¡œ ì²˜ë¦¬
                return LLMResponse(
                    content=raw_response,
                    raw_response=raw_response
                )
            
            # 3. JSON íŒŒì‹±
            data = json.loads(json_str)
            
            # tool_calls íŒŒì‹±
            tool_calls = None
            if data.get("tool_calls"):
                tool_calls = []
                for tc in data["tool_calls"]:
                    tool_calls.append(ToolCall(
                        name=tc.get("name", ""),
                        arguments=tc.get("arguments", {})
                    ))
            
            return LLMResponse(
                thought=data.get("thought"),
                tool_calls=tool_calls,
                content=data.get("content"),
                raw_response=raw_response
            )
            
        except json.JSONDecodeError as e:
            self.logger.warn(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ final answerë¡œ ì²˜ë¦¬
            return LLMResponse(
                content=raw_response,
                raw_response=raw_response
            )
    
    def _execute_tool(self, tool_call: ToolCall) -> ToolResult:
        """Tool ì‹¤í–‰"""
        
        self.logger.info(f"Tool ì‹¤í–‰: {tool_call.name}.{tool_call.action}", {
            "params": tool_call.params
        })
        
        result = self.tools.execute(
            tool_name=tool_call.name,
            action=tool_call.action,
            params=tool_call.params
        )
        
        if result.success:
            self.logger.info(f"Tool ì‹¤í–‰ ì„±ê³µ: {tool_call.name}.{tool_call.action}")
        else:
            self.logger.error(f"Tool ì‹¤í–‰ ì‹¤íŒ¨: {result.error}")
        
        return result
    
    def _generate_partial_response(self) -> str:
        """ìµœëŒ€ step ë„ë‹¬ ì‹œ ë¶€ë¶„ ì‘ë‹µ ìƒì„±"""
        results = self.storage.get_results()
        
        if not results:
            return "ì‘ì—…ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        # ë§ˆì§€ë§‰ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        last_result = results[-1]
        return f"""ìµœëŒ€ ì‹¤í–‰ ë‹¨ê³„({self.max_steps})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.

ì§€ê¸ˆê¹Œì§€ì˜ ì§„í–‰ ìƒí™©:
- ì´ {len(results)}ê°œì˜ ì‘ì—… ìˆ˜í–‰
- ë§ˆì§€ë§‰ ì‘ì—…: {last_result['executor']}.{last_result['action']}

ë¶€ë¶„ ê²°ê³¼:
{str(last_result.get('output', ''))[:500]}

ë” ìì„¸í•œ ë¶„ì„ì´ í•„ìš”í•˜ë©´ ì§ˆë¬¸ì„ ë‚˜ëˆ ì„œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."""
    
    def _mock_llm_response(self) -> str:
        """í…ŒìŠ¤íŠ¸ìš© Mock ì‘ë‹µ"""
        results = self.storage.get_results()
        
        if not results:
            # ì²« ë²ˆì§¸ step - file_tool í˜¸ì¶œ
            return json.dumps({
                "thought": "íŒŒì¼ì„ ë¨¼ì € ì½ì–´ì•¼ í•©ë‹ˆë‹¤ (Mock)",
                "tool_calls": [{
                    "name": "file_tool",
                    "arguments": {
                        "action": "read",
                        "path": "test.txt"
                    }
                }]
            })
        else:
            # ì´í›„ step - final answer
            return json.dumps({
                "thought": "ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤ (Mock)",
                "tool_calls": None,
                "content": f"Mock ë¶„ì„ ì™„ë£Œ. {len(results)}ê°œì˜ stepì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤."
            })

# Orchestrator ì„¤ê³„ ë¬¸ì„œ

## ê°œìš”

OrchestratorëŠ” Multi-Agent Chatbotì˜ **ë‘ë‡Œ** ì—­í• ì„ í•©ë‹ˆë‹¤.
ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë°›ì•„ Toolë“¤ì„ ì¡°ìœ¨í•˜ê³  ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

## í•µì‹¬ ì›ì¹™

```
ReAct (Reasoning + Acting) íŒ¨í„´
- ë§¤ stepë§ˆë‹¤ í˜„ì¬ ìƒíƒœë¥¼ ë³´ê³  ë‹¤ìŒ í–‰ë™ ê²°ì •
- ì´ˆê¸° ê³„íš ì—†ì´ ë‹¨ê³„ë³„ ì§„í–‰
- ê²°ê³¼ë¥¼ ë³´ê³  ìœ ì—°í•˜ê²Œ ëŒ€ì‘
```

## ì „ì²´ íë¦„

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orchestrator (ReAct Loop)                  â”‚
â”‚                                             â”‚
â”‚  while not (complete or max_step):          â”‚
â”‚      1. LLMì—ê²Œ ë‹¤ìŒ í–‰ë™ ì§ˆë¬¸               â”‚
â”‚      2. Tool ì‹¤í–‰                           â”‚
â”‚      3. ê²°ê³¼ ì €ì¥ + ì‹¤ì‹œê°„ ì¶œë ¥ (Streaming)  â”‚
â”‚      4. ì™„ë£Œ ì—¬ë¶€ íŒë‹¨                       â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Final Response
```

## ì„¤ê³„ ê²°ì • ì‚¬í•­

| í•­ëª© | ê²°ì • | ë¹„ê³  |
|------|------|------|
| ê³„íš ë°©ì‹ | ReAct (ë‹¨ê³„ë³„ ê²°ì •) | ë§¤ stepë§ˆë‹¤ LLM íŒë‹¨ |
| LLM ì‘ë‹µ í˜•ì‹ | Function Calling ìŠ¤íƒ€ì¼ | tool_calls í˜•ì‹ |
| ìµœëŒ€ step ì œí•œ | ì œí•œ + LLM íŒë‹¨ | ê¸°ë³¸ 10 step |
| ê·œì¹™ ê¸°ë°˜ ê²°ì • | ë‚˜ì¤‘ì— ì¶”ê°€ | íŒ¨í„´ ë°œê²¬ë˜ë©´ |
| Streaming | ì‹¤ì‹œê°„ ì¶œë ¥ | Generator ë°©ì‹ |
| LLM ì„¤ì • | ë™ì  ë³€ê²½ ê°€ëŠ¥ | UIì—ì„œ ì‹¤ì‹œê°„ ì¡°ì ˆ |

## í´ë˜ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Orchestrator                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - tools: ToolRegistry                                   â”‚
â”‚ - storage: SharedStorage                                â”‚
â”‚ - llm_config: LLMConfig                                 â”‚
â”‚ - max_steps: int                                        â”‚
â”‚ - logger: DebugLogger                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + run(user_query) -> str                                â”‚
â”‚ + run_stream(user_query) -> Generator[StepInfo]         â”‚
â”‚ + update_llm_config(**kwargs)                           â”‚
â”‚ + stop()                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - _ask_llm(user_query) -> LLMResponse                   â”‚
â”‚ - _parse_llm_response(raw) -> LLMResponse               â”‚
â”‚ - _extract_json(text) -> Optional[str]                  â”‚
â”‚ - _sanitize_json_string(text) -> str                    â”‚
â”‚ - _fix_triple_quotes(text) -> str                       â”‚
â”‚ - _execute_tool(tool_call) -> ToolResult                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ uses
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LLMConfig                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + model: str = "llama3.2"                               â”‚
â”‚ + temperature: float = 0.7                              â”‚
â”‚ + top_p: float = 0.9                                    â”‚
â”‚ + max_tokens: int = 2048                                â”‚
â”‚ + base_url: str = "http://localhost:11434"              â”‚
â”‚ + timeout: int = 120                                    â”‚
â”‚ + repeat_penalty: float = 1.1                           â”‚
â”‚ + num_ctx: int = 4096                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ creates
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ToolCall                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + name: str                                             â”‚
â”‚ + arguments: Dict                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + action: str (property)                                â”‚
â”‚ + params: Dict (property)                               â”‚
â”‚ - _infer_action() -> Optional[str]                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ACTION_INFERENCE_RULES: Dict  # action ìë™ ì¶”ë¡  ê·œì¹™   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## LLM ì‘ë‹µ í˜•ì‹ (Function Calling)

### Tool í˜¸ì¶œ ì‹œ

```json
{
    "thought": "Why I'm taking this action",
    "tool_calls": [
        {
            "name": "file_tool",
            "arguments": {
                "action": "read",
                "path": "wifi.log"
            }
        }
    ]
}
```

### ìµœì¢… ë‹µë³€ ì‹œ

```json
{
    "thought": "I have enough information to answer",
    "tool_calls": null,
    "content": "ë¶„ì„ ê²°ê³¼: DMA timeoutì´ ì£¼ìš” ì›ì¸ì…ë‹ˆë‹¤..."
}
```

## JSON íŒŒì‹± ê°•í™”

LLMì´ ì˜ëª»ëœ JSONì„ ë³´ë‚¼ ë•Œ ìë™ ë³µêµ¬í•©ë‹ˆë‹¤.

### 1. ì‚¼ì¤‘ ë”°ì˜´í‘œ ì²˜ë¦¬

```python
# LLMì´ ì´ë ‡ê²Œ ë³´ë‚´ë©´ (ì˜ëª»ëœ JSON)
"content": """
def fibonacci(n):
    return n
"""

# ìë™ìœ¼ë¡œ ë³€í™˜
"content": "\ndef fibonacci(n):\n    return n\n"
```

### 2. ì œì–´ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„

```python
# ë¬¸ìì—´ ë‚´ ì¤„ë°”ê¿ˆì´ ìˆìœ¼ë©´
"content": "line1
line2"

# ìë™ìœ¼ë¡œ ë³€í™˜
"content": "line1\nline2"
```

### 3. ê´„í˜¸ ë§¤ì¹­ JSON ì¶”ì¶œ

```python
# í…ìŠ¤íŠ¸ ì•ë’¤ì— ë¶ˆí•„ìš”í•œ ë‚´ìš©ì´ ìˆì–´ë„
Here's my response:
{"thought": "...", "tool_calls": [...]}

# ì •í™•íˆ JSONë§Œ ì¶”ì¶œ
```

### íŒŒì‹± ìˆœì„œ

```
raw_response
    â†“
_fix_triple_quotes()    # """ â†’ " ë³€í™˜
    â†“
_sanitize_json_string() # ì œì–´ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„  
    â†“
_extract_json()         # ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ JSON ì¶”ì¶œ
    â†“
json.loads()            # íŒŒì‹±
```

## Action ìë™ ì¶”ë¡ 

LLMì´ actionì„ ë¹ ëœ¨ë ¸ì„ ë•Œ arguments ê¸°ë°˜ìœ¼ë¡œ ìë™ ì¶”ë¡ í•©ë‹ˆë‹¤.

### ì¶”ë¡  ê·œì¹™

| Tool | Arguments í‚¤ | ì¶”ë¡ ëœ Action |
|------|-------------|---------------|
| **file_tool** | `content` | write |
| | `path` (only) | read |
| | `pattern` | list_dir |
| **web_tool** | `url` | fetch |
| | `keyword` / `query` | search |
| **llm_tool** | `prompt` | ask |
| | `content` | summarize |
| | `text` | analyze |

### ì˜ˆì‹œ

```python
# LLMì´ action ì—†ì´ ë³´ë‚´ë©´
{"name": "llm_tool", "arguments": {"prompt": "Explain this"}}

# ìë™ ì¶”ë¡ : action = "ask" (prompt í‚¤ ê¸°ë°˜)
```

### tool_name.action í˜•ì‹ ì§€ì›

```python
# ì´ë ‡ê²Œ ë³´ë‚´ë„ ì²˜ë¦¬ë¨
{"name": "file_tool.read", "arguments": {"path": "test.txt"}}

# ìë™ ë¶„ë¦¬: name="file_tool", action="read"
```

## LLM ë™ì  ì„¤ì •

ì‚¬ìš©ìê°€ UIì—ì„œ ì‹¤ì‹œê°„ ë³€ê²½ ê°€ëŠ¥:

```python
@dataclass
class LLMConfig:
    # ëª¨ë¸ ì„ íƒ
    model: str = "llama3.2"           # Ollama ì„œë²„ì—ì„œ ì¡°íšŒ
    
    # ìƒì„± íŒŒë¼ë¯¸í„°  
    temperature: float = 0.7          # ì°½ì˜ì„± (0.0 ~ 2.0)
    top_p: float = 0.9                # nucleus sampling
    max_tokens: int = 2048            # ìµœëŒ€ ì¶œë ¥ ê¸¸ì´
    
    # ì—°ê²° ì„¤ì •
    base_url: str = "http://localhost:11434"
    timeout: int = 120
    
    # ê³ ê¸‰ ì˜µì…˜
    repeat_penalty: float = 1.1       # ë°˜ë³µ ì–µì œ
    num_ctx: int = 4096               # context window í¬ê¸°
```

### ì„¤ì • ì‹œë‚˜ë¦¬ì˜¤

```python
# ì½”ë“œ ë¶„ì„ - ì •í™•ì„± ì¤‘ì‹œ
config.model = "codellama"
config.temperature = 0.2

# ì°½ì˜ì  ìš”ì•½ - ì°½ì˜ì„± ë†’ì„
config.model = "llama3.2"
config.temperature = 0.8

# ê¸´ ë¡œê·¸ ì²˜ë¦¬ - context í™•ì¥
config.num_ctx = 8192
config.max_tokens = 4096
```

## Streaming ì¶œë ¥

### Generator ë°©ì‹ (Gradio ì—°ë™)

```python
def run_stream(user_query: str) -> Generator[StepInfo, None, None]:
    """ê° step ê²°ê³¼ë¥¼ yield"""
    while not complete:
        # Thinking
        yield StepInfo(type=StepType.THINKING, content="ë¶„ì„ ì¤‘...")
        
        # Tool í˜¸ì¶œ
        yield StepInfo(type=StepType.TOOL_CALL, tool_name="file_tool", action="read")
        
        # Tool ê²°ê³¼
        yield StepInfo(type=StepType.TOOL_RESULT, content="íŒŒì¼ ë‚´ìš©...")
    
    # ìµœì¢… ë‹µë³€
    yield StepInfo(type=StepType.FINAL_ANSWER, content="ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤...")
```

### StepType ì¢…ë¥˜

| Type | ì„¤ëª… |
|------|------|
| THINKING | LLM ì‚¬ê³  ê³¼ì • |
| TOOL_CALL | Tool í˜¸ì¶œ ì‹œì‘ |
| TOOL_RESULT | Tool ì‹¤í–‰ ê²°ê³¼ |
| FINAL_ANSWER | ìµœì¢… ë‹µë³€ |
| ERROR | ì˜¤ë¥˜ ë°œìƒ |

## ì¢…ë£Œ ì¡°ê±´

```python
def is_complete(self) -> bool:
    # 1. LLMì´ final_answer ë°˜í™˜
    if last_response.tool_calls is None:
        return True
    
    # 2. ìµœëŒ€ step ë„ë‹¬
    if current_step >= self.max_steps:
        return True
    
    # 3. ìˆ˜ë™ ì¤‘ì§€
    if self.stopped:
        return True
    
    return False
```

## System Prompt ì„¤ê³„

```
You are an AI assistant that helps users by using available tools.

## Available Tools
{tools_schema}

## Response Format
You must respond in valid JSON format only. No other text before or after the JSON.

1. To use a tool:
{
    "thought": "Why I'm taking this action",
    "tool_calls": [
        {
            "name": "tool_name",
            "arguments": {
                "action": "action_name",
                "param1": "value1"
            }
        }
    ]
}

2. To give final answer (when you have enough information):
{
    "thought": "I have enough information to answer",
    "tool_calls": null,
    "content": "Your final answer here"
}

## Rules
- Always respond with valid JSON only
- Use tools when you need external information
- Give final answer when you have enough information
- Be concise and accurate
- One tool call at a time is recommended for clarity

## IMPORTANT: JSON String Format
- NEVER use triple quotes (""") in JSON - they are invalid
- Use \n for newlines inside strings
- Escape double quotes as \"
- Example for code content: "content": "def hello():\n    print(\"Hello\")"
```

## íŒŒì¼ ìœ„ì¹˜

```
multi_agent_chatbot/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ shared_storage.py
â”‚   â”œâ”€â”€ base_tool.py
â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”œâ”€â”€ workspace_manager.py
â”‚   â””â”€â”€ html_utils.py
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ file_tool.py
â”‚   â”œâ”€â”€ web_tool.py
â”‚   â””â”€â”€ llm_tool.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ shared_storage.md
â”‚   â”œâ”€â”€ base_tool.md
â”‚   â”œâ”€â”€ orchestrator.md
â”‚   â””â”€â”€ ui.md
â””â”€â”€ tests/
    â”œâ”€â”€ test_shared_storage.py
    â”œâ”€â”€ test_tools.py
    â”œâ”€â”€ test_orchestrator.py
    â””â”€â”€ test_workspace.py
```

## ì‚¬ìš© ì˜ˆì‹œ

```python
from core.orchestrator import Orchestrator, LLMConfig
from core.shared_storage import SharedStorage
from core.base_tool import ToolRegistry
from tools import FileTool, WebTool, LLMTool

# 1. ì„¤ì •
storage = SharedStorage()
registry = ToolRegistry()
registry.register(FileTool())
registry.register(WebTool())
registry.register(LLMTool())

# 2. Orchestrator ìƒì„±
orchestrator = Orchestrator(
    tools=registry,
    storage=storage,
    max_steps=10
)

# 3. LLM ì„¤ì • ë³€ê²½ (ì„ íƒ)
orchestrator.update_llm_config(
    model="codellama",
    temperature=0.3
)

# 4. ì‹¤í–‰ (Streaming)
for step_info in orchestrator.run_stream("wifi.log ë¶„ì„í•´ì¤˜"):
    if step_info.type == StepType.THINKING:
        print(f"ğŸ’­ {step_info.content}")
    elif step_info.type == StepType.TOOL_CALL:
        print(f"ğŸ”§ {step_info.tool_name}.{step_info.action}")
    elif step_info.type == StepType.TOOL_RESULT:
        print(f"ğŸ“„ {step_info.content[:100]}...")
    elif step_info.type == StepType.FINAL_ANSWER:
        print(f"âœ… {step_info.content}")
```
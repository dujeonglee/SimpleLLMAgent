"""
Multi-Agent Chatbot UI (Updated)
================================
Gradio ê¸°ë°˜ ì›¹ ì¸í„°í˜ì´ìŠ¤ - ì‹¤ì‹œê°„ Streaming ê°œì„ 

ë³€ê²½ì‚¬í•­:
- chat_streamì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
- ê° Step ê²°ê³¼ë¥¼ ì¦‰ì‹œ ì±„íŒ…ì°½ì— ë°˜ì˜
- ìƒíƒœ í‘œì‹œì¤„ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
"""

import os
import sys
import warnings
from typing import List, Generator, Optional, Dict

warnings.filterwarnings("ignore", category=DeprecationWarning, module="gradio")

import gradio as gr

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from core.shared_storage import SharedStorage
from core.base_tool import ToolRegistry
from core.orchestrator import Orchestrator
from core.workspace_manager import WorkspaceManager, ConfigManager
from core.execution_events import (
    ExecutionEvent,
    PlanPromptEvent,
    PlanReadyEvent,
    ThinkingEvent,
    StepPromptEvent,
    ToolCallEvent,
    ToolResultEvent,
    FinalAnswerEvent,
    ErrorEvent
)
from tools.file_tool import FileTool
from tools.llm_tool import LLMTool


# =============================================================================
# Global State
# =============================================================================

class AppState:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ ê´€ë¦¬"""
    
    def __init__(self, workspace_path: str = os.path.join(".", "workspace")):
        self.workspace_path = workspace_path
        
        self.workspace_manager = WorkspaceManager(workspace_path)
        self.config_manager = ConfigManager(os.path.join(workspace_path, "config"))
        
        self.llm_config = self.config_manager.load_llm_config()
        
        self.storage = SharedStorage(debug_enabled=True)
        self.registry = ToolRegistry(debug_enabled=True)
        
        self._setup_tools()
        
        self.orchestrator = Orchestrator(
            tools=self.registry,
            storage=self.storage,
            llm_config=self.llm_config,
            max_steps=self.llm_config.max_steps,
            debug_enabled=True
        )
        
        self.chat_history: List[Dict] = []
        self.system_prompt_history: List[Dict] = []
        self.ollama_connected: bool = False
        self.available_models: List[Dict] = []
    
    def _setup_tools(self):
        """Tools ì„¤ì •"""
        files_dir = os.path.join(self.workspace_path, "files")
        os.makedirs(files_dir, exist_ok=True)

        self.registry.register(FileTool(base_path=files_dir, debug_enabled=True))
        self.registry.register(LLMTool(base_path=files_dir, debug_enabled=True, use_mock=False))
    
    def update_llm_config(self, **kwargs):
        """LLM ì„¤ì • ì—…ë°ì´íŠ¸ ë° ì €ì¥"""
        self.llm_config.update(**kwargs)
        self.orchestrator.update_llm_config(**kwargs)
        
        if 'max_steps' in kwargs:
            self.orchestrator.max_steps = kwargs['max_steps']
        
        self.config_manager.save_llm_config(self.llm_config)
    
    def fetch_ollama_models(self, base_url: str = None) -> tuple[bool, List[Dict]]:
        """Ollama ì„œë²„ì—ì„œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        if base_url is None:
            base_url = self.llm_config.base_url
        
        try:
            import requests
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            response.raise_for_status()
            data = response.json()
            
            models = []
            for model in data.get("models", []):
                name = model.get("name", "unknown")
                size_bytes = model.get("size", 0)
                details = model.get("details", {})
                
                if size_bytes >= 1024 * 1024 * 1024:
                    size_str = f"{size_bytes / (1024**3):.1f}GB"
                else:
                    size_str = f"{size_bytes / (1024**2):.0f}MB"
                
                family = details.get("family", "unknown")
                param_size = details.get("parameter_size", "")
                quantization = details.get("quantization_level", "")
                
                features = []
                if param_size:
                    features.append(param_size)
                if quantization:
                    features.append(quantization)
                if family and family != "unknown":
                    features.append(family)
                
                models.append({
                    "name": name,
                    "size": size_str,
                    "features": ", ".join(features) if features else "N/A",
                    "display": f"{name} ({', '.join(features) if features else size_str})"
                })
            
            self.ollama_connected = True
            self.available_models = models
            return True, models
            
        except Exception as e:
            print(f"[WARN] Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            self.ollama_connected = False
            self.available_models = []
            return False, []


app_state: Optional[AppState] = None


def get_app_state() -> AppState:
    global app_state
    if app_state is None:
        app_state = AppState()
    return app_state


# =============================================================================
# Chat Functions (Updated - Real-time Streaming with ExecutionEvents)
# =============================================================================

def chat_stream(message: str, history: List[Dict]) -> Generator[tuple, None, None]:
    """
    ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬ (Real-time Streaming)

    ê° ExecutionEventì˜ to_display() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    Returns: (history, send_btn_update, stop_btn_update, file_list_html_update)
    """
    state = get_app_state()

    state.orchestrator._stopped = False

    if not message.strip():
        yield history, gr.update(interactive=False), gr.update(interactive=True), gr.update()
        return

    # íŒŒì¼ ëª©ë¡ì„ contextë¡œ ì¤€ë¹„ (orchestratorì— ì „ë‹¬)
    files_context = state.workspace_manager.get_files_for_prompt()

    # ìƒˆ ëŒ€í™” ì¶”ê°€ (user ë©”ì‹œì§€)
    history = history + [
        {"role": "user", "content": message}
    ]

    # ëˆ„ì  ì‘ë‹µ êµ¬ì„±
    accumulated_output = []
    was_stopped = False

    # ì´ˆê¸° ìƒíƒœ í‘œì‹œ
    yield history, gr.update(interactive=False), gr.update(interactive=True), gr.update()

    try:
        for event in state.orchestrator.run_stream(message, files_context):
            # ===== ì¤‘ì§€ ì²´í¬ =====
            if state.orchestrator._stopped:
                was_stopped = True
                break

            # ê° ì´ë²¤íŠ¸ì˜ to_display() ë©”ì„œë“œë¡œ ì¶œë ¥ ìƒì„±
            display_text = event.to_display()

            # StepPromptEventëŠ” ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜ (ToolResultEventì— í¬í•¨ë˜ë¯€ë¡œ)
            # ë”°ë¼ì„œ ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ ë•Œë§Œ ì¶”ê°€
            if display_text:
                temporal_event_str = '''<div style="display: flex; align-items: center; gap: 8px;"><div class="spinner"></div>'''
                if accumulated_output and temporal_event_str in accumulated_output[-1]:
                    accumulated_output.pop()

                accumulated_output.append(display_text)

            # ìµœì¢… ì‘ë‹µ ì—…ë°ì´íŠ¸
            response = "\n".join(accumulated_output)

            # file_toolì˜ write/delete ì™„ë£Œ ì‹œ workspace UI ì—…ë°ì´íŠ¸
            file_list_update = gr.update()
            if isinstance(event, ToolResultEvent):
                if event.tool_name == "file_tool" and event.action in ["write", "delete"] and event.success:
                    file_list_update = gr.update(value=get_files_data())

            # FinalAnswerEventë‚˜ ErrorEventê°€ ì˜¤ë©´ ì¢…ë£Œ ì²˜ë¦¬
            if isinstance(event, (FinalAnswerEvent, ErrorEvent)):
                history[-1] = {"role": "assistant", "content": response}
                yield history, gr.update(interactive=True), gr.update(interactive=False), file_list_update
                break
            else:
                # ì¤‘ê°„ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                if len(history) > 0 and history[-1]["role"] == "assistant":
                    history[-1] = {"role": "assistant", "content": response}
                else:
                    history = history + [{"role": "assistant", "content": response}]
                yield history, gr.update(interactive=False), gr.update(interactive=True), file_list_update

        # ===== ì¤‘ì§€ëœ ê²½ìš° ë©”ì‹œì§€ í‘œì‹œ =====
        if was_stopped:
            accumulated_output.append("\nâ¹ï¸ **ì¤‘ì§€ë¨**")
            response = "\n".join(accumulated_output)
            history[-1] = {"role": "assistant", "content": response}
            yield history, gr.update(interactive=True), gr.update(interactive=False), gr.update()

    except GeneratorExit:
        # Gradioê°€ generatorë¥¼ ì¤‘ë‹¨í•  ë•Œ
        state.orchestrator._stopped = True
        accumulated_output.append("\nâ¹ï¸ **ì¤‘ì§€ë¨**")
        response = "\n".join(accumulated_output)
        if len(history) > 0 and history[-1]["role"] == "assistant":
            history[-1]["content"] = response
        yield history, gr.update(interactive=True), gr.update(interactive=False), gr.update()

    except Exception as e:
        accumulated_output.append(f"\nâŒ **ì˜ˆì™¸ ë°œìƒ**\n\n{str(e)}")
        response = "\n".join(accumulated_output)
        if len(history) > 0 and history[-1]["role"] == "assistant":
            history[-1]["content"] = response
        yield history, gr.update(interactive=True), gr.update(interactive=False), gr.update()

    finally:
        # ì„¸ì…˜ ì •ë¦¬
        state.orchestrator._stopped = False

    state.chat_history = history


def stop_generation():
    """ìƒì„± ì¤‘ì§€"""
    state = get_app_state()
    state.orchestrator.stop()


def clear_chat():
    """ëŒ€í™” ì´ˆê¸°í™”"""
    state = get_app_state()
    state.chat_history = []
    state.storage.reset()
    state.system_prompt_history = []
    return []


# =============================================================================
# LLM Settings Functions
# =============================================================================

def load_settings_for_modal():
    """ì„¤ì • ëª¨ë‹¬ ì—´ ë•Œ í˜„ì¬ ê°’ ë¡œë“œ"""
    state = get_app_state()
    config = state.llm_config
    
    connected, models = state.fetch_ollama_models(config.base_url)
    
    if connected and models:
        model_choices = [m["display"] for m in models]
        current_display = config.model
        for m in models:
            if m["name"] == config.model:
                current_display = m["display"]
                break
    else:
        model_choices = [config.model]
        current_display = config.model
    
    url_status = "âœ… ì—°ê²°ë¨" if connected else "âŒ ì—°ê²° ì‹¤íŒ¨"
    
    return (
        gr.update(visible=True),
        config.base_url,
        url_status,
        gr.update(choices=model_choices, value=current_display),
        config.temperature,
        config.max_tokens,
        config.top_p,
        config.top_k,
        config.repeat_penalty,
        config.frequency_penalty,
        config.presence_penalty,
        config.num_ctx,
        config.max_steps,
        config.timeout
    )


def close_settings_modal():
    return gr.update(visible=False)


def on_url_change(url: str):
    state = get_app_state()
    connected, models = state.fetch_ollama_models(url)
    
    if connected and models:
        model_choices = [m["display"] for m in models]
        current_model = state.llm_config.model
        current_display = current_model
        for m in models:
            if m["name"] == current_model:
                current_display = m["display"]
                break
        
        return (
            "âœ… ì—°ê²°ë¨",
            gr.update(choices=model_choices, value=current_display if current_display in model_choices else model_choices[0])
        )
    else:
        return (
            "âŒ ì—°ê²° ì‹¤íŒ¨",
            gr.update(choices=[state.llm_config.model], value=state.llm_config.model)
        )


def save_settings(url, model_display, temperature, max_tokens, top_p, top_k, repeat_penalty,
                  frequency_penalty, presence_penalty, num_ctx, max_steps, timeout):
    state = get_app_state()

    model_name = model_display.split(" (")[0] if " (" in model_display else model_display

    state.update_llm_config(
        base_url=url,
        model=model_name,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        top_k=top_k,
        repeat_penalty=repeat_penalty,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        num_ctx=num_ctx,
        max_steps=max_steps,
        timeout=timeout
    )

    return gr.update(visible=False)


# =============================================================================
# File Management Functions
# =============================================================================

def get_files_data() -> List[Dict]:
    """íŒŒì¼ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    state = get_app_state()
    files = state.workspace_manager.list_files()
    files_dir = os.path.join(state.workspace_path, "files")

    result = []
    for f in files:
        size = f"{f.size / 1024:.1f}KB" if f.size >= 1024 else f"{f.size}B"
        source = "ğŸ“¤ ì—…ë¡œë“œ" if f.source == "upload" else "ğŸ¤– ìƒì„±"

        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        file_path = os.path.join(files_dir, f.name)
        content = ""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='cp949') as file:
                    content = file.read()
            except:
                content = "[íŒŒì¼ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - ë°”ì´ë„ˆë¦¬ íŒŒì¼ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤]"
        except Exception as e:
            content = f"[ì˜¤ë¥˜ ë°œìƒ: {str(e)}]"

        result.append({
            'name': f.name,
            'size': size,
            'source': source,
            'content': content
        })

    return result


def upload_files(files):
    """íŒŒì¼ ì—…ë¡œë“œ"""
    state = get_app_state()

    if not files:
        return get_files_data(), "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”."

    uploaded = []
    for file in files:
        file_info = state.workspace_manager.save_upload(file.name)
        if file_info:
            uploaded.append(file_info.name)

    if uploaded:
        return get_files_data(), f"âœ… {len(uploaded)}ê°œ íŒŒì¼ ì—…ë¡œë“œë¨"
    return get_files_data(), "âŒ ì—…ë¡œë“œ ì‹¤íŒ¨"


def delete_single_file(filename: str):
    """ë‹¨ì¼ íŒŒì¼ ì‚­ì œ"""
    state = get_app_state()
    success = state.workspace_manager.delete_file(filename)
    if success:
        return get_files_data(), f"âœ… '{filename}' íŒŒì¼ ì‚­ì œë¨"
    return get_files_data(), f"âŒ '{filename}' íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨"


def delete_all_files():
    """ì „ì²´ íŒŒì¼ ì‚­ì œ"""
    state = get_app_state()
    count = state.workspace_manager.delete_all_files()
    return get_files_data(), f"âœ… {count}ê°œ íŒŒì¼ ì‚­ì œë¨"


def refresh_file_list():
    """íŒŒì¼ ëª©ë¡ ìƒˆë¡œê³ ì¹¨"""
    return get_files_data(), ""


# =============================================================================
# Build UI
# =============================================================================

def create_ui() -> gr.Blocks:
    """Gradio UI ìƒì„±"""

    state = get_app_state()
    
    with gr.Blocks(title="Multi-Agent Chatbot") as app:

        # Header
        gr.Markdown("# ğŸ¤– Multi-Agent Chatbot")

        # Settings Modal
        with gr.Column(visible=False, elem_classes=["settings-modal"]) as settings_modal:
            gr.Markdown("## âš™ï¸ LLM Settings")
            
            with gr.Row():
                url_input = gr.Textbox(
                        label="Ollama URL",
                        value=state.llm_config.base_url,
                        placeholder="http://localhost:11434",
                        info="ollama ì„œë²„ ì£¼ì†Œë¥¼ ì…ë ¥í•˜ì„¸ìš”."
                )
                model_dropdown = gr.Dropdown(
                    label="Model",
                    choices=[state.llm_config.model],
                    value=state.llm_config.model,
                    allow_custom_value=True,
                    info="ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”. (ì˜ˆ: gpt-3.5-turbo, llama-2-7b)"
                )
                timeout_slider = gr.Slider(10, 3600, 10, value=state.llm_config.timeout, label="Timeout",
                                           info="LLM ì‘ë‹µ íƒ€ì„ì•„ì›ƒ ê°’ì„ ì§€ì •í•©ë‹ˆë‹¤.")

            with gr.Row():
                url_status = gr.Markdown("â³", elem_id="url-status")

            
            with gr.Row():
                temperature_slider = gr.Slider(0.0, 2.0, 0.1, value=state.llm_config.temperature, label="Temperature",
                                               info="LLMì˜ ì‘ë‹µì˜ ë¬´ì‘ìœ„ì„±ì„ ì¡°ì ˆí•©ë‹ˆë‹¤. ë†’ì€ ê°’ì¼ìˆ˜ë¡ ë‹µë³€ì˜ ì°½ì˜ì ì„±ì´ ì¦ê°€ í•˜ê³  ì¼ê´€ì„±ì´ ê°ì†Œ í•©ë‹ˆë‹¤.")
                max_tokens_slider = gr.Slider(256, 24576, 256, value=state.llm_config.max_tokens, label="Max Tokens",
                                              info="LLMì´ ìƒì„±í•  ìµœëŒ€ í† í°(ë‹¨ì–´ ë˜ëŠ” ë‹¨ì–´ ì¡°ê°) ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ê°’ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ì‘ë‹µì´ ì˜ë¦´ ìˆ˜ ìˆê³ , ê°’ì´ ë„ˆë¬´ í¬ë©´ ë¶ˆí•„ìš”í•˜ê²Œ ê¸´ ì‘ë‹µì´ ìƒì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                num_ctx_slider = gr.Slider(2048, 32768, 1024, value=state.llm_config.num_ctx, label="Context Window",
                                           info="LLMì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´(í† í° ìˆ˜)ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ê°’ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ LLMì´ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë†“ì¹  ìˆ˜ ìˆê³ , ê°’ì´ ë„ˆë¬´ í¬ë©´ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            with gr.Row():
                top_p_slider = gr.Slider(0.0, 1.0, 0.05, value=state.llm_config.top_p, label="Top-p",
                                         info="LLMì´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì„ íƒí•  ë•Œ ê³ ë ¤í•˜ëŠ” í™•ë¥  ë¶„í¬ì˜ ëˆ„ì  í™•ë¥ ì„ ì¡°ì ˆí•©ë‹ˆë‹¤. ë†’ì€ ê°’ì¼ìˆ˜ë¡ ë‹¤ì–‘í•œ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ê³  ì¼ê´€ì„±ì´ ê°ì†Œ í•©ë‹ˆë‹¤.")
                top_k_slider = gr.Slider(1, 100, 1, value=state.llm_config.top_k, label="Top-k",
                                         info="ë‹¤ìŒ í† í° ì„ íƒ ì‹œ ê³ ë ¤í•  ìƒìœ„ í›„ë³´ ê°œìˆ˜ì…ë‹ˆë‹¤. ë‚®ì€ ê°’ì€ ë” ì¼ê´€ì„± ìˆì§€ë§Œ ëœ ë‹¤ì–‘í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")
                repeat_penalty_slider = gr.Slider(1.0, 2.0, 0.1, value=state.llm_config.repeat_penalty, label="Repeat Penalty",
                                                  info="LLMì´ ì´ë¯¸ ìƒì„±í•œ ë‹¨ì–´ë¥¼ ë°˜ë³µí•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤. ê°’ì´ ë†’ì„ ìˆ˜ë¡ ë” ë‹¤ì–‘í•œ ì‘ë‹µì„ ìƒì„±í•˜ê³  ê°’ì´ ë‚®ì„ ìˆ˜ë¡ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")

            with gr.Row():
                frequency_penalty_slider = gr.Slider(0.0, 2.0, 0.1, value=state.llm_config.frequency_penalty, label="Frequency Penalty",
                                                     info="í† í°ì˜ ë¹ˆë„ì— ë”°ë¼ í˜ë„í‹°ë¥¼ ì ìš©í•©ë‹ˆë‹¤. ë†’ì€ ê°’ì€ ìì£¼ ì‚¬ìš©ëœ í† í°ì˜ ì¬ì‚¬ìš©ì„ ì¤„ì…ë‹ˆë‹¤.")
                presence_penalty_slider = gr.Slider(0.0, 2.0, 0.1, value=state.llm_config.presence_penalty, label="Presence Penalty",
                                                    info="ì´ë¯¸ ë“±ì¥í•œ í† í°ì— í˜ë„í‹°ë¥¼ ì ìš©í•©ë‹ˆë‹¤. ë†’ì€ ê°’ì€ ìƒˆë¡œìš´ ì£¼ì œë¡œì˜ ì „í™˜ì„ ì´‰ì§„í•©ë‹ˆë‹¤.")
                max_steps_slider = gr.Slider(10, 100, 1, value=state.llm_config.max_steps, label="Max Steps",
                                             info="í•˜ë‚˜ì˜ ìš”ì²­ì— ëŒ€í•´ì„œ ìµœëŒ€ Step(ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜)ì„ ì œí•œ í•©ë‹ˆë‹¤. ê°’ì´ ë†’ì„ ìˆ˜ë¡ ë” ë§ì€ ë„êµ¬ë“¤ì„ í˜¸ì¶œì„ í•  ìˆ˜ ìˆì§€ë§Œ, ê·¸ë§Œí¼ ë” ë§ì€ ì‹œê°„ì´ ì†Œìš” ë©ë‹ˆë‹¤.")

            with gr.Row():
                save_btn = gr.Button("ğŸ’¾ Save", variant="primary")
                cancel_btn = gr.Button("Cancel")
        
        # Chat Area
        with gr.Column():
            chatbot = gr.Chatbot(
                label="ëŒ€í™”",
                elem_classes=["chatbot"],
                height=500
            )
            
            with gr.Row():
                msg_input = gr.Textbox(
                    placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”... (ì˜ˆ: sample.c íŒŒì¼ì„ ì½ì–´ì„œ ì •ì  ë¶„ì„í•˜ê³  out.mdì— ì €ì¥í•´ì¤˜)",
                    label="",
                    scale=10,
                    container=False
                )
                send_btn = gr.Button("â–¶ï¸ì „ì†¡", variant="primary", scale=1, interactive=True)
                stop_btn = gr.Button("â¹ï¸ì¤‘ì§€", variant="stop", scale=1, interactive=False)
                clear_btn = gr.Button("ğŸ—‘ï¸ì‚­ì œ", scale=1)
                settings_btn = gr.Button("âš™ï¸ Settings", scale=1, variant="secondary")
        
        gr.Markdown("---")
        
        # Tab Panels
        with gr.Tabs():
            
            # Workspace Files Tab
            with gr.TabItem("ğŸ“ Workspace Files"):
                file_upload = gr.File(
                    label="íŒŒì¼ì„ ë“œë˜ê·¸í•˜ê±°ë‚˜ í´ë¦­í•˜ì—¬ ì—…ë¡œë“œ",
                    file_count="multiple",
                    file_types=None
                )

                with gr.Row():
                    delete_all_btn = gr.Button("ğŸ—‘ï¸ ì „ì²´ ì‚­ì œ", size="sm", variant="stop")
                    refresh_files_btn = gr.Button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", size="sm")

                file_status = gr.Markdown("")

                # íŒŒì¼ ëª©ë¡ì„ ìœ„í•œ State
                files_state = gr.State([])

                # ë™ì  íŒŒì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë Œë”ë§í•˜ëŠ” í•¨ìˆ˜
                @gr.render(inputs=[files_state])
                def render_file_list(files):
                    if not files:
                        gr.Markdown("_íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤._")
                        return

                    for file_info in files:
                        with gr.Row():
                            with gr.Column(scale=8):
                                with gr.Accordion(file_info['name'], open=False):
                                    gr.Code(
                                        value=file_info['content'],
                                        language=None,
                                        interactive=False,
                                        max_lines=20
                                    )
                            with gr.Column(scale=2):
                                gr.Markdown(f"**í¬ê¸°:** {file_info['size']}")
                                gr.Markdown(f"**ì¶œì²˜:** {file_info['source']}")
                            with gr.Column(scale=1):
                                delete_btn = gr.Button("ğŸ—‘ï¸", size="sm", variant="stop")
                                delete_btn.click(
                                    fn=lambda fname=file_info['name']: delete_single_file(fname),
                                    outputs=[files_state, file_status]
                                )
            
        
        # =================================================================
        # Event Handlers
        # =================================================================
        
        # Settings Modal
        settings_btn.click(
            fn=load_settings_for_modal,
            outputs=[settings_modal, url_input, url_status, model_dropdown,
                    temperature_slider, max_tokens_slider, top_p_slider, top_k_slider,
                    repeat_penalty_slider, frequency_penalty_slider, presence_penalty_slider,
                    num_ctx_slider, max_steps_slider, timeout_slider]
        )

        cancel_btn.click(fn=close_settings_modal, outputs=[settings_modal])

        url_input.change(fn=on_url_change, inputs=[url_input], outputs=[url_status, model_dropdown])

        save_btn.click(
            fn=save_settings,
            inputs=[url_input, model_dropdown, temperature_slider, max_tokens_slider,
                   top_p_slider, top_k_slider, repeat_penalty_slider,
                   frequency_penalty_slider, presence_penalty_slider,
                   num_ctx_slider, max_steps_slider, timeout_slider],
            outputs=[settings_modal]
        )
        
        # Chat events
        def chat_stream_with_clear(message: str, history: List[Dict]):
            """
            Wrapper that clears input immediately on submit, then streams chat.

            First yield: clears input field immediately
            Subsequent yields: stream from chat_stream
            Returns: (chatbot, send_btn, stop_btn, msg_input, files_state)
            """
            # First yield to clear input immediately
            yield history, gr.update(interactive=False), gr.update(interactive=True), "", gr.update()

            # Then stream the actual chat
            for chatbot_update, send_update, stop_update, file_list_update in chat_stream(message, history):
                yield chatbot_update, send_update, stop_update, "", file_list_update

        def on_chat_complete():
            return gr.update(interactive=True), gr.update(interactive=False)

        msg_input.submit(
            fn=chat_stream_with_clear,
            inputs=[msg_input, chatbot],
            outputs=[chatbot, send_btn, stop_btn, msg_input, files_state],
            concurrency_limit=1
        ).then(
            fn=on_chat_complete,
            outputs=[send_btn, stop_btn]
        )

        send_btn.click(
            fn=chat_stream_with_clear,
            inputs=[msg_input, chatbot],
            outputs=[chatbot, send_btn, stop_btn, msg_input, files_state],
            concurrency_limit=1
        ).then(
            fn=on_chat_complete,
            outputs=[send_btn, stop_btn]
        )

        stop_btn.click(fn=stop_generation)

        clear_btn.click(fn=clear_chat, outputs=[chatbot]).then(
            fn=on_chat_complete,
            outputs=[send_btn, stop_btn]
        )
        
        # File management
        # íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  íŒŒì¼ ì„ íƒ ì°½ ì´ˆê¸°í™”
        file_upload.upload(
            fn=upload_files,
            inputs=[file_upload],
            outputs=[files_state, file_status]
        ).then(
            fn=lambda: None,
            outputs=[file_upload]
        )

        delete_all_btn.click(
            fn=delete_all_files,
            outputs=[files_state, file_status]
        )

        refresh_files_btn.click(
            fn=refresh_file_list,
            outputs=[files_state, file_status]
        )

        # Page Load
        def on_page_load():
            return get_files_data()

        app.load(fn=on_page_load, outputs=[files_state])

    return app


# =============================================================================
# Main
# =============================================================================

def main():
    print("=" * 60)
    print("Multi-Agent Chatbot (with Real-time Streaming)")
    print("=" * 60)
    
    app = create_ui()
    app.queue()
    app.launch(
        server_name="localhost",
        server_port=7860,
        share=False,
        css ="""
            .chatbot .message {
                transition: all 0.2s ease;
            }
            .spinner {
                width: 14px;
                height: 14px;
                border: 2px solid #f3f3f3;
                border-top: 2px solid #3498db;
                border-radius: 50%;
                animation: spin 1s linear infinite;
            }
            @keyframes spin {
                0% { transform: rotate(0deg); }
                100% { transform: rotate(360deg); }
            }
            """
            )


if __name__ == "__main__":
    main()

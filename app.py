"""
Multi-Agent Chatbot UI (Updated)
================================
Gradio ê¸°ë°˜ ì›¹ ì¸í„°í˜ì´ìŠ¤ - ì‹¤ì‹œê°„ Streaming ê°œì„ 

ë³€ê²½ì‚¬í•­:
- chat_streamì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
- ê° Step ê²°ê³¼ë¥¼ ì¦‰ì‹œ ì±„íŒ…ì°½ì— ë°˜ì˜
- ìƒíƒœ í‘œì‹œì¤„ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
"""

import os
import sys
import warnings
from typing import List, Generator, Optional, Dict

warnings.filterwarnings("ignore", category=DeprecationWarning, module="gradio")

import gradio as gr

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from core.shared_storage import SharedStorage
from core.base_tool import ToolRegistry
from core.orchestrator import Orchestrator, StepType
from core.workspace_manager import WorkspaceManager, ConfigManager
from tools.file_tool import FileTool
from tools.llm_tool import LLMTool


# =============================================================================
# Global State
# =============================================================================

class AppState:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ ê´€ë¦¬"""
    
    def __init__(self, workspace_path: str = os.path.join(".", "workspace")):
        self.workspace_path = workspace_path
        
        self.workspace_manager = WorkspaceManager(workspace_path)
        self.config_manager = ConfigManager(os.path.join(workspace_path, "config"))
        
        self.llm_config = self.config_manager.load_llm_config()
        
        self.storage = SharedStorage(debug_enabled=True)
        self.registry = ToolRegistry(debug_enabled=True)
        
        self._setup_tools()
        
        self.orchestrator = Orchestrator(
            tools=self.registry,
            storage=self.storage,
            llm_config=self.llm_config,
            max_steps=self.llm_config.max_steps,
            debug_enabled=True
        )
        
        self.chat_history: List[Dict] = []
        self.system_prompt_history: List[Dict] = []
        self.ollama_connected: bool = False
        self.available_models: List[Dict] = []
    
    def _setup_tools(self):
        """Tools ì„¤ì •"""
        files_dir = os.path.join(self.workspace_path, "files")
        os.makedirs(files_dir, exist_ok=True)

        self.registry.register(FileTool(base_path=files_dir, debug_enabled=True))
        self.registry.register(LLMTool(base_path=files_dir, debug_enabled=True, use_mock=False))
    
    def update_llm_config(self, **kwargs):
        """LLM ì„¤ì • ì—…ë°ì´íŠ¸ ë° ì €ì¥"""
        self.llm_config.update(**kwargs)
        self.orchestrator.update_llm_config(**kwargs)
        
        if 'max_steps' in kwargs:
            self.orchestrator.max_steps = kwargs['max_steps']
        
        self.config_manager.save_llm_config(self.llm_config)
    
    def fetch_ollama_models(self, base_url: str = None) -> tuple[bool, List[Dict]]:
        """Ollama ì„œë²„ì—ì„œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        if base_url is None:
            base_url = self.llm_config.base_url
        
        try:
            import requests
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            response.raise_for_status()
            data = response.json()
            
            models = []
            for model in data.get("models", []):
                name = model.get("name", "unknown")
                size_bytes = model.get("size", 0)
                details = model.get("details", {})
                
                if size_bytes >= 1024 * 1024 * 1024:
                    size_str = f"{size_bytes / (1024**3):.1f}GB"
                else:
                    size_str = f"{size_bytes / (1024**2):.0f}MB"
                
                family = details.get("family", "unknown")
                param_size = details.get("parameter_size", "")
                quantization = details.get("quantization_level", "")
                
                features = []
                if param_size:
                    features.append(param_size)
                if quantization:
                    features.append(quantization)
                if family and family != "unknown":
                    features.append(family)
                
                models.append({
                    "name": name,
                    "size": size_str,
                    "features": ", ".join(features) if features else "N/A",
                    "display": f"{name} ({', '.join(features) if features else size_str})"
                })
            
            self.ollama_connected = True
            self.available_models = models
            return True, models
            
        except Exception as e:
            print(f"[WARN] Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            self.ollama_connected = False
            self.available_models = []
            return False, []


app_state: Optional[AppState] = None


def get_app_state() -> AppState:
    global app_state
    if app_state is None:
        app_state = AppState()
    return app_state


# =============================================================================
# Chat Functions (Updated - Real-time Streaming)
# =============================================================================

def format_tool_result(content: str, tool_name: str, action: str, max_length: int = 300) -> str:
    """Tool ê²°ê³¼ë¥¼ í¬ë§·íŒ… (ê¸¸ì´ ì œí•œ)"""
    content_str = str(content)
    
    if len(content_str) > max_length:
        content_str = content_str[:max_length] + f"\n... ({len(str(content))} chars total)"
    
    return f"```\n{content_str}\n```"


def build_streaming_response(
    plan_content: str,
    step_outputs: List[Dict],
    current_thinking: str = "",
    final_answer: str = ""
) -> str:
    """
    ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬ì„±
    
    Args:
        plan_content: ì‹¤í–‰ ê³„íš
        step_outputs: ì™„ë£Œëœ stepë“¤ì˜ ê²°ê³¼
        current_thinking: í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìƒê°/ìƒíƒœ
        final_answer: ìµœì¢… ë‹µë³€ (ìˆì„ ê²½ìš°)
    """
    parts = []
    
    # 1. ì‹¤í–‰ ê³„íš (ì ‘ì´ì‹)
    if plan_content:
        parts.append(f"<details>\n<summary>ğŸ“‹ ì‹¤í–‰ ê³„íš</summary>\n\n{plan_content}\n</details>")
    
    # 2. ì™„ë£Œëœ Steps (ê°ê° ì ‘ì´ì‹)
    if step_outputs:
        for step in step_outputs:
            header = step.get("header", "")
            result = step.get("result", "")
            if header and result:
                parts.append(header + result)
    
    # 3. í˜„ì¬ ì§„í–‰ ìƒí™© (ì‹¤ì‹œê°„)
    if current_thinking and not final_answer:
        parts.append(f"\nğŸ’­ *{current_thinking}*")
    
    # 4. ìµœì¢… ë‹µë³€
    if final_answer:
        if parts:
            parts.append("\n---\n")
        parts.append(final_answer)
    
    return "\n\n".join(filter(None, parts))


def chat_stream(message: str, history: List[Dict]) -> Generator[List[Dict], None, None]:
    """
    ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬ (Real-time Streaming)
    
    ê° Stepì˜ ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì±„íŒ…ì°½ì— í‘œì‹œí•©ë‹ˆë‹¤.
    """
    state = get_app_state()
    
    state.orchestrator._stopped = False

    if not message.strip():
        yield history, gr.update(interactive=False), gr.update(interactive=True)
        return
    
    # íŒŒì¼ ëª©ë¡ì„ contextë¡œ ì¤€ë¹„ (orchestratorì— ì „ë‹¬)
    files_context = state.workspace_manager.get_files_for_prompt()

    # ìƒˆ ëŒ€í™” ì¶”ê°€ (user ë©”ì‹œì§€)
    history = history + [
        {"role": "user", "content": message},
        {"role": "assistant", "content": ""}
    ]

    # ìƒíƒœ ë³€ìˆ˜
    plan_content = ""
    step_outputs = []
    current_thinking = ""
    final_answer = ""
    current_step_info = {}
    was_stopped = False

    # ì´ˆê¸° ìƒíƒœ í‘œì‹œ
    yield history, gr.update(interactive=False), gr.update(interactive=True)

    try:
        for step in state.orchestrator.run_stream(message, files_context):
            # ===== ì¤‘ì§€ ì²´í¬ =====
            if state.orchestrator._stopped:
                was_stopped = True
                break
            
            # ===== Planning Phase =====
            if step.type == StepType.PLANNING:
                current_thinking = step.content
                history[-1]["content"] = f"ğŸ“‹ *{step.content}*"
                yield history, gr.update(interactive=False), gr.update(interactive=True)
            
            # ===== Plan Ready =====
            elif step.type == StepType.PLAN_READY:
                plan_content = step.content
                current_thinking = "ê³„íš ìˆ˜ë¦½ ì™„ë£Œ, ì‹¤í–‰ ì‹œì‘..."
                
                # ê³„íš í‘œì‹œ
                response = build_streaming_response(
                    plan_content=plan_content,
                    step_outputs=step_outputs,
                    current_thinking=current_thinking
                )
                history[-1]["content"] = response
                yield history, gr.update(interactive=False), gr.update(interactive=True)
            
            # ===== Thinking =====
            elif step.type == StepType.THINKING:
                current_thinking = step.content
                
                response = build_streaming_response(
                    plan_content=plan_content,
                    step_outputs=step_outputs,
                    current_thinking=current_thinking
                )
                history[-1]["content"] = response
                yield history, gr.update(interactive=False), gr.update(interactive=True)
            
            # ===== Tool Call (ì‹œì‘) =====
            elif step.type == StepType.TOOL_CALL:
                tool_name = step.tool_name or "unknown"
                action = step.action or "unknown"
                
                # ìƒˆ step ì‹œì‘
                current_step_info = {
                    "step": step.step,
                    "tool_name": tool_name,
                    "action": action,
                    "header": f"\n<details open>\n<summary>ğŸ”§ Step {step.step}: {tool_name}.{action}</summary>\n\n",
                    "result": "",
                    "status": "running"
                }
                
                # ì§„í–‰ ì¤‘ í‘œì‹œ (ì ‘ì´ì‹ ì—´ë¦° ìƒíƒœ)
                temp_outputs = step_outputs + [{
                    "header": current_step_info["header"],
                    "result": f"â³ *ì‹¤í–‰ ì¤‘...*\n</details>"
                }]
                
                response = build_streaming_response(
                    plan_content=plan_content,
                    step_outputs=temp_outputs,
                    current_thinking=""
                )
                history[-1]["content"] = response
                yield history, gr.update(interactive=False), gr.update(interactive=True)
            
            # ===== Tool Result (ì™„ë£Œ) =====
            elif step.type == StepType.TOOL_RESULT:
                tool_name = step.tool_name or current_step_info.get("tool_name", "unknown")
                action = step.action or current_step_info.get("action", "unknown")
                
                # ê²°ê³¼ í¬ë§·íŒ…
                formatted_result = format_tool_result(step.content, tool_name, action)
                
                # Step ì™„ë£Œ ì •ë³´ ì €ì¥
                completed_step = {
                    "header": current_step_info.get("header", f"\n<details>\n<summary>ğŸ”§ Step {step.step}: {tool_name}.{action}</summary>\n\n"),
                    "result": f"âœ… ì™„ë£Œ\n\n{formatted_result}\n</details>"
                }
                step_outputs.append(completed_step)
                current_step_info = {}
                
                # ì—…ë°ì´íŠ¸
                response = build_streaming_response(
                    plan_content=plan_content,
                    step_outputs=step_outputs,
                    current_thinking=""
                )
                history[-1]["content"] = response
                yield history, gr.update(interactive=False), gr.update(interactive=True)
            
            # ===== Final Answer =====
            elif step.type == StepType.FINAL_ANSWER:
                final_answer = step.content
                
                response = build_streaming_response(
                    plan_content=plan_content,
                    step_outputs=step_outputs,
                    current_thinking="",
                    final_answer=final_answer
                )
                history[-1]["content"] = response
                yield history, gr.update(interactive=False), gr.update(interactive=True)
            
            # ===== Error =====
            elif step.type == StepType.ERROR:
                error_msg = f"âŒ **ì˜¤ë¥˜ ë°œìƒ**\n\n{step.content}"
                
                response = build_streaming_response(
                    plan_content=plan_content,
                    step_outputs=step_outputs,
                    current_thinking="",
                    final_answer=error_msg
                )
                history[-1]["content"] = response
                yield history, gr.update(interactive=True), gr.update(interactive=False)
        
        # ===== ì¤‘ì§€ëœ ê²½ìš° ë©”ì‹œì§€ í‘œì‹œ =====
        if was_stopped:
            stop_msg = "â¹ï¸ **ì‚¬ìš©ìì— ì˜í•´ ì¤‘ì§€ë¨**"
            response = build_streaming_response(
                plan_content=plan_content,
                step_outputs=step_outputs,
                current_thinking="",
                final_answer=stop_msg
            )
            history[-1]["content"] = response
            yield history, gr.update(interactive=True), gr.update(interactive=False)
    
    except GeneratorExit:
        # Gradioê°€ generatorë¥¼ ì¤‘ë‹¨í•  ë•Œ
        state.orchestrator._stopped = True
        stop_msg = "â¹ï¸ **ì¤‘ì§€ë¨**"
        response = build_streaming_response(
            plan_content=plan_content,
            step_outputs=step_outputs,
            current_thinking="",
            final_answer=stop_msg
        )
        history[-1]["content"] = response
        yield history, gr.update(interactive=True), gr.update(interactive=False)

    except Exception as e:
        error_msg = f"âŒ **ì˜ˆì™¸ ë°œìƒ**\n\n{str(e)}"
        history[-1]["content"] = error_msg
        yield history, gr.update(interactive=True), gr.update(interactive=False)
    
    finally:
        # ì„¸ì…˜ ì •ë¦¬
        state.orchestrator._stopped = False
    
    state.chat_history = history


def stop_generation():
    """ìƒì„± ì¤‘ì§€"""
    state = get_app_state()
    state.orchestrator.stop()


def clear_chat():
    """ëŒ€í™” ì´ˆê¸°í™”"""
    state = get_app_state()
    state.chat_history = []
    state.storage.reset()
    state.system_prompt_history = []
    return []


# =============================================================================
# LLM Settings Functions
# =============================================================================

def load_settings_for_modal():
    """ì„¤ì • ëª¨ë‹¬ ì—´ ë•Œ í˜„ì¬ ê°’ ë¡œë“œ"""
    state = get_app_state()
    config = state.llm_config
    
    connected, models = state.fetch_ollama_models(config.base_url)
    
    if connected and models:
        model_choices = [m["display"] for m in models]
        current_display = config.model
        for m in models:
            if m["name"] == config.model:
                current_display = m["display"]
                break
    else:
        model_choices = [config.model]
        current_display = config.model
    
    url_status = "âœ… ì—°ê²°ë¨" if connected else "âŒ ì—°ê²° ì‹¤íŒ¨"
    
    return (
        gr.update(visible=True),
        config.base_url,
        url_status,
        gr.update(choices=model_choices, value=current_display),
        config.temperature,
        config.max_tokens,
        config.top_p,
        config.repeat_penalty,
        config.num_ctx,
        config.max_steps,
        config.timeout
    )


def close_settings_modal():
    return gr.update(visible=False)


def on_url_change(url: str):
    state = get_app_state()
    connected, models = state.fetch_ollama_models(url)
    
    if connected and models:
        model_choices = [m["display"] for m in models]
        current_model = state.llm_config.model
        current_display = current_model
        for m in models:
            if m["name"] == current_model:
                current_display = m["display"]
                break
        
        return (
            "âœ… ì—°ê²°ë¨",
            gr.update(choices=model_choices, value=current_display if current_display in model_choices else model_choices[0])
        )
    else:
        return (
            "âŒ ì—°ê²° ì‹¤íŒ¨",
            gr.update(choices=[state.llm_config.model], value=state.llm_config.model)
        )


def save_settings(url, model_display, temperature, max_tokens, top_p, repeat_penalty, num_ctx, max_steps, timeout):
    state = get_app_state()
    
    model_name = model_display.split(" (")[0] if " (" in model_display else model_display
    
    state.update_llm_config(
        base_url=url,
        model=model_name,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        repeat_penalty=repeat_penalty,
        num_ctx=num_ctx,
        max_steps=max_steps,
        timeout=timeout
    )
    
    return gr.update(visible=False)


# =============================================================================
# File Management Functions
# =============================================================================

def upload_files(files):
    """íŒŒì¼ ì—…ë¡œë“œ"""
    state = get_app_state()
    
    if not files:
        return get_file_list_html(), "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
    
    uploaded = []
    for file in files:
        file_info = state.workspace_manager.save_upload(file.name)
        if file_info:
            uploaded.append(file_info.name)
    
    if uploaded:
        return get_file_list_html(), f"âœ… {len(uploaded)}ê°œ íŒŒì¼ ì—…ë¡œë“œë¨"
    return get_file_list_html(), "âŒ ì—…ë¡œë“œ ì‹¤íŒ¨"


def get_file_list_html() -> str:
    state = get_app_state()
    files = state.workspace_manager.list_files()

    if not files:
        return "<p style='color: gray;'>íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.</p>"
    
    html = "<table style='width:100%; border-collapse:collapse;'>"
    html += "<tr style='background:#f0f0f0;'><th>ì´ë¦„</th><th>í¬ê¸°</th><th>ì¶œì²˜</th></tr>"
    
    for f in files:
        size = f"{f.size / 1024:.1f}KB" if f.size >= 1024 else f"{f.size}B"
        source = "ğŸ“¤ ì—…ë¡œë“œ" if f.source == "upload" else "ğŸ¤– ìƒì„±"
        html += f"<tr><td>{f.name}</td><td>{size}</td><td>{source}</td></tr>"
    
    html += "</table>"
    return html


def get_file_choices() -> List[str]:
    state = get_app_state()
    files = state.workspace_manager.list_files()
    return [f.name for f in files]


def delete_selected_files(selected: List[str]):
    """ì„ íƒëœ íŒŒì¼ ì‚­ì œ"""
    state = get_app_state()
    
    if not selected:
        return get_file_list_html(), "ì‚­ì œí•  íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", gr.update(choices=get_file_choices())
    
    deleted = 0
    for name in selected:
        if state.workspace_manager.delete_file(name):
            deleted += 1
    
    return get_file_list_html(), f"âœ… {deleted}ê°œ íŒŒì¼ ì‚­ì œë¨", gr.update(choices=get_file_choices(), value=[])


def delete_all_files():
    """ì „ì²´ íŒŒì¼ ì‚­ì œ"""
    state = get_app_state()
    count = state.workspace_manager.delete_all_files()
    return get_file_list_html(), f"âœ… {count}ê°œ íŒŒì¼ ì‚­ì œë¨", gr.update(choices=[], value=[])


def refresh_file_list():
    """íŒŒì¼ ëª©ë¡ ìƒˆë¡œê³ ì¹¨"""
    return get_file_list_html(), gr.update(choices=get_file_choices())


# =============================================================================
# SharedStorage Functions
# =============================================================================

def get_shared_storage_tree() -> str:
    state = get_app_state()
    storage = state.storage
    
    html = "<div style='font-family: monospace; font-size: 13px;'>"
    
    context = storage.get_context()
    html += "<details open><summary><b>ğŸ“ Context</b></summary>"
    html += "<div style='margin-left: 20px;'>"
    
    if context:
        user_query = context.get('user_query', 'N/A')
        if user_query and len(user_query) > 50:
            user_query = user_query[:50] + "..."
        html += f"<div>user_query: <code>{user_query}</code></div>"
        html += f"<div>session_id: <code>{context.get('session_id', 'N/A')}</code></div>"
    else:
        html += "<div style='color: gray;'>ì„¸ì…˜ ì—†ìŒ</div>"
    
    html += "</div></details>"
    
    results = storage.get_results()
    html += f"<details open><summary><b>ğŸ“ Results ({len(results)})</b></summary>"
    html += "<div style='margin-left: 20px;'>"
    
    if not results:
        html += "<div style='color: gray;'>ê²°ê³¼ ì—†ìŒ</div>"
    else:
        for i, r in enumerate(results):
            status_icon = "âœ…" if r.get("success", False) else "âŒ"
            tool = r.get("executor", "unknown")
            action = r.get("action", "unknown")
            output_preview = str(r.get("output", ""))
            
            html += f"<details><summary>{status_icon} Step {i+1}: {tool}.{action}</summary>"
            html += f"<div style='margin-left: 20px; background: #f5f5f5; padding: 8px; border-radius: 4px;'>"
            html += f"<pre style='white-space: pre-wrap; margin: 0;'>{output_preview}</pre>"
            html += "</div></details>"
    
    html += "</div></details>"
    
    history = storage.get_history()
    html += f"<details><summary><b>ğŸ“ History ({len(history)})</b></summary>"
    html += "<div style='margin-left: 20px;'>"
    
    if not history:
        html += "<div style='color: gray;'>íˆìŠ¤í† ë¦¬ ì—†ìŒ</div>"
    else:
        for i, h in enumerate(history[-5:]):
            query = h.get("user_query", "")[:30]
            status = "âœ…" if h.get("status") == "completed" else "âš ï¸"
            html += f"<div>{status} {query}...</div>"
    
    html += "</div></details>"
    html += "</div>"
    
    return html


def refresh_shared_storage() -> str:
    return get_shared_storage_tree()


# =============================================================================
# History Functions
# =============================================================================

def get_history_html() -> str:
    state = get_app_state()
    history_data = state.storage.get_history()
    
    if not history_data:
        return "<p style='color: gray;'>íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
    
    html = ""
    for i, session in enumerate(reversed(history_data[-10:])):
        query = session.get("user_query", "")[:50]
        if len(session.get("user_query", "")) > 50:
            query += "..."
        
        status_icon = "âœ…" if session.get("status") == "completed" else "âš ï¸"
        time_str = session.get("created_at", "")[:16]
        
        html += f"""
        <div style='padding:8px; margin:4px 0; background:#f5f5f5; border-radius:4px;'>
            <span>{status_icon}</span>
            <strong>{query}</strong>
            <span style='color:gray; font-size:0.8em;'>{time_str}</span>
        </div>
        """
    
    return html


# =============================================================================
# Build UI
# =============================================================================

def create_ui() -> gr.Blocks:
    """Gradio UI ìƒì„±"""

    def show_toast(message):
        """Gradio toast ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ëŠ” JavaScript í•¨ìˆ˜."""
        js_code = f"""
        gradioApp().showToast("{message}");
        """
        return js_code

    state = get_app_state()
    
    with gr.Blocks(title="Multi-Agent Chatbot") as app:
        
        # Header
        with gr.Row():
            gr.Markdown("# ğŸ¤– Multi-Agent Chatbot")
            settings_btn = gr.Button("âš™ï¸ Settings", scale=1, variant="secondary")
        
        # Settings Modal
        with gr.Column(visible=False, elem_classes=["settings-modal"]) as settings_modal:
            gr.Markdown("## âš™ï¸ LLM Settings")
            
            with gr.Row():
                url_input = gr.Textbox(
                        label="Ollama URL",
                        value=state.llm_config.base_url,
                        placeholder="http://localhost:11434",
                        info="ollama ì„œë²„ ì£¼ì†Œë¥¼ ì…ë ¥í•˜ì„¸ìš”."
                )
                model_dropdown = gr.Dropdown(
                    label="Model",
                    choices=[state.llm_config.model],
                    value=state.llm_config.model,
                    allow_custom_value=True,
                    info="ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”. (ì˜ˆ: gpt-3.5-turbo, llama-2-7b)"
                )
                timeout_slider = gr.Slider(30.0, 1800.0, 300, value=state.llm_config.timeout, label="Timeout",
                                           info="LLM ì‘ë‹µ íƒ€ì„ì•„ì›ƒ ê°’ì„ ì§€ì •í•©ë‹ˆë‹¤.")

            with gr.Row():
                url_status = gr.Markdown("â³", elem_id="url-status")

            
            with gr.Row():
                temperature_slider = gr.Slider(0.0, 2.0, 0.1, value=state.llm_config.temperature, label="Temperature",
                                               info="LLMì˜ ì‘ë‹µì˜ ë¬´ì‘ìœ„ì„±ì„ ì¡°ì ˆí•©ë‹ˆë‹¤. ë†’ì€ ê°’ì¼ìˆ˜ë¡ ë‹µë³€ì˜ ì°½ì˜ì ì„±ì´ ì¦ê°€ í•˜ê³  ì¼ê´€ì„±ì´ ê°ì†Œ í•©ë‹ˆë‹¤.")
                max_tokens_slider = gr.Slider(256, 4096, 256, value=state.llm_config.max_tokens, label="Max Tokens",
                                              info="LLMì´ ìƒì„±í•  ìµœëŒ€ í† í°(ë‹¨ì–´ ë˜ëŠ” ë‹¨ì–´ ì¡°ê°) ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ê°’ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ì‘ë‹µì´ ì˜ë¦´ ìˆ˜ ìˆê³ , ê°’ì´ ë„ˆë¬´ í¬ë©´ ë¶ˆí•„ìš”í•˜ê²Œ ê¸´ ì‘ë‹µì´ ìƒì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                num_ctx_slider = gr.Slider(2048, 32768, 1024, value=state.llm_config.num_ctx, label="Context Window",
                                           info="LLMì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´(í† í° ìˆ˜)ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ê°’ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ LLMì´ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë†“ì¹  ìˆ˜ ìˆê³ , ê°’ì´ ë„ˆë¬´ í¬ë©´ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            with gr.Row():
                top_p_slider = gr.Slider(0.0, 1.0, 0.05, value=state.llm_config.top_p, label="Top-p",
                                         info="LLMì´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì„ íƒí•  ë•Œ ê³ ë ¤í•˜ëŠ” í™•ë¥  ë¶„í¬ì˜ ëˆ„ì  í™•ë¥ ì„ ì¡°ì ˆí•©ë‹ˆë‹¤. ë†’ì€ ê°’ì¼ìˆ˜ë¡ ë‹¤ì–‘í•œ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ê³  ì¼ê´€ì„±ì´ ê°ì†Œ í•©ë‹ˆë‹¤.")
                repeat_penalty_slider = gr.Slider(1.0, 2.0, 0.1, value=state.llm_config.repeat_penalty, label="Repeat Penalty",
                                                  info="LLMì´ ì´ë¯¸ ìƒì„±í•œ ë‹¨ì–´ë¥¼ ë°˜ë³µí•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤. ê°’ì´ ë†’ì„ ìˆ˜ë¡ ë” ë‹¤ì–‘í•œ ì‘ë‹µì„ ìƒì„±í•˜ê³  ê°’ì´ ë‚®ì„ ìˆ˜ë¡ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")
                max_steps_slider = gr.Slider(10, 100, 50, value=state.llm_config.max_steps, label="Max Steps",
                                             info="í•˜ë‚˜ì˜ ìš”ì²­ì— ëŒ€í•´ì„œ ìµœëŒ€ Step(ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜)ì„ ì œí•œ í•©ë‹ˆë‹¤. ê°’ì´ ë†’ì„ ìˆ˜ë¡ ë” ë§ì€ ë„êµ¬ë“¤ì„ í˜¸ì¶œì„ í•  ìˆ˜ ìˆì§€ë§Œ, ê·¸ë§Œí¼ ë” ë§ì€ ì‹œê°„ì´ ì†Œìš” ë©ë‹ˆë‹¤.")
            
            with gr.Row():
                save_btn = gr.Button("ğŸ’¾ Save", variant="primary")
                cancel_btn = gr.Button("Cancel")
        
        # Chat Area
        with gr.Column():
            chatbot = gr.Chatbot(
                label="ëŒ€í™”",
                elem_classes=["chatbot"],
                height=500
            )
            
            with gr.Row():
                msg_input = gr.Textbox(
                    placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”... (ì˜ˆ: sample.c íŒŒì¼ì„ ì½ì–´ì„œ ì •ì  ë¶„ì„í•˜ê³  out.mdì— ì €ì¥í•´ì¤˜)",
                    label="",
                    scale=10,
                    container=False
                )
                send_btn = gr.Button("â–¶ï¸ì „ì†¡", variant="primary", scale=1, interactive=True)
                stop_btn = gr.Button("â¹ï¸ì¤‘ì§€", variant="stop", scale=1, interactive=False)
                clear_btn = gr.Button("ğŸ—‘ï¸ì‚­ì œ", scale=1)
        
        gr.Markdown("---")
        
        # Tab Panels
        with gr.Tabs():
            
            # Workspace Files Tab
            with gr.TabItem("ğŸ“ Workspace Files"):
                with gr.Row():
                    file_upload = gr.File(label="íŒŒì¼ ì—…ë¡œë“œ", file_count="multiple", file_types=None)
                    upload_btn = gr.Button("ğŸ“¤ ì—…ë¡œë“œ", size="sm")
                
                file_list_html = gr.HTML(get_file_list_html())
                
                with gr.Row():
                    file_select = gr.CheckboxGroup(choices=get_file_choices(), label="ì‚­ì œí•  íŒŒì¼ ì„ íƒ")
                
                with gr.Row():
                    delete_selected_btn = gr.Button("ğŸ—‘ï¸ ì„ íƒ ì‚­ì œ", size="sm")
                    delete_all_btn = gr.Button("ğŸ—‘ï¸ ì „ì²´ ì‚­ì œ", size="sm", variant="stop")
                    refresh_files_btn = gr.Button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", size="sm")
                
                file_status = gr.Markdown("")
            
            # SharedStorage Tab
            with gr.TabItem("ğŸ’¾ SharedStorage"):
                storage_tree = gr.HTML(get_shared_storage_tree())
                refresh_storage_btn = gr.Button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", size="sm")
            
            # History Tab
            with gr.TabItem("ğŸ“œ History"):
                history_html = gr.HTML(get_history_html())
                refresh_history_btn = gr.Button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", size="sm")
        
        # =================================================================
        # Event Handlers
        # =================================================================
        
        # Settings Modal
        settings_btn.click(
            fn=load_settings_for_modal,
            outputs=[settings_modal, url_input, url_status, model_dropdown,
                    temperature_slider, max_tokens_slider, top_p_slider,
                    repeat_penalty_slider, num_ctx_slider, max_steps_slider, timeout_slider]
        )

        cancel_btn.click(fn=close_settings_modal, outputs=[settings_modal])
        
        url_input.change(fn=on_url_change, inputs=[url_input], outputs=[url_status, model_dropdown])
        
        save_btn.click(
            fn=save_settings,
            inputs=[url_input, model_dropdown, temperature_slider, max_tokens_slider,
                   top_p_slider, repeat_penalty_slider, num_ctx_slider, max_steps_slider, timeout_slider],
            outputs=[settings_modal]
        )
        
        # Chat events
        def chat_stream_with_clear(message: str, history: List[Dict]):
            """
            Wrapper that clears input immediately on submit, then streams chat.

            First yield: clears input field immediately
            Subsequent yields: stream from chat_stream
            """
            # First yield to clear input immediately
            yield history, gr.update(interactive=False), gr.update(interactive=True), ""

            # Then stream the actual chat
            for chatbot_update, send_update, stop_update in chat_stream(message, history):
                yield chatbot_update, send_update, stop_update, ""

        def on_chat_complete():
            return get_history_html(), get_shared_storage_tree(), gr.update(interactive=True), gr.update(interactive=False)
        
        msg_input.submit(
            fn=chat_stream_with_clear,
            inputs=[msg_input, chatbot],
            outputs=[chatbot, send_btn, stop_btn, msg_input]
        ).then(
            fn=on_chat_complete,
            outputs=[history_html, storage_tree, send_btn, stop_btn]
        )
        
        send_btn.click(
            fn=chat_stream_with_clear,
            inputs=[msg_input, chatbot],
            outputs=[chatbot, send_btn, stop_btn, msg_input]
        ).then(
            fn=on_chat_complete,
            outputs=[history_html, storage_tree, send_btn, stop_btn]
        )
        
        stop_btn.click(fn=stop_generation)
        
        clear_btn.click(fn=clear_chat, outputs=[chatbot]).then(
            fn=on_chat_complete,
            outputs=[history_html, storage_tree, send_btn, stop_btn]
        )
        
        # File management
        upload_btn.click(
            fn=upload_files,
            inputs=[file_upload],
            outputs=[file_list_html, file_status]
        ).then(
            fn=lambda: gr.update(choices=get_file_choices()),
            outputs=[file_select]
        )
        
        delete_selected_btn.click(
            fn=delete_selected_files,
            inputs=[file_select],
            outputs=[file_list_html, file_status, file_select]
        )
        
        delete_all_btn.click(
            fn=delete_all_files,
            outputs=[file_list_html, file_status, file_select]
        )
        
        refresh_files_btn.click(fn=refresh_file_list, outputs=[file_list_html, file_select])
        
        # SharedStorage & History
        refresh_storage_btn.click(fn=refresh_shared_storage, outputs=[storage_tree])
        refresh_history_btn.click(fn=get_history_html, outputs=[history_html])
        
        # Page Load
        def on_page_load():
            return (
                get_file_list_html(),
                gr.update(choices=get_file_choices()),
                get_shared_storage_tree(),
                get_history_html()
            )
        
        app.load(fn=on_page_load, outputs=[file_list_html, file_select, storage_tree, history_html])

    return app


# =============================================================================
# Main
# =============================================================================

def main():
    print("=" * 60)
    print("Multi-Agent Chatbot (with Real-time Streaming)")
    print("=" * 60)
    
    app = create_ui()
    app.launch(
        server_name="localhost",
        server_port=7860,
        share=False,
        css ="""
            .chatbot .message {
                transition: all 0.2s ease;
            }
            """
    )


if __name__ == "__main__":
    main()

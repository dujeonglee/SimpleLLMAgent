#!/usr/bin/env python3
"""
LLM Agent with Ollama - GUI ë²„ì „ (Streaming ì§€ì›)
"""

import os
import json
import re
import sys
import threading
import urllib.parse
import urllib.request
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path
from tkinter import messagebox

try:
    import tkinter as tk
    from tkinter import scrolledtext, ttk
except ImportError:
    print("Error: tkinter not installed")
    print("Run: sudo apt-get install python3-tk")
    sys.exit(1)

# ============================================================================
# ì „ì—­ ë³€ìˆ˜ - ì €ì¥ì†Œ
# ============================================================================

LLM_RESPONSE_STORAGE: Dict[str, Any] = {}      # ask_llm ê²°ê³¼ ì €ì¥ì†Œ
TOOL_RESULT_STORAGE: Dict[str, Any] = {}       # Tool ì‹¤í–‰ ê²°ê³¼ ì €ì¥ì†Œ

# LLM í´ë¼ì´ì–¸íŠ¸ ì°¸ì¡° (ask_llmì—ì„œ ì‚¬ìš©)
_OLLAMA_CLIENT: Optional['OllamaClient'] = None
_CURRENT_MODEL: Optional[str] = None

# ============================================================================
# Tool ê²°ê³¼ ì €ì¥ì†Œ ê´€ë¦¬ í•¨ìˆ˜
# ============================================================================

def store_tool_result(key: str, data: Any) -> None:
    """Tool ì‹¤í–‰ ê²°ê³¼ë¥¼ ì €ì¥"""
    global TOOL_RESULT_STORAGE
    TOOL_RESULT_STORAGE[key] = data

def get_tool_result(key: str) -> Any:
    """ì €ì¥ëœ Tool ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
    return TOOL_RESULT_STORAGE.get(key)

def resolve_references(arguments: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì¸ìì—ì„œ $key ì°¸ì¡°ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜

    ì˜ˆ: {"context": "$file_content"} -> {"context": "ì‹¤ì œ íŒŒì¼ ë‚´ìš©..."}

    ì§€ì›í•˜ëŠ” ì°¸ì¡° í˜•ì‹:
    - $key: TOOL_RESULT_STORAGE[key] ì „ì²´ ê°’
    - $key.field: TOOL_RESULT_STORAGE[key]["field"] íŠ¹ì • í•„ë“œ
    """
    resolved = {}

    for param_name, param_value in arguments.items():
        if isinstance(param_value, str) and param_value.startswith("$"):
            # $key ë˜ëŠ” $key.field í˜•ì‹ íŒŒì‹±
            ref = param_value[1:]  # $ ì œê±°

            if "." in ref:
                # $key.field í˜•ì‹
                key, field = ref.split(".", 1)
                stored_data = TOOL_RESULT_STORAGE.get(key)

                if stored_data is None:
                    raise ValueError(f"Reference '{param_value}' not found. Available keys: {list(TOOL_RESULT_STORAGE.keys())}")

                if isinstance(stored_data, dict) and field in stored_data:
                    resolved[param_name] = stored_data[field]
                else:
                    raise ValueError(f"Field '{field}' not found in '{key}'. Available fields: {list(stored_data.keys()) if isinstance(stored_data, dict) else 'N/A'}")
            else:
                # $key í˜•ì‹
                key = ref
                stored_data = TOOL_RESULT_STORAGE.get(key)

                if stored_data is None:
                    raise ValueError(f"Reference '{param_value}' not found. Available keys: {list(TOOL_RESULT_STORAGE.keys())}")

                resolved[param_name] = stored_data
        else:
            resolved[param_name] = param_value

    return resolved

def list_stored_keys() -> Dict[str, List[str]]:
    """ì €ì¥ëœ ëª¨ë“  í‚¤ ëª©ë¡ ë°˜í™˜"""
    result = {
        "tool_results": list(TOOL_RESULT_STORAGE.keys()),
        "llm_responses": list(LLM_RESPONSE_STORAGE.keys())
    }
    return result

# ============================================================================
# ë„êµ¬ ì •ì˜
# ============================================================================

def get_current_time() -> Dict[str, str]:
    """í˜„ì¬ ì‹œê°„"""
    now = datetime.now()
    return {
        "current_time": now.strftime("%Y-%m-%d %H:%M:%S"),
        "current_week": now.strftime("%W"),
        "day_of_week": now.strftime("%A")
    }

def get_file(base_dir: str = ".", pattern: str = "*") -> Dict[str, Any]:
    """
    í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  íŒŒì¼ì„ ìƒëŒ€ ê²½ë¡œë¡œ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜.
    """
    try:
        base_path = Path(base_dir).resolve()

        # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not base_path.exists():
            return {
                "base_dir": base_dir,
                "files": [],
                "count": 0,
                "result": "failure",
                "error": f"Directory '{base_dir}' does not exist"
            }

        # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  íŒŒì¼ ì°¾ê¸°
        if pattern == "*":
            all_files = [
                str(f.relative_to(base_path))
                for f in base_path.rglob("*")
                if f.is_file()
            ]
        else:
            all_files = [
                str(f.relative_to(base_path))
                for f in base_path.rglob(pattern)
                if f.is_file()
            ]
        
        result = {
            "base_dir": str(base_path),
            "files": sorted(all_files),
            "count": len(all_files),
            "result": "success"
        }

        return result

    except Exception as e:
        return {
            "base_dir": base_dir,
            "files": [],
            "count": 0,
            "result": "failure",
            "error": str(e)
        }

def read_file(file_path: str, encoding: str = "utf-8") -> Dict[str, Any]:
    """
    íŒŒì¼ì˜ ë‚´ìš©ì„ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜.
    """
    try:
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(file_path):
            return {
                "filename": file_path,
                "content": None,
                "result": "failure",
                "error": f"File '{file_path}' does not exist"
            }
        
        with open(file_path, 'r', encoding=encoding) as f:
            content = f.read()

        file_size = os.path.getsize(file_path)

        result = {
            "filename": file_path,
            "content": content,
            "size": file_size,
            "lines": len(content.splitlines()),
            "result": "success"
        }

        return result

    except UnicodeDecodeError:
        # ë°”ì´ë„ˆë¦¬ íŒŒì¼ ì²˜ë¦¬
        try:
            with open(file_path, 'rb') as f:
                binary_content = f.read()

            return {
                "filename": file_path,
                "content": f"<binary file, {len(binary_content)} bytes>",
                "size": len(binary_content),
                "result": "success",
                "note": "Binary file, content not displayed"
            }
        except Exception as e:
            return {
                "filename": file_path,
                "content": None,
                "result": "failure",
                "error": str(e)
            }
    except Exception as e:
        return {
            "filename": file_path,
            "content": None,
            "result": "failure",
            "error": str(e)
        }

def write_file(file_path: str, content: object) -> Dict[str, Any]:
    """
    íŒŒì¼ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    """
    try:
        # ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ì¶œ
        dir_path = os.path.dirname(file_path)

        # ë””ë ‰í† ë¦¬ê°€ ìˆê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ìƒì„±
        if dir_path and not os.path.exists(dir_path):
            os.makedirs(dir_path, exist_ok=True)

        # íŒŒì¼ ì €ì¥
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(str(content))

        file_size = os.path.getsize(file_path)

        result = {
            "filename": file_path,
            "size": file_size,
            "result": "success"
        }

        return result

    except Exception as e:
        return {
            "filename": file_path,
            "result": "failure",
            "error": str(e)
        }

def delete_file(file_path: str) -> Dict[str, Any]:
    """Deletes a file."""
    try:
        if not os.path.exists(file_path):
            return {
                "filename": file_path,
                "result": "failure",
                "error": f"File '{file_path}' does not exist"
            }

        os.remove(file_path)
        return {
                "filename": file_path,
                "result": "success",
        }

    except Exception as e:
        return {
            "filename": file_path,
            "result": "failure",
            "error": str(e)
        }

def ask_llm(key: str, query: str, context: str = "") -> Dict[str, Any]:
    """
    LLMì— ì¿¼ë¦¬ë¥¼ ë³´ë‚´ê³  ê²°ê³¼ë¥¼ ì „ì—­ ì €ì¥ì†Œì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜.

    Args:
        key: ê²°ê³¼ë¥¼ ì €ì¥í•  í‚¤ ì´ë¦„
        query: LLMì— ë³´ë‚¼ ì§ˆë¬¸/ìš”ì²­
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì˜ˆ: íŒŒì¼ ë‚´ìš©, ì´ì „ ê²°ê³¼ ë“±)
    """
    global LLM_RESPONSE_STORAGE, _OLLAMA_CLIENT, _CURRENT_MODEL

    try:
        if _OLLAMA_CLIENT is None or _CURRENT_MODEL is None:
            return {
                "key": key,
                "result": "failure",
                "error": "LLM client not initialized. Please connect first."
            }

        # ì´ë¯¸ ê°™ì€ í‚¤ê°€ ìˆìœ¼ë©´ ê²½ê³ 
        if key in LLM_RESPONSE_STORAGE:
            existing_warning = f"Warning: Key '{key}' already exists and will be overwritten."
        else:
            existing_warning = None

        # ë©”ì‹œì§€ êµ¬ì„±
        if context:
            full_prompt = f"""Context:
{context}

Request:
{query}

Please provide a detailed and helpful response."""
        else:
            full_prompt = query

        messages = [
            {"role": "user", "content": full_prompt}
        ]

        # LLM í˜¸ì¶œ (non-streaming)
        response = _OLLAMA_CLIENT.chat_stream(
            model=_CURRENT_MODEL,
            messages=messages,
            temperature=0.7,
            max_tokens=4000,
            callback=None  # ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ ì „ì²´ ì‘ë‹µ ë°›ê¸°
        )

        # ê²°ê³¼ ì €ì¥
        LLM_RESPONSE_STORAGE[key] = {
            "query": query,
            "context_provided": bool(context),
            "response": response,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        result = {
            "key": key,
            "result": "success",
            "response_length": len(response),
            "response_preview": response[:500] + "..." if len(response) > 500 else response,
            "stored_keys": list(LLM_RESPONSE_STORAGE.keys())
        }

        if existing_warning:
            result["warning"] = existing_warning

        return result

    except Exception as e:
        return {
            "key": key,
            "result": "failure",
            "error": str(e)
        }

def get_llm_response(key: str, is_remove: bool = False) -> Dict[str, Any]:
    """
    ì €ì¥ëœ LLM ì‘ë‹µì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜.

    Args:
        key: ê°€ì ¸ì˜¬ ì‘ë‹µì˜ í‚¤
        is_remove: Trueë©´ ê°€ì ¸ì˜¨ í›„ í•´ë‹¹ ì—”íŠ¸ë¦¬ ì‚­ì œ
    """
    global LLM_RESPONSE_STORAGE

    try:
        if key not in LLM_RESPONSE_STORAGE:
            return {
                "key": key,
                "result": "failure",
                "error": f"Key '{key}' not found in storage",
                "available_keys": list(LLM_RESPONSE_STORAGE.keys())
            }

        data = LLM_RESPONSE_STORAGE[key]

        result = {
            "key": key,
            "result": "success",
            "data": data,
            "removed": is_remove
        }

        if is_remove:
            del LLM_RESPONSE_STORAGE[key]
            result["remaining_keys"] = list(LLM_RESPONSE_STORAGE.keys())

        return result

    except Exception as e:
        return {
            "key": key,
            "result": "failure",
            "error": str(e)
        }

def list_storage() -> Dict[str, Any]:
    """
    ëª¨ë“  ì €ì¥ì†Œì˜ í‚¤ ëª©ë¡ê³¼ ê°„ë‹¨í•œ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    """
    tool_info = {}
    for key, value in TOOL_RESULT_STORAGE.items():
        if isinstance(value, dict):
            tool_info[key] = {
                "type": "dict",
                "fields": list(value.keys())
            }
        elif isinstance(value, str):
            tool_info[key] = {
                "type": "string",
                "length": len(value),
                "preview": value[:100] + "..." if len(value) > 100 else value
            }
        else:
            tool_info[key] = {
                "type": type(value).__name__
            }

    llm_info = {}
    for key, value in LLM_RESPONSE_STORAGE.items():
        if isinstance(value, dict):
            llm_info[key] = {
                "fields": list(value.keys()),
                "response_preview": value.get("response", "")[:100] + "..." if len(value.get("response", "")) > 100 else value.get("response", "")
            }

    return {
        "result": "success",
        "tool_results": tool_info,
        "llm_responses": llm_info,
        "tool_result_count": len(TOOL_RESULT_STORAGE),
        "llm_response_count": len(LLM_RESPONSE_STORAGE)
    }

TOOLS = {
    "get_current_time": {
        "function": get_current_time,
        "description": "Get current date and time",
        "parameters": {}
    },
    "get_file": {
        "function": get_file,
        "description": "Recursively get all files in a directory with relative paths",
        "parameters": {
            "base_dir": {
                "type": "string",
                "required": False,
                "default": ".",
                "description": "Base directory to search from (default: current directory)"
            },
            "pattern": {
                "type": "string",
                "required": False,
                "default": "*",
                "description": "File pattern to match (e.g., '*.c', '*.py', default: '*' for all files)"
            }
        }
    },
    "read_file": {
        "function": read_file,
        "description": "Read the contents of a file",
        "parameters": {
            "file_path": {
                "type": "string",
                "required": True,
                "description": "Path to the file to read"
            },
            "encoding": {
                "type": "string",
                "required": False,
                "default": "utf-8",
                "description": "File encoding (default: utf-8)"
            }
        }
    },
    "write_file": {
        "function": write_file,
        "description": "Write content to a file",
        "parameters": {
            "file_path": {
                "type": "string",
                "required": True,
                "description": "Path where the file should be written"
            },
            "content": {
                "type": "string",
                "required": True,
                "description": "Content to write to the file"
            }
        }
    },
    "delete_file": {
        "function": delete_file,
        "description": "Delete a file",
        "parameters": {
            "file_path": {
                "type": "string",
                "required": True,
                "description": "Path to the file to delete"
            }
        }
    },
    "ask_llm": {
        "function": ask_llm,
        "description": "Send a query to LLM and store the response with a key. Use this for complex analysis tasks that need separate LLM processing.",
        "parameters": {
            "key": {
                "type": "string",
                "required": True,
                "description": "Unique key to store the response (e.g., 'analysis_result', 'code_review')"
            },
            "query": {
                "type": "string",
                "required": True,
                "description": "The question or request to send to LLM"
            },
            "context": {
                "type": "string",
                "required": False,
                "default": "",
                "description": "Additional context like file content, previous results, etc."
            }
        }
    },
    "get_llm_response": {
        "function": get_llm_response,
        "description": "Retrieve a stored LLM response by key. Use this to get results from previous ask_llm calls.",
        "parameters": {
            "key": {
                "type": "string",
                "required": True,
                "description": "The key of the stored response to retrieve"
            },
            "is_remove": {
                "type": "boolean",
                "required": False,
                "default": False,
                "description": "If true, delete the entry after retrieving (default: false)"
            }
        }
    },
    "list_storage": {
        "function": list_storage,
        "description": "List all stored keys in tool_results and llm_responses storage. Use to see available $key references.",
        "parameters": {}
    }
}

class OllamaClient:
    """Ollama API í´ë¼ì´ì–¸íŠ¸ with streaming"""

    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url

    def chat_stream(self, model: str, messages: List[Dict[str, str]], 
                   temperature: float = 0.7, max_tokens: int = 4000,
                   callback=None) -> str:
        """
        â­ Streaming chat - ì‹¤ì‹œê°„ìœ¼ë¡œ í† í° ìƒì„±
        
        Args:
            callback: ê° í† í°ë§ˆë‹¤ í˜¸ì¶œë  í•¨ìˆ˜ callback(token)
        """
        try:
            payload = {
                "model": model,
                "messages": messages,
                "stream": True,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            }

            headers = {'Content-Type': 'application/json'}
            data = json.dumps(payload).encode('utf-8')
            req = urllib.request.Request(f"{self.base_url}/api/chat", data=data, headers=headers)
            with urllib.request.urlopen(req, timeout=1800) as response:
                if response.getcode() != 200:
                    raise Exception(f"Ollama API error: {response.getcode()}")
                full_content = ""
                for line in response:
                    if line:
                        try:
                            chunk = json.loads(line.decode('utf-8'))
                            if "message" in chunk:
                                content = chunk["message"].get("content", "")
                                full_content += content

                                # â­ ì½œë°±ìœ¼ë¡œ ì‹¤ì‹œê°„ ì „ë‹¬
                                if callback:
                                    callback(content)

                            if chunk.get("done", False):
                                break
                        except json.JSONDecodeError:
                            continue

                return full_content

        except Exception as e:
            raise Exception(f"Ollama error: {str(e)}")

# ============================================================================
# Agent
# ============================================================================

class OllamaAgent:
    """Ollama Agent"""

    def __init__(self, ollama_url: str, model: str):
        global _OLLAMA_CLIENT, _CURRENT_MODEL

        self.ollama = OllamaClient(ollama_url)
        self.model = model
        self.conversation_history = []

        # ì „ì—­ ë³€ìˆ˜ì— í´ë¼ì´ì–¸íŠ¸ ì°¸ì¡° ì €ì¥ (ask_llmì—ì„œ ì‚¬ìš©)
        _OLLAMA_CLIENT = self.ollama
        _CURRENT_MODEL = self.model

    def _create_system_prompt(self) -> str:
        tools_desc = []
        for name, info in TOOLS.items():
            desc = f"- {name}: {info['description']}"

            # íŒŒë¼ë¯¸í„° ìƒì„¸ ì •ë³´
            if info['parameters']:
                params = []
                for param_name, param_info in info['parameters'].items():
                    required = "required" if param_info['required'] else "optional"
                    param_type = param_info['type']
                    param_str = f"{param_name} ({param_type}, {required}"

                    if 'default' in param_info:
                        param_str += f", default={param_info['default']}"
                    param_str += ")"

                    if 'description' in param_info:
                        param_str += f" - {param_info['description']}"

                    params.append(param_str)

                desc += "\n  Parameters:\n    " + "\n    ".join(params)
            else:
                desc += "\n  Parameters: None"

            tools_desc.append(desc)

        tools_text = "\n\n".join(tools_desc)

        return f"""You are a WiFi driver development assistant.

Available tools:
{tools_text}

When you need a tool, respond EXACTLY in this format:
TOOL_CALL: tool_name
ARGUMENTS: {{"param_name": "value"}}
STORE_AS: key_name

Important guidelines:
- Use correct JSON types: strings in "quotes", numbers without quotes, booleans as true/false
- All required parameters must be provided
- Optional parameters can be omitted (defaults will be used)
- Follow the parameter descriptions carefully

â­ REFERENCE SYSTEM - CRITICAL:
Every tool result is automatically stored with the key specified in STORE_AS.
You can reference stored data using $key syntax in arguments:
- $key: Get the entire stored result
- $key.field: Get a specific field from the stored result

Example workflow:
1. Read a file:
   TOOL_CALL: read_file
   ARGUMENTS: {{"file_path": "hello.c"}}
   STORE_AS: source_code

2. Analyze with LLM (reference the file content):
   TOOL_CALL: ask_llm
   ARGUMENTS: {{"key": "analysis", "query": "Analyze this code", "context": "$source_code.content"}}
   STORE_AS: llm_result

3. Save the analysis:
   TOOL_CALL: write_file
   ARGUMENTS: {{"file_path": "report.md", "content": "$analysis.response"}}
   STORE_AS: save_result

Common field references:
- $key.content: File content from read_file
- $key.files: File list from get_file  
- $key.response: LLM response from ask_llm (via get_llm_response)
- $key.result: Success/failure status

Use list_storage tool to see all available keys and their fields.
Do NOT generate large content in arguments - always use $key references!

Language guideline:
- ALWAYS respond in the same language the user is using
- If user writes in Korean (í•œê¸€), respond in Korean
- If user writes in English, respond in English

Be concise and helpful."""

    def _parse_tool_call(self, response: str) -> Optional[Dict[str, Any]]:
        tool_match = re.search(r'TOOL_CALL:\s*(\w+)', response, re.IGNORECASE)
        if not tool_match:
            return None

        tool_name = tool_match.group(1)
        args_match = re.search(r'ARGUMENTS:\s*({.*?})', response, re.DOTALL | re.IGNORECASE)

        arguments = {}
        if args_match:
            try:
                arguments = json.loads(args_match.group(1))
            except:
                pass

        # STORE_AS í‚¤ ì¶”ì¶œ
        store_match = re.search(r'STORE_AS:\s*(\w+)', response, re.IGNORECASE)
        store_as = store_match.group(1) if store_match else None

        return {"tool": tool_name, "arguments": arguments, "store_as": store_as}

    def _execute_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        if tool_name not in TOOLS:
            return {"error": f"Unknown tool: {tool_name}"}

        try:
            # $key ì°¸ì¡° í•´ê²°
            resolved_args = resolve_references(arguments)
            return TOOLS[tool_name]["function"](**resolved_args)
        except ValueError as e:
            # ì°¸ì¡° í•´ê²° ì‹¤íŒ¨
            return {"error": f"Reference error: {str(e)}"}
        except Exception as e:
            return {"error": str(e)}

    def _summarize_tool_result(self, result: Any, store_as: Optional[str]) -> str:
        """Tool ê²°ê³¼ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½"""
        if isinstance(result, dict):
            summary_parts = []

            # ê²°ê³¼ ìƒíƒœ
            if "result" in result:
                summary_parts.append(f"status: {result['result']}")

            # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ í‘œì‹œ
            if "error" in result:
                summary_parts.append(f"error: {result['error']}")
                return "{" + ", ".join(summary_parts) + "}"

            # ì£¼ìš” í•„ë“œ ìš”ì•½
            for key, value in result.items():
                if key in ["result", "error"]:
                    continue

                if isinstance(value, str):
                    if len(value) > 100:
                        summary_parts.append(f"{key}: <{len(value)} chars>")
                    else:
                        summary_parts.append(f"{key}: \"{value[:50]}...\"" if len(value) > 50 else f"{key}: \"{value}\"")
                elif isinstance(value, list):
                    summary_parts.append(f"{key}: [{len(value)} items]")
                elif isinstance(value, dict):
                    summary_parts.append(f"{key}: {{...}}")
                else:
                    summary_parts.append(f"{key}: {value}")

            summary = "{" + ", ".join(summary_parts) + "}"

            if store_as:
                summary += f"\nâ†’ Stored as ${store_as}"
                # ì‚¬ìš© ê°€ëŠ¥í•œ í•„ë“œ íŒíŠ¸
                fields = [k for k in result.keys() if k != "result"]
                if fields:
                    summary += f"\nâ†’ Available: ${store_as}.{', ${store_as}.'.join(fields[:5])}"

            return summary
        else:
            return str(result)[:200]

    def chat(self, user_message: str, stream_callback=None, 
             status_callback=None, confirm_callback=None, max_iterations: int = 5,
             max_tokens: int = 4000) -> str:
        """
        Agent ë©”ì¸ ë£¨í”„

        Args:
            stream_callback: ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ì½œë°±
            status_callback: ìƒíƒœ ë©”ì‹œì§€ ì½œë°±
            confirm_callback: ë„êµ¬ ì‹¤í–‰ í™•ì¸ ì½œë°± - confirm_callback(tool_name, arguments) -> bool
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        """
        self.conversation_history.append({
            "role": "user",
            "content": user_message
        })

        for iteration in range(max_iterations):
            if status_callback:
                status_callback(f"ğŸ”„ Iteration {iteration + 1}")

            # â­ ë§¤ iterationë§ˆë‹¤ ìµœì‹  íˆìŠ¤í† ë¦¬ë¡œ messages ìƒì„±
            messages = [
                {"role": "system", "content": self._create_system_prompt()}
            ] + self.conversation_history

            try:
                # LLM í˜¸ì¶œ (streaming)
                llm_response = self.ollama.chat_stream(
                    model=self.model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=max_tokens,  # íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ ê°’ ì‚¬ìš©
                    callback=stream_callback
                )

                # ë„êµ¬ í˜¸ì¶œ í™•ì¸
                tool_call = self._parse_tool_call(llm_response)

                if tool_call is None:
                    # ìµœì¢… ì‘ë‹µ
                    self.conversation_history.append({
                        "role": "assistant",
                        "content": llm_response
                    })
                    if status_callback:
                        status_callback("âœ… Complete")
                    return llm_response

                # ë„êµ¬ ì‹¤í–‰
                tool_name = tool_call["tool"]
                arguments = tool_call["arguments"]
                store_as = tool_call["store_as"]

                # â­ ì‚¬ìš©ìì—ê²Œ í™•ì¸ ìš”ì²­
                if confirm_callback:
                    if status_callback:
                        status_callback(f"â¸ï¸ Waiting for confirmation...")

                    confirmed = confirm_callback(tool_name, arguments)

                    if not confirmed:
                        # ì‚¬ìš©ìê°€ ê±°ë¶€í•¨
                        if status_callback:
                            status_callback("âŒ Tool execution cancelled by user")

                        return "Tool execution was cancelled by user. How would you like to proceed?"

                if status_callback:
                    status_callback(f"ğŸ”§ Calling tool: {tool_name}")

                tool_result = self._execute_tool(tool_name, arguments)

                # â­ ê²°ê³¼ë¥¼ ì €ì¥ì†Œì— ì €ì¥
                if store_as:
                    store_tool_result(store_as, tool_result)
                    if status_callback:
                        status_callback(f"ğŸ’¾ Stored as: ${store_as}")

                if status_callback:
                    status_callback(f"ğŸ“Š Tool completed")

                # ê²°ê³¼ë¥¼ ëŒ€í™”ì— ì¶”ê°€
                self.conversation_history.append({
                    "role": "assistant",
                    "content": llm_response
                })

                # â­ ê°„ê²°í•œ ê²°ê³¼ ë©”ì‹œì§€ (ì „ì²´ ë°ì´í„° ëŒ€ì‹  ìš”ì•½ë§Œ)
                result_summary = self._summarize_tool_result(tool_result, store_as)
                tool_result_message = f"""TOOL_RESULT:
{result_summary}

Available keys: {list(TOOL_RESULT_STORAGE.keys())}
Use $key or $key.field syntax to reference stored data in next tool calls."""

                self.conversation_history.append({
                    "role": "user",
                    "content": tool_result_message
                })

                # ì¤„ë°”ê¿ˆ ì¶”ê°€
                if stream_callback:
                    stream_callback("\n\n")

            except Exception as e:
                if status_callback:
                    status_callback(f"âŒ Error: {str(e)}")
                return f"Error: {str(e)}"

        return "Max iterations reached"

    def reset(self):
        global LLM_RESPONSE_STORAGE, TOOL_RESULT_STORAGE
        self.conversation_history = []
        # ëŒ€í™” ì´ˆê¸°í™” ì‹œ ì €ì¥ì†Œë„ í´ë¦¬ì–´
        LLM_RESPONSE_STORAGE = {}
        TOOL_RESULT_STORAGE = {}


# ============================================================================
# GUI
# ============================================================================

class AgentGUI:
    """GUI Application"""

    def __init__(self, root):
        self.root = root
        self.root.title("Agent ğŸ¤–")
        self.root.geometry("1000x800")

        self.agent = None
        self.processing = False
        self.confirm_tool_execution = tk.BooleanVar(value=True)  # ê¸°ë³¸ê°’: í™•ì¸ ìš”ì²­

        self.setup_ui()

    def setup_ui(self):
        """UI êµ¬ì„±"""

        # ===== ì„¤ì • í”„ë ˆì„ =====
        config_frame = ttk.LabelFrame(self.root, text="âš™ï¸ Ollama ì„¤ì •", padding=10)
        config_frame.pack(fill=tk.X, padx=10, pady=5)

        # URL
        ttk.Label(config_frame, text="URL:").grid(row=0, column=0, padx=5, sticky=tk.W)
        self.url_entry = ttk.Entry(config_frame, width=35)
        self.url_entry.insert(0, "http://192.168.0.30:11434")
        self.url_entry.grid(row=0, column=1, padx=5)

        # Model
        def _model_update(event):
            self.agent = OllamaAgent(self.url_entry.get(), self.model_entry.get())
        ttk.Label(config_frame, text="Model:").grid(row=0, column=2, padx=5, sticky=tk.W)
        self.model_var = tk.StringVar(value="llama3.1")
        self.model_entry = ttk.Combobox(config_frame, state="readonly", width=30)
        self.model_entry.bind("<<ComboboxSelected>>", _model_update)
        self.model_entry.grid(row=0, column=3, padx=5)

        # Connect button
        self.refresh_btn = ttk.Button(config_frame, text="Refresh", command=self.connect)
        self.refresh_btn.grid(row=0, column=4, padx=10)

        # Status
        self.status_label = ttk.Label(config_frame, text="â— Not connected", foreground="red")
        self.status_label.grid(row=0, column=5, padx=10)

        # Max tokens ì„¤ì •
        ttk.Label(config_frame, text="Max tokens:").grid(row=1, column=0, padx=5, sticky=tk.W, pady=5)
        self.max_tokens_var = tk.IntVar(value=4000)
        max_tokens_spinbox = ttk.Spinbox(
            config_frame, 
            from_=1000, 
            to=32000, 
            increment=1000,
            textvariable=self.max_tokens_var,
            width=10
        )
        max_tokens_spinbox.grid(row=1, column=1, padx=5, pady=5, sticky=tk.W)

        # ë„êµ¬ ì‹¤í–‰ í™•ì¸ ì˜µì…˜
        confirm_check = ttk.Checkbutton(
            config_frame, 
            text="Confirm tool execution",
            variable=self.confirm_tool_execution
        )
        confirm_check.grid(row=1, column=2, padx=10)

        # ===== ì±„íŒ… ì˜ì—­ =====
        chat_frame = ttk.LabelFrame(self.root, text="ğŸ’¬ Conversation", padding=10)
        chat_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        # ì±„íŒ… ë””ìŠ¤í”Œë ˆì´
        self.chat_display = scrolledtext.ScrolledText(
            chat_frame,
            wrap=tk.WORD,
            width=100,
            height=30,
            font=("Consolas", 10),
            state=tk.DISABLED
        )
        self.chat_display.pack(fill=tk.BOTH, expand=True, pady=(0, 5))

        # íƒœê·¸ ì„¤ì •
        self.chat_display.tag_config("user", foreground="#2196F3", font=("Consolas", 10, "bold"))
        self.chat_display.tag_config("assistant", foreground="#4CAF50", font=("Consolas", 10))
        self.chat_display.tag_config("system", foreground="#FF9800", font=("Consolas", 9, "italic"))
        self.chat_display.tag_config("tool", foreground="#9C27B0", font=("Consolas", 9))

        # ===== ì…ë ¥ ì˜ì—­ =====
        input_frame = ttk.Frame(chat_frame)
        input_frame.pack(fill=tk.X)

        self.input_text = tk.Text(input_frame, height=3, font=("Consolas", 10))
        self.input_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 5))
        self.input_text.bind("<Control-Return>", lambda e: self.send_message())

        # ë²„íŠ¼ í”„ë ˆì„
        btn_frame = ttk.Frame(input_frame)
        btn_frame.pack(side=tk.RIGHT, fill=tk.Y)

        self.send_btn = ttk.Button(btn_frame, text="Send\n(Ctrl+Enter)", 
                                   command=self.send_message, state=tk.DISABLED)
        self.send_btn.pack(fill=tk.BOTH, expand=True, pady=(0, 5))

        self.reset_btn = ttk.Button(btn_frame, text="Reset\nChat", 
                                    command=self.reset_chat, state=tk.DISABLED)
        self.reset_btn.pack(fill=tk.BOTH, expand=True)

        # ===== ë„êµ¬ ì •ë³´ =====
        tools_frame = ttk.LabelFrame(self.root, text="ğŸ”§ Available Tools", padding=10)
        tools_frame.pack(fill=tk.X, padx=10, pady=5)

        tools_text = "  â€¢  ".join([f"{name}" for name in TOOLS.keys()])
        ttk.Label(tools_frame, text=tools_text, font=("Consolas", 9)).pack()

    def append_text(self, text: str, tag: str = None):
        """í…ìŠ¤íŠ¸ ì¶”ê°€ (thread-safe)"""
        def _append():
            self.chat_display.config(state=tk.NORMAL)
            if tag:
                self.chat_display.insert(tk.END, text, tag)
            else:
                self.chat_display.insert(tk.END, text)
            self.chat_display.see(tk.END)
            self.chat_display.config(state=tk.DISABLED)

        self.root.after(0, _append)

    def set_status(self, text: str, color: str = "black"):
        """ìƒíƒœ ì—…ë°ì´íŠ¸"""
        def _update():
            self.status_label.config(text=text, foreground=color)
        self.root.after(0, _update)

    def confirm_tool_execution_dialog(self, tool_name: str, arguments: Dict[str, Any]) -> bool:
        """
        â­ ë„êµ¬ ì‹¤í–‰ ì „ ì‚¬ìš©ì í™•ì¸ ë‹¤ì´ì–¼ë¡œê·¸

        Returns:
            True: ì‹¤í–‰ ìŠ¹ì¸, False: ì‹¤í–‰ ê±°ë¶€
        """
        # í™•ì¸ ëª¨ë“œê°€ ì•„ë‹ˆë©´ ë°”ë¡œ ìŠ¹ì¸
        if not self.confirm_tool_execution.get():
            return True

        # ì¸ìˆ˜ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
        args_formatted = json.dumps(arguments, indent=2, ensure_ascii=False)

        # ë„êµ¬ ì„¤ëª… ê°€ì ¸ì˜¤ê¸°
        tool_desc = TOOLS.get(tool_name, {}).get('description', 'No description')

        message = f"""The agent wants to execute a tool:

Tool: {tool_name}
Description: {tool_desc}

Arguments:
{args_formatted}

Do you want to proceed?"""

        # ë©”ì‹œì§€ ë°•ìŠ¤ í‘œì‹œ (Yes/No)
        result = messagebox.askyesno(
            "Confirm Tool Execution",
            message,
            icon='question'
        )

        return result

    def connect(self):
        """Ollama ì—°ê²°"""

        def _connect():
            def _update_model_select():
                def _fetch_models(base_url) -> List[str]:
                    url = urllib.parse.urljoin(base_url, "/api/tags")
                    with urllib.request.urlopen(url) as response:
                        data = json.load(response)
                        models = [model["name"] for model in data["models"]]
                        return models

                def _show_error(text):
                    self.model_entry.set(text)
                    self.model_entry.config(foreground="red")
                    self.model_entry["values"] = []

                try:
                    models = _fetch_models(self.url_entry.get())
                    self.model_entry["values"] = models
                    if models:
                        self.model_entry.set(models[0])
                    else:
                        _show_error("You need download a model!")
                except Exception:  # noqa
                    _show_error("Error! Please check the host.")
            try:
                _update_model_select()
                self.set_status(f"â— Connected", "green")
                self.append_text(f"[System] Connected to {self.url_entry.get()}\n", "system")
                self.send_btn.config(state=tk.NORMAL)
                self.reset_btn.config(state=tk.NORMAL)
            except Exception as e:
                self.append_text(f"[System] âŒ Connection failed: {str(e)}\n", "system")
                self.set_status("â— Connection failed", "red")

        threading.Thread(target=_connect, daemon=True).start()

    def send_message(self):
        """ë©”ì‹œì§€ ì „ì†¡"""
        if self.processing or not self.agent:
            return

        user_input = self.input_text.get("1.0", tk.END).strip()
        if not user_input:
            return

        # ì…ë ¥ì°½ í´ë¦¬ì–´
        self.input_text.delete("1.0", tk.END)

        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        self.append_text(f"ğŸ‘¤ You:\n", "user")
        self.append_text(f"{user_input}\n\n")

        # ë²„íŠ¼ ë¹„í™œì„±í™”
        self.processing = True
        self.send_btn.config(state=tk.DISABLED)
        self.input_text.config(state=tk.DISABLED)

        # Assistant í—¤ë”
        self.append_text(f"ğŸ¤– Assistant:\n", "assistant")

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
        def _process():
            try:
                # ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
                def stream_cb(token):
                    self.append_text(token)

                # ìƒíƒœ ì½œë°±
                def status_cb(status):
                    self.append_text(f"\n[{status}]\n", "system")

                # â­ í™•ì¸ ì½œë°± (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
                def confirm_cb(tool_name, arguments):
                    # threading.Eventë¡œ ë™ê¸°í™”
                    result_container = [None]
                    event = threading.Event()

                    def _ask():
                        result_container[0] = self.confirm_tool_execution_dialog(tool_name, arguments)
                        event.set()

                    self.root.after(0, _ask)
                    event.wait()  # ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸°

                    return result_container[0]

                # Agent ì‹¤í–‰
                self.agent.chat(
                    user_input,
                    stream_callback=stream_cb,
                    status_callback=status_cb,
                    confirm_callback=confirm_cb,
                    max_tokens=self.max_tokens_var.get()
                )

                self.append_text("\n\n" + "="*80 + "\n\n")

            except Exception as e:
                self.append_text(f"\n\nâŒ Error: {str(e)}\n\n", "system")

            finally:
                # ë²„íŠ¼ ì¬í™œì„±í™”
                self.processing = False
                self.root.after(0, lambda: self.send_btn.config(state=tk.NORMAL))
                self.root.after(0, lambda: self.input_text.config(state=tk.NORMAL))
                self.root.after(0, lambda: self.input_text.focus())

        threading.Thread(target=_process, daemon=True).start()

    def reset_chat(self):
        """ì±„íŒ… ì´ˆê¸°í™”"""
        if self.agent:
            self.agent.reset()

        self.chat_display.config(state=tk.NORMAL)
        self.chat_display.delete("1.0", tk.END)
        self.chat_display.config(state=tk.DISABLED)

        self.append_text("[System] Chat reset! ğŸ”„\n\n", "system")

# ============================================================================
# Main
# ============================================================================

def main():
    root = tk.Tk()
    app = AgentGUI(root)
    app.reset_chat()
    root.mainloop()

if __name__ == "__main__":
    main()
